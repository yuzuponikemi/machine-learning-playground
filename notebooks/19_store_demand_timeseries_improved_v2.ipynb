{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯19: Store Demandæ™‚ç³»åˆ—äºˆæ¸¬ - GBDTÃ—æ™‚ç³»åˆ—ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰ â°\n",
    "\n",
    "**å­¦ç¿’ç›®æ¨™**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§GBDTã‚’ä½¿ã„ã€SMAPE < 15%ã‚’é”æˆ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å­¦ã¶ã“ã¨\n",
    "\n",
    "### 1. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®åŸºç¤ â­â­â­\n",
    "- Trendï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰ã€Seasonalityï¼ˆå­£ç¯€æ€§ï¼‰ã€Residualsï¼ˆæ®‹å·®ï¼‰ã®åˆ†è§£\n",
    "- Autocorrelationï¼ˆè‡ªå·±ç›¸é–¢ï¼‰ã®ç†è§£\n",
    "- æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–\n",
    "\n",
    "### 2. æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° â­â­â­\n",
    "- Lag featuresï¼ˆãƒ©ã‚°ç‰¹å¾´é‡ï¼‰: 1æ—¥å‰ã€7æ—¥å‰ã€30æ—¥å‰...\n",
    "- Rolling window statisticsï¼ˆç§»å‹•çª“çµ±è¨ˆé‡ï¼‰\n",
    "- å‘¨æœŸæ€§ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆæ›œæ—¥ã€æœˆã€ç¥æ—¥ï¼‰\n",
    "- Fourier featuresï¼ˆãƒ•ãƒ¼ãƒªã‚¨ç‰¹å¾´é‡ï¼‰\n",
    "\n",
    "### 3. æ™‚ç³»åˆ—ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚° â­â­â­\n",
    "- Temporal splitï¼ˆæ™‚é–“åˆ†å‰²ï¼‰\n",
    "- Walk-forward validation\n",
    "- GBDT for time series\n",
    "- Multi-step forecasting\n",
    "\n",
    "### 4. è©•ä¾¡æŒ‡æ¨™ â­â­\n",
    "- SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "- MAE, RMSE for time series\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ç›®æ¨™\n",
    "\n",
    "- **SMAPE**: < 15%\n",
    "- **Kaggle Store Item Demand**: Top 25%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# æ™‚ç³»åˆ—åˆ†æ\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# æ©Ÿæ¢°å­¦ç¿’\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# GBDT\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# è¨­å®š\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('tab10')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "data_dir = Path('../data/store_demand')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    train = pd.read_csv(data_dir / 'train.csv', parse_dates=['date'])\n",
    "    test = pd.read_csv(data_dir / 'test.csv', parse_dates=['date'])\n",
    "    print(\"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ ãƒ‡ãƒ¢ç”¨ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\")\n",
    "    # ãƒ‡ãƒ¢ç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "    dates = pd.date_range(start='2013-01-01', end='2017-12-31', freq='D')\n",
    "    n_stores = 10\n",
    "    n_items = 50\n",
    "    \n",
    "    data_list = []\n",
    "    for store in range(1, n_stores + 1):\n",
    "        for item in range(1, n_items + 1):\n",
    "            # ãƒˆãƒ¬ãƒ³ãƒ‰ + å­£ç¯€æ€§ + ãƒã‚¤ã‚º\n",
    "            trend = np.linspace(10, 50, len(dates))\n",
    "            seasonality = 20 * np.sin(np.arange(len(dates)) * 2 * np.pi / 365.25)\n",
    "            weekly = 10 * np.sin(np.arange(len(dates)) * 2 * np.pi / 7)\n",
    "            noise = np.random.normal(0, 5, len(dates))\n",
    "            sales = np.maximum(0, trend + seasonality + weekly + noise + store * 2 + item)\n",
    "            \n",
    "            df_temp = pd.DataFrame({\n",
    "                'date': dates,\n",
    "                'store': store,\n",
    "                'item': item,\n",
    "                'sales': sales\n",
    "            })\n",
    "            data_list.append(df_temp)\n",
    "    \n",
    "    train = pd.concat(data_list, ignore_index=True)\n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆæ¬¡ã®3ãƒ¶æœˆï¼‰\n",
    "    test_dates = pd.date_range(start='2018-01-01', end='2018-03-31', freq='D')\n",
    "    test = pd.DataFrame({\n",
    "        'date': np.tile(test_dates, n_stores * n_items),\n",
    "        'store': np.repeat(np.arange(1, n_stores + 1), len(test_dates) * n_items),\n",
    "        'item': np.tile(np.repeat(np.arange(1, n_items + 1), len(test_dates)), n_stores)\n",
    "    })\n",
    "\n",
    "print(f\"\\nTrain shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nDate range: {train['date'].min()} to {train['date'].max()}\")\n",
    "\n",
    "# æœ€åˆã®æ•°è¡Œ\n",
    "display(train.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 æ™‚ç³»åˆ—ã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ã¤ã®åº—èˆ—ãƒ»å•†å“ã®æ™‚ç³»åˆ—ã‚’å¯è¦–åŒ–\n",
    "sample = train[(train['store'] == 1) & (train['item'] == 1)].copy()\n",
    "sample = sample.set_index('date')['sales']\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# 1. å…ƒã®æ™‚ç³»åˆ—\n",
    "sample.plot(ax=axes[0])\n",
    "axes[0].set_title('Sales Time Series (Store 1, Item 1)')\n",
    "axes[0].set_ylabel('Sales')\n",
    "\n",
    "# 2. ç§»å‹•å¹³å‡\n",
    "sample.rolling(window=30).mean().plot(ax=axes[1], label='30-day MA')\n",
    "sample.rolling(window=90).mean().plot(ax=axes[1], label='90-day MA')\n",
    "axes[1].set_title('Moving Averages')\n",
    "axes[1].set_ylabel('Sales')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. æœˆæ¬¡é›†è¨ˆ\n",
    "monthly = sample.resample('M').sum()\n",
    "monthly.plot(ax=axes[2], marker='o')\n",
    "axes[2].set_title('Monthly Sales')\n",
    "axes[2].set_ylabel('Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 å­£ç¯€æ€§åˆ†è§£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­£ç¯€æ€§åˆ†è§£ï¼ˆåŠ æ³•ãƒ¢ãƒ‡ãƒ«ï¼‰\n",
    "decomposition = seasonal_decompose(sample, model='additive', period=365)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "\n",
    "decomposition.observed.plot(ax=axes[0])\n",
    "axes[0].set_ylabel('Observed')\n",
    "axes[0].set_title('Time Series Decomposition')\n",
    "\n",
    "decomposition.trend.plot(ax=axes[1])\n",
    "axes[1].set_ylabel('Trend')\n",
    "\n",
    "decomposition.seasonal.plot(ax=axes[2])\n",
    "axes[2].set_ylabel('Seasonal')\n",
    "\n",
    "decomposition.resid.plot(ax=axes[3])\n",
    "axes[3].set_ylabel('Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 è‡ªå·±ç›¸é–¢åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ACF\n",
    "plot_acf(sample.dropna(), lags=50, ax=axes[0])\n",
    "axes[0].set_title('Autocorrelation Function (ACF)')\n",
    "\n",
    "# PACF\n",
    "plot_pacf(sample.dropna(), lags=50, ax=axes[1])\n",
    "axes[1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š ACF/PACFã‹ã‚‰ã€æœ‰æ„ãªãƒ©ã‚°ï¼ˆlagï¼‰ã‚’ç‰¹å®šã§ãã¾ã™\")\n",
    "print(\"   - 7æ—¥ãƒ©ã‚°: é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³\")\n",
    "print(\"   - 30æ—¥ãƒ©ã‚°: æœˆæ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³\")\n",
    "print(\"   - 365æ—¥ãƒ©ã‚°: å¹´æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¨ 2. æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° â­â­â­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_features(df):\n",
    "    \"\"\"\n",
    "    æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®ä½œæˆ\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # æ—¥ä»˜ç‰¹å¾´é‡\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    # é€±æœ«ãƒ•ãƒ©ã‚°\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    \n",
    "    # æœˆåˆãƒ»æœˆæœ«ãƒ•ãƒ©ã‚°\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    \n",
    "    # å‘¨æœŸæ€§ã®æ­£å¼¦ãƒ»ä½™å¼¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    \n",
    "    print(f\"âœ… æ™‚ç³»åˆ—ç‰¹å¾´é‡ä½œæˆå®Œäº†: {df.shape[1]} ç‰¹å¾´é‡\")\n",
    "    return df\n",
    "\n",
    "# ç‰¹å¾´é‡ä½œæˆ\n",
    "train = create_time_series_features(train)\n",
    "test = create_time_series_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Lag Featuresï¼ˆãƒ©ã‚°ç‰¹å¾´é‡ï¼‰ â­â­â­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, lags=[1, 7, 14, 28, 30, 60, 90]):\n",
    "    \"\"\"\n",
    "    Lag featuresï¼ˆéå»ã®å£²ä¸Šï¼‰ã®ä½œæˆ\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['store', 'item', 'date'])\n",
    "    \n",
    "    if 'sales' in df.columns:\n",
    "        for lag in lags:\n",
    "            df[f'sales_lag_{lag}'] = df.groupby(['store', 'item'])['sales'].shift(lag)\n",
    "    \n",
    "    print(f\"âœ… Lagç‰¹å¾´é‡ä½œæˆ: {len(lags)} lags\")\n",
    "    return df\n",
    "\n",
    "# Train ãƒ‡ãƒ¼ã‚¿ã«Lagç‰¹å¾´é‡ã‚’è¿½åŠ \n",
    "train = create_lag_features(train, lags=[1, 7, 14, 28, 30, 60, 90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Rolling Window Statisticsï¼ˆç§»å‹•çª“çµ±è¨ˆé‡ï¼‰ â­â­â­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df, windows=[7, 14, 30, 60, 90]):\n",
    "    \"\"\"\n",
    "    Rolling window statistics\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['store', 'item', 'date'])\n",
    "    \n",
    "    if 'sales' in df.columns:\n",
    "        for window in windows:\n",
    "            # Rolling mean\n",
    "            df[f'sales_rolling_mean_{window}'] = (\n",
    "                df.groupby(['store', 'item'])['sales']\n",
    "                .transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "            )\n",
    "            \n",
    "            # Rolling std\n",
    "            df[f'sales_rolling_std_{window}'] = (\n",
    "                df.groupby(['store', 'item'])['sales']\n",
    "                .transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "            )\n",
    "            \n",
    "            # Rolling max/min\n",
    "            df[f'sales_rolling_max_{window}'] = (\n",
    "                df.groupby(['store', 'item'])['sales']\n",
    "                .transform(lambda x: x.rolling(window=window, min_periods=1).max())\n",
    "            )\n",
    "            df[f'sales_rolling_min_{window}'] = (\n",
    "                df.groupby(['store', 'item'])['sales']\n",
    "                .transform(lambda x: x.rolling(window=window, min_periods=1).min())\n",
    "            )\n",
    "    \n",
    "    print(f\"âœ… Rollingç‰¹å¾´é‡ä½œæˆ: {len(windows)} windows\")\n",
    "    return df\n",
    "\n",
    "# Rollingç‰¹å¾´é‡è¿½åŠ \n",
    "train = create_rolling_features(train, windows=[7, 14, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "\n",
    "### âš ï¸ é‡è¦: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã¯ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãªã„ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢\n",
    "feature_cols = [col for col in train.columns if col not in ['date', 'sales']]\n",
    "X = train[feature_cols]\n",
    "y = train['sales']\n",
    "\n",
    "# æ¬ æå€¤è£œå®Œï¼ˆLagç‰¹å¾´é‡ã§ç™ºç”Ÿï¼‰\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†\")\n",
    "print(f\"   ç‰¹å¾´é‡æ•°: {X.shape[1]}\")\n",
    "print(f\"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}\")\n",
    "print(f\"\\nç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆæŠœç²‹ï¼‰:\")\n",
    "for i, col in enumerate(X.columns[:20], 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆTemporal Splitï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ™‚ç³»åˆ—é †ã§ã‚½ãƒ¼ãƒˆ\n",
    "train_sorted = train.sort_values('date').reset_index(drop=True)\n",
    "X_sorted = train_sorted[feature_cols].fillna(0)\n",
    "y_sorted = train_sorted['sales']\n",
    "\n",
    "# æœ€å¾Œã®3ãƒ¶æœˆã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "split_date = train_sorted['date'].max() - pd.Timedelta(days=90)\n",
    "train_mask = train_sorted['date'] < split_date\n",
    "val_mask = train_sorted['date'] >= split_date\n",
    "\n",
    "X_train = X_sorted[train_mask]\n",
    "y_train = y_sorted[train_mask]\n",
    "X_val = X_sorted[val_mask]\n",
    "y_val = y_sorted[val_mask]\n",
    "\n",
    "print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²:\")\n",
    "print(f\"   Train: {len(X_train)} samples\")\n",
    "print(f\"   Validation: {len(X_val)} samples\")\n",
    "print(f\"   Split date: {split_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 4. GBDT Time Series Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 SMAPEè©•ä¾¡é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error\n",
    "    \"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 100 * np.mean(diff)\n",
    "\n",
    "print(\"âœ… SMAPEè©•ä¾¡é–¢æ•°å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "params_lgb = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 6,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(**params_lgb)\n",
    "model_lgb.fit(X_train, y_train,\n",
    "             eval_set=[(X_val, y_val)],\n",
    "             callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "\n",
    "# äºˆæ¸¬\n",
    "preds_lgb = model_lgb.predict(X_val)\n",
    "preds_lgb = np.maximum(0, preds_lgb)  # è² ã®å€¤ã‚’0ã«ã‚¯ãƒªãƒƒãƒ—\n",
    "\n",
    "# ã‚¹ã‚³ã‚¢\n",
    "smape_lgb = smape(y_val, preds_lgb)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_val, preds_lgb))\n",
    "mae_lgb = mean_absolute_error(y_val, preds_lgb)\n",
    "\n",
    "print(f\"\\nğŸ“Š LightGBM Results:\")\n",
    "print(f\"   SMAPE: {smape_lgb:.2f}%\")\n",
    "print(f\"   RMSE:  {rmse_lgb:.2f}\")\n",
    "print(f\"   MAE:   {mae_lgb:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# XGBoostãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nparams_xgb = {\n    'objective': 'reg:squarederror',\n    'max_depth': 6,\n    'learning_rate': 0.05,\n    'n_estimators': 1000,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'random_state': RANDOM_STATE\n}\n\nmodel_xgb = xgb.XGBRegressor(\n    early_stopping_rounds=50,\n   **params_xgb)\nmodel_xgb.fit(X_train, y_train,\n             eval_set=[(X_val, y_val)],\n    verbose=False)\n\npreds_xgb = np.maximum(0, model_xgb.predict(X_val))\nsmape_xgb = smape(y_val, preds_xgb)\n\nprint(f\"\\nğŸ“Š XGBoost SMAPE: {smape_xgb:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CatBoostãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nparams_cat = {\n    'loss_function': 'RMSE',\n    'depth': 6,\n    'learning_rate': 0.05,\n    'iterations': 1000,\n    'random_state': RANDOM_STATE,\n    'verbose': False\n}\n\nmodel_cat = CatBoostRegressor(**params_cat)\nmodel_cat.fit(X_train, y_train,\n             eval_set=(X_val, y_val)verbose=False)\n\npreds_cat = np.maximum(0, model_cat.predict(X_val))\nsmape_cat = smape(y_val, preds_cat)\n\nprint(f\"\\nğŸ“Š CatBoost SMAPE: {smape_cat:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ­ 5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
    "weights = [0.4, 0.3, 0.3]\n",
    "ensemble_preds = (weights[0] * preds_lgb + \n",
    "                  weights[1] * preds_xgb + \n",
    "                  weights[2] * preds_cat)\n",
    "\n",
    "smape_ensemble = smape(y_val, ensemble_preds)\n",
    "\n",
    "print(f\"\\nğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ:\")\n",
    "print(f\"   LightGBM: {smape_lgb:.2f}%\")\n",
    "print(f\"   XGBoost:  {smape_xgb:.2f}%\")\n",
    "print(f\"   CatBoost: {smape_cat:.2f}%\")\n",
    "print(f\"   Ensemble: {smape_ensemble:.2f}% â­\")\n",
    "\n",
    "if smape_ensemble < 15:\n",
    "    print(f\"\\nğŸ‰ ç›®æ¨™é”æˆï¼SMAPE < 15%\")\n",
    "else:\n",
    "    print(f\"\\nğŸ’ª ã‚‚ã†å°‘ã—ï¼ç›®æ¨™SMAPE: 15%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 6. çµæœã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ã¤ã®åº—èˆ—ãƒ»å•†å“ã®äºˆæ¸¬ã‚’å¯è¦–åŒ–\n",
    "sample_mask = (train_sorted['store'] == 1) & (train_sorted['item'] == 1)\n",
    "sample_val_mask = sample_mask & val_mask\n",
    "\n",
    "if sample_val_mask.sum() > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # å®Ÿæ¸¬å€¤\n",
    "    dates_val = train_sorted.loc[sample_val_mask, 'date']\n",
    "    actual_val = y_val[sample_val_mask.values[val_mask]]\n",
    "    predicted_val = ensemble_preds[sample_val_mask.values[val_mask]]\n",
    "    \n",
    "    ax.plot(dates_val, actual_val, label='Actual', marker='o', alpha=0.7)\n",
    "    ax.plot(dates_val, predicted_val, label='Predicted', marker='x', alpha=0.7)\n",
    "    ax.set_title('Actual vs Predicted Sales (Store 1, Item 1)')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Sales')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 7. ã¾ã¨ã‚\n",
    "\n",
    "### å­¦ã‚“ã ã“ã¨\n",
    "\n",
    "1. âœ… æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®åŸºç¤ï¼ˆTrendã€Seasonalityã€ACF/PACFï¼‰\n",
    "2. âœ… Lag featuresï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨ï¼‰\n",
    "3. âœ… Rolling window statisticsï¼ˆç§»å‹•çµ±è¨ˆé‡ï¼‰\n",
    "4. âœ… Temporal splitï¼ˆæ™‚é–“åˆ†å‰²ï¼‰ã®é‡è¦æ€§\n",
    "5. âœ… GBDTã§æ™‚ç³»åˆ—äºˆæ¸¬\n",
    "6. âœ… SMAPEè©•ä¾¡æŒ‡æ¨™\n",
    "\n",
    "### ã•ã‚‰ã«å­¦ã¶ã«ã¯\n",
    "\n",
    "- **ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯20**: Optunaã§æœ€é©åŒ–\n",
    "- **ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯21**: SHAPã§ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ\n",
    "- **ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯24**: æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ·±æ˜ã‚Š\n",
    "\n",
    "---\n",
    "\n",
    "**â° Happy Forecasting! ğŸ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}