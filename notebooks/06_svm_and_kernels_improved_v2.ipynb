{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ノートブック06: サポートベクターマシン (SVM) とカーネル法\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックでは、サポートベクターマシン (SVM) とカーネル法について学びます:\n",
    "\n",
    "1. **SVMの基礎**\n",
    "   - マージン最大化の原理\n",
    "   - サポートベクターの役割\n",
    "   - ハードマージンとソフトマージン\n",
    "\n",
    "2. **線形SVM**\n",
    "   - 線形分離可能なデータへの適用\n",
    "   - Cパラメータの意味と調整\n",
    "\n",
    "3. **カーネル法**\n",
    "   - カーネルトリックの原理\n",
    "   - RBF (Radial Basis Function) カーネル\n",
    "   - 多項式カーネル\n",
    "   - シグモイドカーネル\n",
    "\n",
    "4. **非線形分類**\n",
    "   - make_moons、make_circlesデータセット\n",
    "   - カーネル別の決定境界の可視化\n",
    "\n",
    "5. **ハイパーパラメータチューニング**\n",
    "   - GridSearchCVによるC、gammaの最適化\n",
    "   - 過学習と未学習のバランス\n",
    "\n",
    "6. **実践例**\n",
    "   - 乳がん診断データセット\n",
    "   - 他の分類器との比較\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 日本語フォント設定\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# ランダムシード固定\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. SVMの基礎: マージン最大化\n",
    "\n",
    "### SVMの核心アイデア\n",
    "\n",
    "サポートベクターマシン (SVM) は、**マージンを最大化**することで分類境界を決定します。\n",
    "\n",
    "- **マージン**: 決定境界と最も近いデータポイントとの距離\n",
    "- **サポートベクター**: マージン上にある重要なデータポイント\n",
    "- **目的**: マージンを最大化することで、汎化性能を向上\n",
    "\n",
    "### 線形SVMのデモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形分離可能なデータの生成\n",
    "X, y = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 線形SVMの訓練\n",
    "svm_linear = SVC(kernel='linear', C=1.0)\n",
    "svm_linear.fit(X, y)\n",
    "\n",
    "# 決定境界とマージンの可視化\n",
    "def plot_svm_decision_boundary(model, X, y, title='SVM Decision Boundary'):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # メッシュグリッドの作成\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # 予測\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # 決定境界のプロット\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    plt.contour(xx, yy, Z, colors='k', linewidths=0.5)\n",
    "    \n",
    "    # データポイントのプロット\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', \n",
    "                edgecolors='k', s=100, alpha=0.8)\n",
    "    \n",
    "    # サポートベクターの強調表示\n",
    "    if hasattr(model, 'support_vectors_'):\n",
    "        plt.scatter(model.support_vectors_[:, 0], \n",
    "                    model.support_vectors_[:, 1],\n",
    "                    s=200, linewidth=1.5, facecolors='none', \n",
    "                    edgecolors='green', label='Support Vectors')\n",
    "    \n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_svm_decision_boundary(svm_linear, X, y, 'Linear SVM: Margin Maximization')\n",
    "\n",
    "print(f\"Number of support vectors: {len(svm_linear.support_vectors_)}\")\n",
    "print(f\"Total data points: {len(X)}\")\n",
    "print(f\"Percentage of support vectors: {len(svm_linear.support_vectors_) / len(X) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 観察ポイント\n",
    "\n",
    "1. **サポートベクター (緑の円)**: 決定境界に最も近いポイント。これらのポイントだけがモデルの決定に影響\n",
    "2. **マージン**: サポートベクターと決定境界の距離が最大化されている\n",
    "3. **汎化性能**: マージンが大きいほど、未知のデータに対する性能が向上\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cパラメータ: ソフトマージンSVM\n",
    "\n",
    "### Cパラメータの役割\n",
    "\n",
    "実際のデータは完全には線形分離できないことが多いです。Cパラメータは、**誤分類の許容度**を制御します:\n",
    "\n",
    "- **C が大きい**: 誤分類を許さない → マージンが小さくなり、過学習のリスク\n",
    "- **C が小さい**: 誤分類を許容 → マージンが大きくなり、未学習のリスク\n",
    "\n",
    "### Cの影響を可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノイズのあるデータの生成\n",
    "X_noisy, y_noisy = make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=1.0,\n",
    "    flip_y=0.1,  # ノイズ追加\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 異なるCの値でSVMを訓練\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, C in enumerate(C_values):\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_noisy, y_noisy)\n",
    "    \n",
    "    # メッシュグリッドの作成\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_noisy[:, 0].min() - 1, X_noisy[:, 0].max() + 1\n",
    "    y_min, y_max = X_noisy[:, 1].min() - 1, X_noisy[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    axes[idx].contour(xx, yy, Z, colors='k', linewidths=0.5)\n",
    "    axes[idx].scatter(X_noisy[:, 0], X_noisy[:, 1], c=y_noisy, \n",
    "                      cmap='coolwarm', edgecolors='k', s=50, alpha=0.8)\n",
    "    axes[idx].scatter(svm.support_vectors_[:, 0], \n",
    "                      svm.support_vectors_[:, 1],\n",
    "                      s=150, linewidth=1.5, facecolors='none', \n",
    "                      edgecolors='green')\n",
    "    \n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=10)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=10)\n",
    "    axes[idx].set_title(f'C = {C}\\nSupport Vectors: {len(svm.support_vectors_)}',\n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nC parameter comparison:\")\n",
    "print(\"- Small C (0.1, 1): Wider margin, more support vectors, allows misclassification\")\n",
    "print(\"- Large C (10, 100): Narrower margin, fewer support vectors, minimizes misclassification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. カーネル法: 非線形分類への拡張\n",
    "\n",
    "### カーネルトリックとは?\n",
    "\n",
    "線形分離できないデータに対して、SVMは**カーネルトリック**を使用します:\n",
    "\n",
    "1. データを高次元空間に写像\n",
    "2. 高次元空間で線形分離を実行\n",
    "3. 元の空間に戻すと非線形境界になる\n",
    "\n",
    "### 主なカーネル関数\n",
    "\n",
    "1. **線形カーネル**: $K(x, x') = x \\cdot x'$\n",
    "2. **RBFカーネル (Gaussian)**: $K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$\n",
    "3. **多項式カーネル**: $K(x, x') = (\\gamma x \\cdot x' + r)^d$\n",
    "4. **シグモイドカーネル**: $K(x, x') = \\tanh(\\gamma x \\cdot x' + r)$\n",
    "\n",
    "### 非線形データでのカーネル比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2つの非線形データセットを生成\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.15, factor=0.5, random_state=42)\n",
    "\n",
    "datasets = [\n",
    "    ('Moons Dataset', X_moons, y_moons),\n",
    "    ('Circles Dataset', X_circles, y_circles)\n",
    "]\n",
    "\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "\n",
    "for dataset_name, X_data, y_data in datasets:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "    \n",
    "    for idx, kernel in enumerate(kernels):\n",
    "        # SVMモデルの訓練\n",
    "        if kernel == 'poly':\n",
    "            svm = SVC(kernel=kernel, degree=3, C=1.0)\n",
    "        else:\n",
    "            svm = SVC(kernel=kernel, C=1.0, gamma='auto')\n",
    "        \n",
    "        svm.fit(X_data, y_data)\n",
    "        \n",
    "        # メッシュグリッド\n",
    "        h = 0.02\n",
    "        x_min, x_max = X_data[:, 0].min() - 0.5, X_data[:, 0].max() + 0.5\n",
    "        y_min, y_max = X_data[:, 1].min() - 0.5, X_data[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "        \n",
    "        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "        axes[idx].scatter(X_data[:, 0], X_data[:, 1], c=y_data, \n",
    "                          cmap='coolwarm', edgecolors='k', s=50, alpha=0.8)\n",
    "        \n",
    "        accuracy = svm.score(X_data, y_data)\n",
    "        axes[idx].set_xlabel('Feature 1', fontsize=10)\n",
    "        axes[idx].set_ylabel('Feature 2', fontsize=10)\n",
    "        axes[idx].set_title(f'{kernel.upper()} Kernel\\nAccuracy: {accuracy:.3f}',\n",
    "                            fontsize=11, fontweight='bold')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    fig.suptitle(f'{dataset_name}: Kernel Comparison', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nKernel Performance Summary:\")\n",
    "print(\"- Linear: Poor for non-linear data\")\n",
    "print(\"- RBF: Best for most non-linear patterns\")\n",
    "print(\"- Polynomial: Good for specific curved patterns\")\n",
    "print(\"- Sigmoid: Similar to neural network activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 観察ポイント\n",
    "\n",
    "1. **Moons Dataset**: RBFと多項式カーネルが優れた性能\n",
    "2. **Circles Dataset**: RBFカーネルが明らかに最適\n",
    "3. **線形カーネル**: 非線形パターンには不十分\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RBFカーネルのgammaパラメータ\n",
    "\n",
    "### gammaパラメータの役割\n",
    "\n",
    "RBFカーネルでは、gammaパラメータが**決定境界の複雑さ**を制御します:\n",
    "\n",
    "- **gamma が大きい**: 各サンプルの影響範囲が狭い → 複雑な境界 → 過学習のリスク\n",
    "- **gamma が小さい**: 各サンプルの影響範囲が広い → 滑らかな境界 → 未学習のリスク\n",
    "\n",
    "### gammaの影響を可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moonsデータセットを使用\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "\n",
    "# 異なるgammaの値でSVMを訓練\n",
    "gamma_values = [0.1, 1, 10, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, gamma in enumerate(gamma_values):\n",
    "    svm = SVC(kernel='rbf', C=1.0, gamma=gamma)\n",
    "    svm.fit(X_moons, y_moons)\n",
    "    \n",
    "    # メッシュグリッド\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_moons[:, 0].min() - 0.5, X_moons[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_moons[:, 1].min() - 0.5, X_moons[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    axes[idx].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, \n",
    "                      cmap='coolwarm', edgecolors='k', s=50, alpha=0.8)\n",
    "    axes[idx].scatter(svm.support_vectors_[:, 0], \n",
    "                      svm.support_vectors_[:, 1],\n",
    "                      s=150, linewidth=1.5, facecolors='none', \n",
    "                      edgecolors='green')\n",
    "    \n",
    "    accuracy = svm.score(X_moons, y_moons)\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=10)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=10)\n",
    "    axes[idx].set_title(f'gamma = {gamma}\\nAccuracy: {accuracy:.3f}\\nSupport Vectors: {len(svm.support_vectors_)}',\n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\ngamma parameter effects:\")\n",
    "print(\"- Small gamma (0.1): Smooth decision boundary, may underfit\")\n",
    "print(\"- Medium gamma (1): Balanced complexity\")\n",
    "print(\"- Large gamma (10, 100): Complex boundary, may overfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ハイパーパラメータチューニング: GridSearchCV\n",
    "\n",
    "最適なC、gamma、kernelの組み合わせを見つけるために、GridSearchCVを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# パラメータグリッドの定義\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# GridSearchCVの実行\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Set Accuracy: {grid_search.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Top 5の結果を表示\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df = results_df.sort_values('rank_test_score')\n",
    "print(\"\\nTop 5 Parameter Combinations:\")\n",
    "print(results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].head())\n",
    "\n",
    "# 最適モデルの可視化\n",
    "best_model = grid_search.best_estimator_\n",
    "plot_svm_decision_boundary(best_model, X_moons, y_moons, \n",
    "                           f'Best SVM Model\\n{grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C vs gamma のヒートマップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBFカーネルに限定してヒートマップを作成\n",
    "rbf_results = results_df[results_df['param_kernel'] == 'rbf'].copy()\n",
    "\n",
    "# ピボットテーブルの作成\n",
    "pivot_table = rbf_results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_gamma',\n",
    "    columns='param_C'\n",
    ")\n",
    "\n",
    "# ヒートマップの可視化\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'CV Accuracy'})\n",
    "plt.title('SVM RBF Kernel: C vs gamma Performance', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('C Parameter', fontsize=12)\n",
    "plt.ylabel('gamma Parameter', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHeatmap Interpretation:\")\n",
    "print(\"- Dark red regions: Best performance\")\n",
    "print(\"- Light yellow regions: Poor performance\")\n",
    "print(\"- Diagonal patterns often indicate overfitting/underfitting balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 実践例: 乳がん診断データセット\n",
    "\n",
    "実際のデータセットを使って、SVMの性能を評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乳がんデータセットの読み込み\n",
    "cancer_data = load_breast_cancer()\n",
    "X_cancer = cancer_data.data\n",
    "y_cancer = cancer_data.target\n",
    "feature_names = cancer_data.feature_names\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"- Samples: {X_cancer.shape[0]}\")\n",
    "print(f\"- Features: {X_cancer.shape[1]}\")\n",
    "print(f\"- Classes: {len(np.unique(y_cancer))} (Malignant: {sum(y_cancer==0)}, Benign: {sum(y_cancer==1)})\")\n",
    "print(f\"\\nFeature names (first 10): {list(feature_names[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "# 標準化 (SVMでは重要!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data preprocessing complete.\")\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMモデルの訓練と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数のカーネルでSVMを訓練\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "svm_models = {}\n",
    "\n",
    "print(\"Training SVM models with different kernels...\\n\")\n",
    "\n",
    "for kernel in kernels:\n",
    "    print(f\"Training {kernel.upper()} kernel...\")\n",
    "    \n",
    "    if kernel == 'poly':\n",
    "        svm = SVC(kernel=kernel, degree=3, C=1.0, random_state=42)\n",
    "    else:\n",
    "        svm = SVC(kernel=kernel, C=1.0, random_state=42)\n",
    "    \n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    svm_models[kernel] = svm\n",
    "    \n",
    "    # 評価\n",
    "    train_acc = svm.score(X_train_scaled, y_train)\n",
    "    test_acc = svm.score(X_test_scaled, y_test)\n",
    "    cv_scores = cross_val_score(svm, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"  Support Vectors: {len(svm.support_vectors_)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCVで最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータグリッド\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting Grid Search for optimal parameters...\")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZED SVM RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Set Accuracy: {grid_search.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# 最適モデルで予測\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred = best_svm.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混同行列の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix: Breast Cancer Diagnosis\\n(Optimized SVM)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# エラー分析\n",
    "false_positives = cm[0, 1]\n",
    "false_negatives = cm[1, 0]\n",
    "print(f\"\\nError Analysis:\")\n",
    "print(f\"False Positives (Malignant predicted as Benign): {false_positives}\")\n",
    "print(f\"False Negatives (Benign predicted as Malignant): {false_negatives}\")\n",
    "print(f\"\\nNote: In medical diagnosis, minimizing false negatives is critical!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 他の分類器との比較\n",
    "\n",
    "SVMの性能を、他の分類アルゴリズムと比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数のモデルを訓練\n",
    "models = {\n",
    "    'SVM (RBF)': SVC(kernel='rbf', C=10, gamma=0.001, random_state=42),\n",
    "    'SVM (Linear)': SVC(kernel='linear', C=1.0, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=10000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Comparing multiple classifiers...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # 訓練\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 評価\n",
    "    train_acc = model.score(X_train_scaled, y_train)\n",
    "    test_acc = model.score(X_test_scaled, y_test)\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Train: {train_acc:.4f} | Test: {test_acc:.4f} | CV: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# 結果をDataFrameに変換\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バープロット\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Train vs Test Accuracy\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, results_df['Train Accuracy'], width, \n",
    "            label='Train', alpha=0.8, color='steelblue')\n",
    "axes[0].bar(x + width/2, results_df['Test Accuracy'], width, \n",
    "            label='Test', alpha=0.8, color='coral')\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Train vs Test Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "\n",
    "# CV Mean with Error Bars\n",
    "axes[1].bar(x, results_df['CV Mean'], yerr=results_df['CV Std'], \n",
    "            alpha=0.8, color='seagreen', capsize=5)\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('CV Accuracy', fontsize=12)\n",
    "axes[1].set_title('Cross-Validation Performance', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## まとめ\n",
    "\n",
    "### SVMの主要ポイント\n",
    "\n",
    "1. **マージン最大化**\n",
    "   - SVMは決定境界とサポートベクターの距離を最大化\n",
    "   - 汎化性能の向上につながる\n",
    "\n",
    "2. **Cパラメータ**\n",
    "   - 誤分類の許容度を制御\n",
    "   - 大きいC → 過学習のリスク\n",
    "   - 小さいC → 未学習のリスク\n",
    "\n",
    "3. **カーネル法**\n",
    "   - 線形分離できないデータを高次元空間で処理\n",
    "   - RBFカーネルが最も汎用的\n",
    "   - 多項式、シグモイドカーネルも状況により有効\n",
    "\n",
    "4. **gammaパラメータ (RBF)**\n",
    "   - 決定境界の複雑さを制御\n",
    "   - 大きいgamma → 複雑な境界\n",
    "   - 小さいgamma → 滑らかな境界\n",
    "\n",
    "5. **前処理の重要性**\n",
    "   - SVMは特徴量のスケールに敏感\n",
    "   - StandardScalerによる標準化が必須\n",
    "\n",
    "6. **ハイパーパラメータチューニング**\n",
    "   - GridSearchCVで最適なC、gamma、kernelを探索\n",
    "   - Cross-validationで汎化性能を評価\n",
    "\n",
    "### SVMの長所と短所\n",
    "\n",
    "**長所:**\n",
    "- 高次元データに強い\n",
    "- 非線形境界を効果的に学習\n",
    "- 過学習に対して比較的ロバスト\n",
    "- 少数のサンプルでも良好な性能\n",
    "\n",
    "**短所:**\n",
    "- 大規模データセットでは訓練が遅い (O(n²〜n³))\n",
    "- ハイパーパラメータの調整が必要\n",
    "- 確率的な予測が得られない (predict_probaはあるが近似)\n",
    "- 解釈性が低い\n",
    "\n",
    "### 実践的な使用ガイドライン\n",
    "\n",
    "1. **データの前処理**: 必ずStandardScalerで標準化\n",
    "2. **まず線形カーネルを試す**: データが線形分離可能か確認\n",
    "3. **RBFカーネルを試す**: 非線形パターンがあれば\n",
    "4. **GridSearchCVで最適化**: C、gammaを体系的に探索\n",
    "5. **小〜中規模データに最適**: 大規模データではRandom ForestやGBDTを検討\n",
    "\n",
    "---\n",
    "\n",
    "## 練習問題\n",
    "\n",
    "1. **カーネル比較**: Irisデータセットで、異なるカーネルの性能を比較してください\n",
    "2. **パラメータ可視化**: C vs gammaのヒートマップを作成し、最適領域を特定してください\n",
    "3. **医療診断**: 他の医療データセット (糖尿病、心臓病など) でSVMを適用してください\n",
    "4. **非線形データ生成**: make_moonsやmake_circlesのノイズパラメータを変えて、SVMの性能を評価してください\n",
    "5. **アンサンブル**: 複数のSVMモデル (異なるカーネル) を組み合わせて、性能向上を試みてください\n",
    "\n",
    "---\n",
    "\n",
    "**次のステップ**: ノートブック07で、k-NN (k近傍法) とクラスタリングを学びます!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
