{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. Tabularディープラーニング - Neural Networks for Tabular Data\n",
    "\n",
    "## 概要\n",
    "テーブルデータに対してニューラルネットワークを適用し、GBDTとの性能比較を行います。\n",
    "\n",
    "## 学習目標\n",
    "- Tabularデータでのディープラーニングの特徴を理解できる\n",
    "- scikit-learnでニューラルネットワークを実装できる\n",
    "- GBDTとの比較ができる\n",
    "- 実務での使い分けができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 設定\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tabularデータとディープラーニング\n",
    "\n",
    "### Tabularデータの特徴\n",
    "\n",
    "テーブルデータ（行列形式）は最も一般的なデータ形式です。\n",
    "\n",
    "### 従来の常識\n",
    "\n",
    "**GBDT（Gradient Boosting Decision Trees）が最強**\n",
    "- XGBoost、LightGBM、CatBoost\n",
    "- Kaggleコンペで圧倒的な実績\n",
    "- 特徴量エンジニアリング不要\n",
    "- スケーリング不要\n",
    "\n",
    "### ニューラルネットワークの利点\n",
    "\n",
    "一方、ニューラルネットワークには以下の利点があります：\n",
    "- エンドツーエンドの学習\n",
    "- 埋め込み表現の学習\n",
    "- 柔軟なアーキテクチャ\n",
    "- 大規模データでの優位性\n",
    "\n",
    "### いつNNを使うべきか？\n",
    "\n",
    "| 条件 | 推奨 |\n",
    "|------|------|\n",
    "| データサイズ < 10k | GBDT |\n",
    "| データサイズ > 100k | NN検討 |\n",
    "| 解釈可能性が重要 | GBDT |\n",
    "| エンドツーエンド学習 | NN |\n",
    "| 高速推論が必要 | GBDT |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込み（乳がん診断データ）\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "print(f\"データサイズ: {X.shape}\")\n",
    "print(f\"特徴量数: {X.shape[1]}\")\n",
    "print(f\"サンプル数: {X.shape[0]}\")\n",
    "print(f\"\\nターゲット分布:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\n特徴量の例（最初の5列）:\")\n",
    "print(X.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# さらに検証データを分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"訓練データ: {X_train.shape}\")\n",
    "print(f\"検証データ: {X_val.shape}\")\n",
    "print(f\"テストデータ: {X_test.shape}\")\n",
    "\n",
    "# スケーリング（ニューラルネットワークには必須）\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n重要: ニューラルネットワークでは特徴量のスケーリングが必須\")\n",
    "print(\"訓練データでfitし、検証・テストデータはtransformのみ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ニューラルネットワークの構築\n",
    "\n",
    "### MLPClassifier の特徴\n",
    "\n",
    "scikit-learnの`MLPClassifier`は多層パーセプトロン（Multi-Layer Perceptron）を実装しています。\n",
    "\n",
    "**主要なパラメータ:**\n",
    "- `hidden_layer_sizes`: 隠れ層のサイズ（例: (100, 50)は2層で各層100と50ノード）\n",
    "- `activation`: 活性化関数（'relu', 'tanh', 'logistic'）\n",
    "- `solver`: 最適化アルゴリズム（'adam', 'sgd', 'lbfgs'）\n",
    "- `alpha`: L2正則化の強さ\n",
    "- `learning_rate_init`: 初期学習率\n",
    "- `max_iter`: 最大イテレーション数\n",
    "- `early_stopping`: 検証データでのearly stopping\n",
    "\n",
    "### アーキテクチャ\n",
    "\n",
    "```\n",
    "Input (30 features)\n",
    "    ↓\n",
    "Hidden Layer 1 (100 nodes) + ReLU\n",
    "    ↓\n",
    "Hidden Layer 2 (50 nodes) + ReLU\n",
    "    ↓\n",
    "Output (2 classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットワークの構築\n",
    "print(\"ニューラルネットワークの学習中...\")\n",
    "\n",
    "nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # 2層: 100ノード、50ノード\n",
    "    activation='relu',              # ReLU活性化関数\n",
    "    solver='adam',                  # Adamオプティマイザ\n",
    "    alpha=0.0001,                   # L2正則化\n",
    "    learning_rate_init=0.001,       # 初期学習率\n",
    "    max_iter=500,                   # 最大イテレーション\n",
    "    early_stopping=True,            # Early stopping有効\n",
    "    validation_fraction=0.1,        # 検証データの割合\n",
    "    n_iter_no_change=10,            # Early stoppingの忍耐力\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 学習\n",
    "nn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n学習完了\")\n",
    "print(f\"イテレーション数: {nn_model.n_iter_}\")\n",
    "print(f\"層の数: {nn_model.n_layers_}\")\n",
    "print(f\"出力層のサイズ: {nn_model.n_outputs_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習曲線の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線（損失）\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(nn_model.loss_curve_, linewidth=2, color='steelblue')\n",
    "plt.xlabel('Iteration', fontsize=11)\n",
    "plt.ylabel('Loss', fontsize=11)\n",
    "plt.title('Neural Network Training Loss Curve', fontsize=12, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"最終損失: {nn_model.loss_:.6f}\")\n",
    "print(f\"総イテレーション数: {nn_model.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 検証データでの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証データで評価\n",
    "y_val_pred = nn_model.predict(X_val_scaled)\n",
    "y_val_proba = nn_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_auc = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Neural Networkの性能（検証データ）\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"ROC-AUC:  {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. テストデータでの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータで評価\n",
    "y_pred_nn = nn_model.predict(X_test_scaled)\n",
    "y_proba_nn = nn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "auc_nn = roc_auc_score(y_test, y_proba_nn)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Neural Networkの性能（テストデータ）\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_nn:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_nn:.4f}\")\n",
    "print(\"\\n混同行列:\")\n",
    "cm_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "print(cm_nn)\n",
    "print(\"\\n詳細レポート:\")\n",
    "print(classification_report(y_test, y_pred_nn, \n",
    "                           target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列の可視化\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=11)\n",
    "plt.ylabel('True Label', fontsize=11)\n",
    "plt.title('Confusion Matrix - Neural Network', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. GBDTとの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "print(\"LightGBMの学習中...\")\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "y_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "auc_lgb = roc_auc_score(y_test, y_proba_lgb)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LightGBMの性能（テストデータ）\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_lgb:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "print(\"XGBoostの学習中...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"XGBoostの性能（テストデータ）\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 性能比較の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能比較\n",
    "comparison_df = pd.DataFrame([\n",
    "    {'Model': 'Neural Network', 'Accuracy': accuracy_nn, 'ROC-AUC': auc_nn},\n",
    "    {'Model': 'LightGBM', 'Accuracy': accuracy_lgb, 'ROC-AUC': auc_lgb},\n",
    "    {'Model': 'XGBoost', 'Accuracy': accuracy_xgb, 'ROC-AUC': auc_xgb}\n",
    "]).sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"モデル性能比較\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "colors = ['steelblue', 'lightgreen', 'coral']\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "axes[0].bar(x_pos, comparison_df['Accuracy'].values, \n",
    "            alpha=0.7, edgecolor='black', color=colors)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(comparison_df['Model'].values)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, (idx, row) in enumerate(comparison_df.iterrows()):\n",
    "    axes[0].text(i, row['Accuracy'] + 0.002, f\"{row['Accuracy']:.4f}\",\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1].bar(x_pos, comparison_df['ROC-AUC'].values, \n",
    "            alpha=0.7, edgecolor='black', color=colors)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(comparison_df['Model'].values)\n",
    "axes[1].set_ylabel('ROC-AUC', fontsize=11)\n",
    "axes[1].set_title('ROC-AUC Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, (idx, row) in enumerate(comparison_df.iterrows()):\n",
    "    axes[1].text(i, row['ROC-AUC'] + 0.002, f\"{row['ROC-AUC']:.4f}\",\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n観察:\")\n",
    "best_model = comparison_df.iloc[0]['Model']\n",
    "print(f\"- 最良モデル: {best_model} (ROC-AUC: {comparison_df.iloc[0]['ROC-AUC']:.4f})\")\n",
    "print(\"- ニューラルネットワークはGBDTと競争力のある性能を示す\")\n",
    "print(\"- 小規模データではGBDTも依然として強力\")\n",
    "print(\"- データサイズや問題によって最適なモデルは異なる\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 特徴量重要度の比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBMの特徴量重要度\n",
    "lgb_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': lgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# XGBoostの特徴量重要度\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# LightGBM\n",
    "top_lgb = lgb_importance.head(10)\n",
    "axes[0].barh(range(len(top_lgb)), top_lgb['importance'].values, \n",
    "             color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(top_lgb)))\n",
    "axes[0].set_yticklabels(top_lgb['feature'].values, fontsize=9)\n",
    "axes[0].set_xlabel('Importance', fontsize=11)\n",
    "axes[0].set_title('LightGBM - Top 10 Features', fontsize=12, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# XGBoost\n",
    "top_xgb = xgb_importance.head(10)\n",
    "axes[1].barh(range(len(top_xgb)), top_xgb['importance'].values, \n",
    "             color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(top_xgb)))\n",
    "axes[1].set_yticklabels(top_xgb['feature'].values, fontsize=9)\n",
    "axes[1].set_xlabel('Importance', fontsize=11)\n",
    "axes[1].set_title('XGBoost - Top 10 Features', fontsize=12, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 重要な特徴量:\")\n",
    "print(\"\\nLightGBM:\")\n",
    "for idx, row in lgb_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nXGBoost:\")\n",
    "for idx, row in xgb_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ハイパーパラメータの影響\n",
    "\n",
    "ニューラルネットワークの性能は、ハイパーパラメータに大きく依存します。\n",
    "主要なパラメータの影響を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なるアーキテクチャでの比較\n",
    "architectures = [\n",
    "    (50,),           # 1層: 50ノード\n",
    "    (100,),          # 1層: 100ノード\n",
    "    (100, 50),       # 2層: 100, 50ノード\n",
    "    (100, 50, 25),   # 3層: 100, 50, 25ノード\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=arch,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.0001,\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    arch_str = '-'.join(map(str, arch))\n",
    "    results.append({\n",
    "        'Architecture': arch_str,\n",
    "        'Accuracy': acc,\n",
    "        'ROC-AUC': auc,\n",
    "        'Iterations': model.n_iter_\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nアーキテクチャ別の性能比較:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x_pos = np.arange(len(results_df))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].bar(x_pos, results_df['Accuracy'].values, alpha=0.7, edgecolor='black', color='steelblue')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(results_df['Architecture'].values, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('Accuracy by Architecture', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, row in results_df.iterrows():\n",
    "    axes[0].text(i, row['Accuracy'] + 0.002, f\"{row['Accuracy']:.4f}\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1].bar(x_pos, results_df['ROC-AUC'].values, alpha=0.7, edgecolor='black', color='coral')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(results_df['Architecture'].values, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('ROC-AUC', fontsize=11)\n",
    "axes[1].set_title('ROC-AUC by Architecture', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, row in results_df.iterrows():\n",
    "    axes[1].text(i, row['ROC-AUC'] + 0.002, f\"{row['ROC-AUC']:.4f}\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n観察:\")\n",
    "best_arch = results_df.loc[results_df['ROC-AUC'].idxmax()]\n",
    "print(f\"- 最良アーキテクチャ: {best_arch['Architecture']} (AUC: {best_arch['ROC-AUC']:.4f})\")\n",
    "print(\"- 深すぎるネットワークは必ずしも良い結果をもたらさない\")\n",
    "print(\"- Tabularデータには2-3層で十分なことが多い\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. まとめ\n",
    "\n",
    "### 本ノートブックで学んだこと\n",
    "\n",
    "1. **Tabularデータでのニューラルネットワーク**\n",
    "   - scikit-learnのMLPClassifierで簡単に実装可能\n",
    "   - スケーリングが必須\n",
    "   - Early stoppingで過学習防止\n",
    "\n",
    "2. **GBDTとの比較**\n",
    "   - NNもGBDTと競争力のある性能\n",
    "   - 小規模データではGBDTが依然強力\n",
    "   - データサイズや問題によって最適なモデルは異なる\n",
    "\n",
    "3. **アーキテクチャの重要性**\n",
    "   - 深すぎるネットワークは不要\n",
    "   - Tabularデータには2-3層で十分\n",
    "   - ハイパーパラメータチューニングが重要\n",
    "\n",
    "4. **実務での使い分け**\n",
    "   - 小規模データ（<10k）: GBDT推奨\n",
    "   - 大規模データ（>100k）: NN検討\n",
    "   - 解釈可能性重視: GBDT\n",
    "   - まず両方試して比較\n",
    "\n",
    "### 重要なポイント\n",
    "\n",
    "- **スケーリング必須**: StandardScaler推奨\n",
    "- **Early stopping**: 過学習を防ぐ\n",
    "- **適切な深さ**: 2-3層が最適\n",
    "- **比較検証**: まずGBDTを試し、NNで改善を図る\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "- ハイパーパラメータチューニング（GridSearch、RandomSearch）\n",
    "- より複雑なアーキテクチャの実験\n",
    "- Kaggleコンペでの実践\n",
    "- PyTorchでのカスタムアーキテクチャ\n",
    "- TabNet、FT-Transformerなど専用モデルの学習"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
