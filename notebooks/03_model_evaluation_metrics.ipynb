{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Model Evaluation Metrics\n",
    "\n",
    "Learn how to properly evaluate machine learning models.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand regression metrics (MSE, RMSE, MAE, R²)\n",
    "- Understand classification metrics (Accuracy, Precision, Recall, F1, ROC-AUC)\n",
    "- Visualize confusion matrices\n",
    "- Plot learning curves and validation curves\n",
    "- Use cross-validation effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, learning_curve, validation_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    # Regression metrics\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    # Classification metrics\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve, auc\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression data\n",
    "X, y = make_regression(n_samples=500, n_features=10, noise=20, random_state=42)\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model = MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Regression Metrics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean Squared Error (MSE):     {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error:      {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE):    {mae:.4f}\")\n",
    "print(f\"R² Score:                     {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot: Predicted vs Actual\n",
    "axes[0].scatter(y_test, y_pred, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual')\n",
    "axes[0].set_ylabel('Predicted')\n",
    "axes[0].set_title(f'Predicted vs Actual (R² = {r2:.3f})')\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred\n",
    "axes[1].scatter(y_pred, residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residual Plot')\n",
    "\n",
    "# Residual distribution\n",
    "axes[2].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[2].axvline(x=0, color='r', linestyle='--')\n",
    "axes[2].set_xlabel('Residual')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title(f'Residual Distribution (MAE = {mae:.2f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Explanations\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "|--------|---------|----------------|\n",
    "| MSE | Σ(y - ŷ)² / n | Penalizes large errors more |\n",
    "| RMSE | √MSE | Same units as target |\n",
    "| MAE | Σ|y - ŷ| / n | Average absolute error |\n",
    "| R² | 1 - SS_res/SS_tot | Variance explained (0-1) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=10,\n",
    "    n_classes=2, weights=[0.7, 0.3],  # Imbalanced\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "y_prob = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f\"Class distribution in test set: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(\"Classification Metrics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:     {accuracy:.4f}\")\n",
    "print(f\"Precision:    {precision:.4f}\")\n",
    "print(f\"Recall:       {recall:.4f}\")\n",
    "print(f\"F1 Score:     {f1:.4f}\")\n",
    "print(f\"ROC-AUC:      {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Normalized\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract values\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nTrue Negatives: {tn}, False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}, True Positives: {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Explanations\n",
    "\n",
    "| Metric | Formula | Use When |\n",
    "|--------|---------|----------|\n",
    "| Accuracy | (TP+TN)/(TP+TN+FP+FN) | Balanced classes |\n",
    "| Precision | TP/(TP+FP) | Cost of false positives high |\n",
    "| Recall | TP/(TP+FN) | Cost of false negatives high |\n",
    "| F1 | 2·(P·R)/(P+R) | Balance precision & recall |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate curves\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_prob)\n",
    "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, 'b-', lw=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', lw=1, label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[1].plot(recall_curve, precision_curve, 'b-', lw=2, label=f'PR (AUC = {pr_auc:.3f})')\n",
    "baseline = np.sum(y_test) / len(y_test)\n",
    "axes[1].axhline(y=baseline, color='k', linestyle='--', label=f'Baseline ({baseline:.2f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend(loc='lower left')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Metrics vs threshold\n",
    "thresholds = np.linspace(0.1, 0.9, 50)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_prob >= thresh).astype(int)\n",
    "    precisions.append(precision_score(y_test, y_pred_thresh, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred_thresh, zero_division=0))\n",
    "    f1s.append(f1_score(y_test, y_pred_thresh, zero_division=0))\n",
    "\n",
    "axes[0].plot(thresholds, precisions, 'b-', label='Precision')\n",
    "axes[0].plot(thresholds, recalls, 'r-', label='Recall')\n",
    "axes[0].plot(thresholds, f1s, 'g-', label='F1')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Metrics vs Classification Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = np.argmax(f1s)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "axes[0].axvline(x=optimal_threshold, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Probability distribution\n",
    "axes[1].hist(y_prob[y_test == 0], bins=30, alpha=0.5, label='Class 0', color='blue')\n",
    "axes[1].hist(y_prob[y_test == 1], bins=30, alpha=0.5, label='Class 1', color='red')\n",
    "axes[1].axvline(x=0.5, color='k', linestyle='--', label='Threshold=0.5')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Probability Distribution by Class')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal threshold (max F1): {optimal_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-class Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multi-class data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=10,\n",
    "    n_classes=4, n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "clf_multi = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "clf_multi.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_multi = clf_multi.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Test set distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class metrics\n",
    "print(\"Multi-class Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_multi))\n",
    "\n",
    "# Different averaging methods\n",
    "print(\"\\nF1 Scores with different averaging:\")\n",
    "for avg in ['micro', 'macro', 'weighted']:\n",
    "    f1_avg = f1_score(y_test, y_pred_multi, average=avg)\n",
    "    print(f\"  {avg:8s}: {f1_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class confusion matrix\n",
    "cm_multi = confusion_matrix(y_test, y_pred_multi)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Multi-class Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X, y = make_classification(n_samples=500, n_features=20, n_informative=10,\n",
    "                          n_classes=2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Cross-validation with different metrics\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42)\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "results = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    scores = cross_val_score(clf, X_scaled, y, cv=5, scoring=metric)\n",
    "    results[metric] = scores\n",
    "    print(f\"{metric:12s}: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "positions = np.arange(len(metrics))\n",
    "bp = ax.boxplot([results[m] for m in metrics], positions=positions, widths=0.6)\n",
    "\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('5-Fold Cross-Validation Scores')\n",
    "\n",
    "# Add mean markers\n",
    "means = [results[m].mean() for m in metrics]\n",
    "ax.scatter(positions, means, marker='D', color='red', s=50, zorder=3, label='Mean')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curve\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    clf, X_scaled, y,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "test_mean = test_scores.mean(axis=1)\n",
    "test_std = test_scores.std(axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, 'o-', color='green', label='Cross-validation score')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- High training, low CV → Overfitting (need more data or regularization)\")\n",
    "print(\"- Both low → Underfitting (need more complex model)\")\n",
    "print(\"- Both converging high → Good fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve for alpha (regularization)\n",
    "param_range = np.logspace(-5, 1, 10)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42),\n",
    "    X_scaled, y,\n",
    "    param_name='alpha',\n",
    "    param_range=param_range,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "test_mean = test_scores.mean(axis=1)\n",
    "test_std = test_scores.std(axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.semilogx(param_range, train_mean, 'o-', color='blue', label='Training score')\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "\n",
    "plt.semilogx(param_range, test_mean, 'o-', color='green', label='Cross-validation score')\n",
    "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
    "\n",
    "plt.xlabel('Alpha (Regularization)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Curve: Effect of Regularization')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal alpha\n",
    "optimal_alpha = param_range[np.argmax(test_mean)]\n",
    "print(f\"Optimal alpha: {optimal_alpha:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve for hidden layer size\n",
    "hidden_sizes = [(10,), (25,), (50,), (100,), (200,)]\n",
    "\n",
    "train_scores_list = []\n",
    "test_scores_list = []\n",
    "\n",
    "for size in hidden_sizes:\n",
    "    clf = MLPClassifier(hidden_layer_sizes=size, max_iter=500, random_state=42)\n",
    "    scores = cross_val_score(clf, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Also get training scores\n",
    "    clf.fit(X_scaled, y)\n",
    "    train_acc = clf.score(X_scaled, y)\n",
    "    \n",
    "    train_scores_list.append(train_acc)\n",
    "    test_scores_list.append(scores.mean())\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "x_labels = [str(s[0]) for s in hidden_sizes]\n",
    "x_pos = np.arange(len(hidden_sizes))\n",
    "\n",
    "plt.plot(x_pos, train_scores_list, 'o-', color='blue', label='Training score')\n",
    "plt.plot(x_pos, test_scores_list, 'o-', color='green', label='CV score')\n",
    "\n",
    "plt.xticks(x_pos, x_labels)\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of Network Size on Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Compare with cross-validation\n",
    "results = {}\n",
    "print(\"Model Comparison (5-Fold CV):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    results[name] = scores\n",
    "    print(f\"{name:25s}: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "names = list(results.keys())\n",
    "positions = np.arange(len(names))\n",
    "\n",
    "bp = ax.boxplot([results[name] for name in names], positions=positions, widths=0.6)\n",
    "\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(names, rotation=15)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Comparison: 5-Fold Cross-Validation')\n",
    "\n",
    "# Add mean markers\n",
    "means = [results[name].mean() for name in names]\n",
    "ax.scatter(positions, means, marker='D', color='red', s=50, zorder=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "### Regression Metrics\n",
    "- **MSE/RMSE**: Penalize large errors\n",
    "- **MAE**: Average absolute error\n",
    "- **R²**: Variance explained\n",
    "\n",
    "### Classification Metrics\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: True positives / predicted positives\n",
    "- **Recall**: True positives / actual positives\n",
    "- **F1**: Harmonic mean of precision and recall\n",
    "- **ROC-AUC**: Area under ROC curve\n",
    "\n",
    "### Model Evaluation Tools\n",
    "- **Confusion Matrix**: Detailed error analysis\n",
    "- **Cross-Validation**: Robust performance estimation\n",
    "- **Learning Curves**: Diagnose overfitting/underfitting\n",
    "- **Validation Curves**: Tune hyperparameters\n",
    "\n",
    "### Key Takeaways\n",
    "- Choose metrics based on your problem requirements\n",
    "- Use cross-validation for robust estimates\n",
    "- Learning curves help diagnose model issues\n",
    "- Always consider class imbalance when choosing metrics\n",
    "\n",
    "### Next Steps\n",
    "Continue to **Notebook 04** to learn about linear models with parameter simulation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
