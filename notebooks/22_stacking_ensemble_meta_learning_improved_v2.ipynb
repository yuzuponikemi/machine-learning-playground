{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. Stackingアンサンブル - メタ学習 (Stacking Ensemble - Meta Learning)\n",
    "\n",
    "## 概要\n",
    "複数のモデルを組み合わせて最強の予測モデルを構築する手法を学びます。\n",
    "\n",
    "## 学習目標\n",
    "- アンサンブル学習の種類を理解できる\n",
    "- Stackingの仕組みを理解できる\n",
    "- 多層Stackingを実装できる\n",
    "- Out-of-Foldの重要性を理解できる\n",
    "- 本番環境でStackingを実装できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 必要なライブラリのインポート\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer, make_classification\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression, Ridge, RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (\n    RandomForestClassifier, GradientBoostingClassifier,\n    VotingClassifier, StackingClassifier\n)\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 設定\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.rcParams['axes.unicode_minus'] = False\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. アンサンブル学習とは\n",
    "\n",
    "### 「三人寄れば文殊の知恵」\n",
    "\n",
    "複数のモデルを組み合わせることで、単一モデルより高い性能を達成します。\n",
    "\n",
    "### アンサンブルの種類\n",
    "\n",
    "1. **Bagging（バギング）**\n",
    "   - データをランダムサンプリング\n",
    "   - 並列に学習\n",
    "   - 多数決や平均で統合\n",
    "   - 例: Random Forest\n",
    "\n",
    "2. **Boosting（ブースティング）**\n",
    "   - 逐次的に学習\n",
    "   - 前のモデルの誤りを修正\n",
    "   - 加重平均で統合\n",
    "   - 例: Gradient Boosting、XGBoost\n",
    "\n",
    "3. **Stacking（スタッキング）**\n",
    "   - 複数の異なるモデルを使用\n",
    "   - メタモデルで統合\n",
    "   - 最も柔軟で強力\n",
    "   - Kaggleで人気"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stackingの仕組み\n",
    "\n",
    "### 基本構造\n",
    "\n",
    "```\n",
    "入力データ\n",
    "    ↓\n",
    "┌────────┬────────┬────────┐\n",
    "│Model 1 │Model 2 │Model 3 │ ← Base Models (Level 0)\n",
    "└────────┴────────┴────────┘\n",
    "    ↓        ↓        ↓\n",
    "  pred1    pred2    pred3\n",
    "    └────────┴────────┘\n",
    "           ↓\n",
    "    ┌──────────┐\n",
    "    │Meta Model│ ← Meta Learner (Level 1)\n",
    "    └──────────┘\n",
    "         ↓\n",
    "    最終予測\n",
    "```\n",
    "\n",
    "### 重要なポイント\n",
    "\n",
    "1. **多様性**\n",
    "   - 異なるタイプのモデルを使う\n",
    "   - 異なる特徴量セットを使う\n",
    "   - 異なるハイパーパラメータを使う\n",
    "\n",
    "2. **Out-of-Fold予測**\n",
    "   - 訓練データの過学習を防ぐ\n",
    "   - K-Foldで予測を生成\n",
    "   - メタモデルの汎化性能を保つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット読み込み\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"訓練データ: {X_train.shape}\")\n",
    "print(f\"テストデータ: {X_test.shape}\")\n",
    "print(f\"クラス分布: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Voting vs Stacking\n",
    "\n",
    "### まずはVotingから"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベースモデルの定義\n",
    "models = {\n",
    "    'Logistic': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# 個別モデルの性能\n",
    "print(\"個別モデルの性能:\")\n",
    "print(\"=\" * 50)\n",
    "individual_scores = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    individual_scores[name] = scores.mean()\n",
    "    print(f\"{name:20s}: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[(name, model) for name, model in models.items()],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[(name, model) for name, model in models.items()],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# 評価\n",
    "print(\"\\nVoting Classifier:\")\n",
    "print(\"=\" * 50)\n",
    "for name, model in [('Hard Voting', voting_hard), ('Soft Voting', voting_soft)]:\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"{name:20s}: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Classifier\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[(name, model) for name, model in models.items()],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5  # Out-of-fold予測用\n",
    ")\n",
    "\n",
    "# 評価\n",
    "scores = cross_val_score(stacking, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"\\nStacking Classifier:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"CV Score: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "# テストデータでの評価\n",
    "stacking.fit(X_train, y_train)\n",
    "test_score = stacking.score(X_test, y_test)\n",
    "print(f\"Test Score: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の比較\n",
    "all_scores = {**individual_scores,\n",
    "              'Voting (Hard)': cross_val_score(voting_hard, X_train, y_train, cv=5).mean(),\n",
    "              'Voting (Soft)': cross_val_score(voting_soft, X_train, y_train, cv=5).mean(),\n",
    "              'Stacking': scores.mean()}\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(all_scores)), list(all_scores.values()), color=['C0']*4 + ['C1', 'C2', 'C3'])\n",
    "plt.xticks(range(len(all_scores)), list(all_scores.keys()), rotation=45, ha='right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Comparison')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nベストモデル:\", max(all_scores, key=all_scores.get))\n",
    "print(f\"スコア: {max(all_scores.values()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. カスタムStacking実装\n",
    "\n",
    "### Out-of-Fold予測の仕組み\n",
    "\n",
    "より細かい制御のため、自分でStackingを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oof_predictions(models, X_train, y_train, X_test, n_folds=5):\n",
    "    \"\"\"\n",
    "    Out-of-Fold予測を生成\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : dict\n",
    "        モデル名とモデルのディクショナリ\n",
    "    X_train, y_train : array\n",
    "        訓練データ\n",
    "    X_test : array\n",
    "        テストデータ\n",
    "    n_folds : int\n",
    "        Fold数\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_meta : array\n",
    "        訓練用メタ特徴量\n",
    "    test_meta : array\n",
    "        テスト用メタ特徴量\n",
    "    \"\"\"\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # メタ特徴量の初期化\n",
    "    train_meta = np.zeros((X_train.shape[0], len(models)))\n",
    "    test_meta = np.zeros((X_test.shape[0], len(models)))\n",
    "    \n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        print(f\"Processing {name}...\")\n",
    "        \n",
    "        test_preds_folds = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "            # Foldごとに学習\n",
    "            X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "            \n",
    "            # 学習\n",
    "            model.fit(X_tr, y_tr)\n",
    "            \n",
    "            # Out-of-fold予測（訓練データ）\n",
    "            train_meta[val_idx, i] = model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            # テストデータ予測\n",
    "            test_preds_folds.append(model.predict_proba(X_test)[:, 1])\n",
    "        \n",
    "        # テストデータ予測の平均\n",
    "        test_meta[:, i] = np.mean(test_preds_folds, axis=0)\n",
    "    \n",
    "    return train_meta, test_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-fold予測の生成\n",
    "print(\"Out-of-Fold予測を生成中...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_meta, test_meta = create_oof_predictions(\n",
    "    models, X_train, y_train, X_test, n_folds=5\n",
    ")\n",
    "\n",
    "print(f\"\\n訓練メタ特徴量: {train_meta.shape}\")\n",
    "print(f\"テストメタ特徴量: {test_meta.shape}\")\n",
    "\n",
    "# メタ特徴量の確認\n",
    "meta_df = pd.DataFrame(train_meta, columns=models.keys())\n",
    "print(\"\\nメタ特徴量のサンプル:\")\n",
    "print(meta_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メタモデルの学習\n",
    "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "meta_model.fit(train_meta, y_train)\n",
    "\n",
    "# 予測\n",
    "train_pred = meta_model.predict(train_meta)\n",
    "test_pred = meta_model.predict(test_meta)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "test_accuracy = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(f\"訓練精度: {train_accuracy:.4f}\")\n",
    "print(f\"テスト精度: {test_accuracy:.4f}\")\n",
    "\n",
    "# メタモデルの係数（各ベースモデルの重要度）\n",
    "print(\"\\nメタモデルの係数（ベースモデルの重み）:\")\n",
    "for name, coef in zip(models.keys(), meta_model.coef_[0]):\n",
    "    print(f\"  {name:20s}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 多層Stacking\n",
    "\n",
    "### Level 2のStacking\n",
    "\n",
    "さらに性能を上げるため、複数層のStackingを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Level 0: より多様なモデル\nlevel0_models = {\n    'LR': LogisticRegression(max_iter=1000, random_state=42),\n    'RF1': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n    'RF2': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=43),\n    'GB1': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n    'GB2': GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, random_state=43),\n    'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n}\n\n# Level 1: Level 0の予測を統合\nlevel1_models = {\n    'LR2': LogisticRegression(max_iter=1000, random_state=44),\n    'RF': RandomForestClassifier(n_estimators=50, random_state=42)\n}\n\nprint(\"多層Stacking構築中...\")\nprint(\"=\" * 50)\n\n# Level 0の予測\ntrain_level0, test_level0 = create_oof_predictions(\n    level0_models, X_train, y_train, X_test, n_folds=5\n)\n\n# Level 1の予測\ntrain_level1, test_level1 = create_oof_predictions(\n    level1_models, train_level0, y_train, test_level0, n_folds=5\n)\n\n# 最終メタモデル\nfinal_meta = LogisticRegression(max_iter=1000, random_state=42)\nfinal_meta.fit(train_level1, y_train)\n\nfinal_pred = final_meta.predict(test_level1)\nfinal_accuracy = accuracy_score(y_test, final_pred)\n\nprint(f\"\\n多層Stackingテスト精度: {final_accuracy:.4f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 本番実装\n",
    "\n",
    "### 再利用可能なStackingクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStacking:\n",
    "    \"\"\"\n",
    "    カスタムStackingクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, base_models, meta_model, n_folds=5, use_proba=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_models : list of tuples\n",
    "            (name, model)のリスト\n",
    "        meta_model : estimator\n",
    "            メタモデル\n",
    "        n_folds : int\n",
    "            Fold数\n",
    "        use_proba : bool\n",
    "            確率予測を使うか（分類の場合）\n",
    "        \"\"\"\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "        self.use_proba = use_proba\n",
    "        self.trained_base_models = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"学習\"\"\"\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        # メタ特徴量の初期化\n",
    "        meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        # 各ベースモデルの学習\n",
    "        for i, (name, model) in enumerate(self.base_models):\n",
    "            print(f\"Training {name}...\")\n",
    "            \n",
    "            for train_idx, val_idx in kfold.split(X):\n",
    "                X_tr, X_val = X[train_idx], X[val_idx]\n",
    "                y_tr = y[train_idx]\n",
    "                \n",
    "                # クローンして学習\n",
    "                from sklearn.base import clone\n",
    "                model_clone = clone(model)\n",
    "                model_clone.fit(X_tr, y_tr)\n",
    "                \n",
    "                # Out-of-fold予測\n",
    "                if self.use_proba and hasattr(model_clone, 'predict_proba'):\n",
    "                    meta_features[val_idx, i] = model_clone.predict_proba(X_val)[:, 1]\n",
    "                else:\n",
    "                    meta_features[val_idx, i] = model_clone.predict(X_val)\n",
    "            \n",
    "            # 全データで最終学習\n",
    "            model.fit(X, y)\n",
    "            self.trained_base_models.append((name, model))\n",
    "        \n",
    "        # メタモデルの学習\n",
    "        print(\"Training meta model...\")\n",
    "        self.meta_model.fit(meta_features, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"予測\"\"\"\n",
    "        # ベースモデルの予測\n",
    "        meta_features = np.zeros((X.shape[0], len(self.trained_base_models)))\n",
    "        \n",
    "        for i, (name, model) in enumerate(self.trained_base_models):\n",
    "            if self.use_proba and hasattr(model, 'predict_proba'):\n",
    "                meta_features[:, i] = model.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                meta_features[:, i] = model.predict(X)\n",
    "        \n",
    "        # メタモデルで最終予測\n",
    "        return self.meta_model.predict(meta_features)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"確率予測\"\"\"\n",
    "        meta_features = np.zeros((X.shape[0], len(self.trained_base_models)))\n",
    "        \n",
    "        for i, (name, model) in enumerate(self.trained_base_models):\n",
    "            if self.use_proba and hasattr(model, 'predict_proba'):\n",
    "                meta_features[:, i] = model.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                meta_features[:, i] = model.predict(X)\n",
    "        \n",
    "        return self.meta_model.predict_proba(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カスタムStackingの使用\n",
    "custom_stacking = CustomStacking(\n",
    "    base_models=[(name, model) for name, model in models.items()],\n",
    "    meta_model=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    n_folds=5\n",
    ")\n",
    "\n",
    "print(\"カスタムStacking学習中...\")\n",
    "print(\"=\" * 50)\n",
    "custom_stacking.fit(X_train, y_train)\n",
    "\n",
    "# 予測と評価\n",
    "y_pred_custom = custom_stacking.predict(X_test)\n",
    "custom_accuracy = accuracy_score(y_test, y_pred_custom)\n",
    "\n",
    "print(f\"\\nカスタムStackingテスト精度: {custom_accuracy:.4f}\")\n",
    "print(\"\\n分類レポート:\")\n",
    "print(classification_report(y_test, y_pred_custom, target_names=cancer.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. まとめ\n",
    "\n",
    "### 本ノートブックで学んだこと\n",
    "\n",
    "1. **アンサンブルの種類**\n",
    "   - Bagging、Boosting、Stacking\n",
    "   - それぞれの特徴と使い分け\n",
    "\n",
    "2. **Stackingの基礎**\n",
    "   - ベースモデルとメタモデル\n",
    "   - 多様性の重要性\n",
    "   - Voting vs Stacking\n",
    "\n",
    "3. **Out-of-Fold予測**\n",
    "   - 過学習の防止\n",
    "   - K-Foldによる予測生成\n",
    "   - 汎化性能の保持\n",
    "\n",
    "4. **カスタム実装**\n",
    "   - 柔軟な制御\n",
    "   - 多層Stacking\n",
    "   - 再利用可能なクラス\n",
    "\n",
    "5. **本番実装**\n",
    "   - CustomStackingクラス\n",
    "   - fit/predict インターフェース\n",
    "   - プロダクション対応\n",
    "\n",
    "### Stackingを使うべきとき\n",
    "\n",
    "- ✅ 最高の性能が必要（Kaggleなど）\n",
    "- ✅ 計算リソースに余裕がある\n",
    "- ✅ 複数の強力なベースモデルがある\n",
    "- ✅ モデルの多様性を活かしたい\n",
    "\n",
    "### 注意点\n",
    "\n",
    "- ⚠️ 計算コストが高い\n",
    "- ⚠️ 解釈性が低い\n",
    "- ⚠️ 過学習のリスク（Out-of-Fold必須）\n",
    "- ⚠️ デプロイの複雑さ\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "- 実際のKaggleコンペでStackingを活用\n",
    "- より高度なStacking戦略を探求\n",
    "- ニューラルネットワークとの組み合わせ\n",
    "- AutoMLツールの活用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}