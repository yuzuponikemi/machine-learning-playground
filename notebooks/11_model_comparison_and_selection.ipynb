{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 11: Model Comparison and Selection\n",
    "\n",
    "Systematically compare models and select the best one.\n",
    "\n",
    "## Learning Objectives\n",
    "- Compare multiple models fairly\n",
    "- Use statistical tests for significance\n",
    "- Understand ensemble methods\n",
    "- Make informed model selection decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=12,\n",
    "    n_redundant=4,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Define Models to Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with reasonable defaults\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"Comparing {len(models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Cross-Validation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for all models\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "results = {}\n",
    "cv_scores = {}\n",
    "\n",
    "print(\"Cross-Validation Results (10-fold):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    cv_scores[name] = scores\n",
    "    \n",
    "    results[name] = {\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'min': scores.min(),\n",
    "        'max': scores.max()\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:25s}: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "# Sort by mean score\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
    "\n",
    "print(\"\\nRanking:\")\n",
    "for rank, (name, res) in enumerate(sorted_results, 1):\n",
    "    print(f\"{rank}. {name}: {res['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "names = list(cv_scores.keys())\n",
    "scores_list = [cv_scores[name] for name in names]\n",
    "\n",
    "bp = axes[0].boxplot(scores_list, labels=names, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Comparison: CV Scores')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Bar plot with error bars\n",
    "means = [results[name]['mean'] for name in names]\n",
    "stds = [results[name]['std'] for name in names]\n",
    "\n",
    "x_pos = np.arange(len(names))\n",
    "axes[1].bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(names, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Mean CV Score with Std Dev')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise t-tests between models\n",
    "model_names = list(cv_scores.keys())\n",
    "n_models = len(model_names)\n",
    "\n",
    "# Create p-value matrix\n",
    "p_values = np.ones((n_models, n_models))\n",
    "\n",
    "for i in range(n_models):\n",
    "    for j in range(i+1, n_models):\n",
    "        # Paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(cv_scores[model_names[i]], \n",
    "                                          cv_scores[model_names[j]])\n",
    "        p_values[i, j] = p_value\n",
    "        p_values[j, i] = p_value\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(p_values, dtype=bool), k=1)\n",
    "sns.heatmap(p_values, annot=True, fmt='.3f', cmap='RdYlGn_r',\n",
    "            xticklabels=model_names, yticklabels=model_names,\n",
    "            mask=mask, vmin=0, vmax=0.1)\n",
    "plt.title('Pairwise t-test p-values\\n(Green = significant difference at Î±=0.05)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"p < 0.05: Statistically significant difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and evaluate on test set\n",
    "test_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    test_results.append({\n",
    "        'Model': name,\n",
    "        'Train': train_score,\n",
    "        'Test': test_score,\n",
    "        'Overfit': train_score - test_score\n",
    "    })\n",
    "\n",
    "df_test = pd.DataFrame(test_results).sort_values('Test', ascending=False)\n",
    "print(\"Test Set Results:\")\n",
    "print(df_test.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier (Hard Voting)\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "        ('mlp', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42))\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Voting Classifier (Soft Voting)\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "        ('mlp', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Evaluate ensembles\n",
    "print(\"Ensemble Methods:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, model in [('Hard Voting', voting_hard), ('Soft Voting', voting_soft)]:\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  CV Score: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "    print(f\"  Test Score: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Classifier\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "        ('svm', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "scores = cross_val_score(stacking, X_train_scaled, y_train, cv=5)\n",
    "stacking.fit(X_train_scaled, y_train)\n",
    "test_score = stacking.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\nStacking Classifier:\")\n",
    "print(f\"  CV Score: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "print(f\"  Test Score: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Final Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison\n",
    "all_models = {\n",
    "    **models,\n",
    "    'Voting (Soft)': voting_soft,\n",
    "    'Stacking': stacking\n",
    "}\n",
    "\n",
    "final_comparison = []\n",
    "\n",
    "for name, model in all_models.items():\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    if not hasattr(model, 'classes_'):\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    final_comparison.append({\n",
    "        'Model': name,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std(),\n",
    "        'Test': test_score\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(final_comparison).sort_values('Test', ascending=False)\n",
    "print(\"Final Model Comparison:\")\n",
    "print(df_final.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_model_name = df_final.iloc[0]['Model']\n",
    "best_model = all_models[best_model_name]\n",
    "\n",
    "print(f\"\\nSelected Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {df_final.iloc[0]['Test']:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "### Model Comparison\n",
    "- Use cross-validation for fair comparison\n",
    "- Consider both mean and variance of scores\n",
    "- Statistical tests for significance\n",
    "\n",
    "### Ensemble Methods\n",
    "- **Voting**: Combine predictions (hard/soft)\n",
    "- **Stacking**: Use meta-learner\n",
    "- Often outperform individual models\n",
    "\n",
    "### Selection Criteria\n",
    "- Performance (accuracy, F1, etc.)\n",
    "- Stability (low variance across folds)\n",
    "- Overfitting (train-test gap)\n",
    "- Interpretability and speed\n",
    "\n",
    "### Next Steps\n",
    "Continue to **Notebook 12** for the complete ML pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
