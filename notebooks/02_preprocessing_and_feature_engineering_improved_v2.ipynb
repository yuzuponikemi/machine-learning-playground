{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. データ前処理と特徴量エンジニアリング (Preprocessing and Feature Engineering)\n",
    "\n",
    "## 概要\n",
    "生のデータを機械学習モデルが扱える形式に変換する方法を学びます。\n",
    "\n",
    "## 学習目標\n",
    "- データのスケーリングと正規化ができる\n",
    "- 欠損値を適切に処理できる\n",
    "- カテゴリカル変数をエンコードできる\n",
    "- 新しい特徴量を作成できる\n",
    "- 次元削減の手法を理解できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler, \n",
    "    LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# 設定\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データスケーリング\n",
    "\n",
    "### なぜスケーリングが必要か？\n",
    "\n",
    "機械学習アルゴリズムの多くは、特徴量のスケールに敏感です。\n",
    "\n",
    "**スケーリングが重要な理由:**\n",
    "- ニューラルネットワーク: 学習の収束速度に影響\n",
    "- SVM: 距離ベースの計算に影響\n",
    "- k-NN: ユークリッド距離の計算に影響\n",
    "- PCA: 分散が大きい特徴量に偏る\n",
    "\n",
    "### 主なスケーリング手法\n",
    "\n",
    "1. **StandardScaler（標準化）**\n",
    "   - 平均0、標準偏差1に変換\n",
    "   - 式: z = (x - μ) / σ\n",
    "   - 最もよく使われる\n",
    "\n",
    "2. **MinMaxScaler（正規化）**\n",
    "   - 0〜1の範囲に変換\n",
    "   - 式: x' = (x - min) / (max - min)\n",
    "   - 外れ値に敏感\n",
    "\n",
    "3. **RobustScaler（ロバスト標準化）**\n",
    "   - 中央値と四分位範囲を使用\n",
    "   - 外れ値に頑健"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータ生成\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'age': np.random.randint(20, 70, 100),\n",
    "    'salary': np.random.randint(30000, 150000, 100),\n",
    "    'score': np.random.uniform(0, 100, 100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 外れ値を追加\n",
    "df.loc[0, 'salary'] = 500000\n",
    "df.loc[1, 'age'] = 95\n",
    "\n",
    "print(\"元のデータの統計:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3つのスケーリング手法を比較\n",
    "scalers = {\n",
    "    'Original': None,\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, scaler) in enumerate(scalers.items()):\n",
    "    if scaler is None:\n",
    "        data_scaled = df.values\n",
    "    else:\n",
    "        data_scaled = scaler.fit_transform(df.values)\n",
    "    \n",
    "    # 箱ひげ図\n",
    "    axes[idx].boxplot(data_scaled, labels=df.columns)\n",
    "    axes[idx].set_title(f'{name}')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スケーリングの注意点\n",
    "\n",
    "**重要: データリークを防ぐ**\n",
    "\n",
    "```python\n",
    "# ❌ 間違い: 全データでfit\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "\n",
    "# ✅ 正しい: 訓練データのみでfit\n",
    "X_train, X_test = train_test_split(X)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # fitしない！\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 欠損値の処理\n",
    "\n",
    "### 欠損値とは\n",
    "\n",
    "データが記録されていない、または利用できない状態です。\n",
    "\n",
    "**欠損値の原因:**\n",
    "- データ収集時のエラー\n",
    "- センサーの故障\n",
    "- 回答者が質問をスキップ\n",
    "- データ統合時の不一致\n",
    "\n",
    "### 欠損値の処理方法\n",
    "\n",
    "1. **削除**\n",
    "   - 欠損が少ない場合\n",
    "   - データ量が十分にある場合\n",
    "\n",
    "2. **補完（Imputation）**\n",
    "   - 平均値/中央値/最頻値\n",
    "   - k-NN補完\n",
    "   - 予測モデルでの補完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値を含むデータ生成\n",
    "np.random.seed(42)\n",
    "df_missing = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5, 6, np.nan, 8, 9, 10],\n",
    "    'B': [10, np.nan, 30, 40, np.nan, 60, 70, 80, 90, 100],\n",
    "    'C': [100, 200, 300, np.nan, 500, np.nan, 700, 800, 900, 1000]\n",
    "})\n",
    "\n",
    "print(\"欠損値の状況:\")\n",
    "print(df_missing.isnull().sum())\n",
    "print(f\"\\n欠損率: {df_missing.isnull().sum().sum() / df_missing.size * 100:.1f}%\")\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(df_missing.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Values Pattern')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 様々な補完方法の比較\n",
    "# 平均値補完\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_mean = pd.DataFrame(\n",
    "    imputer_mean.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "# 中央値補完\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "df_median = pd.DataFrame(\n",
    "    imputer_median.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "# k-NN補完\n",
    "imputer_knn = KNNImputer(n_neighbors=3)\n",
    "df_knn = pd.DataFrame(\n",
    "    imputer_knn.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "# 比較\n",
    "print(\"列Aの欠損値（元: NaN）:\")\n",
    "print(f\"  平均値補完: {df_mean.loc[2, 'A']:.2f}\")\n",
    "print(f\"  中央値補完: {df_median.loc[2, 'A']:.2f}\")\n",
    "print(f\"  k-NN補完: {df_knn.loc[2, 'A']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. カテゴリカル変数のエンコーディング\n",
    "\n",
    "### カテゴリカル変数とは\n",
    "\n",
    "数値ではなく、カテゴリー（ラベル）で表現される変数です。\n",
    "\n",
    "**例:**\n",
    "- 名義変数: 色（赤、青、緑）、性別（男、女）\n",
    "- 順序変数: 評価（低、中、高）、学年（1年、2年、3年）\n",
    "\n",
    "### エンコーディング手法\n",
    "\n",
    "1. **Label Encoding**\n",
    "   - 各カテゴリーに整数を割り当て\n",
    "   - 順序変数に適している\n",
    "   - 順序関係のない変数には不適切\n",
    "\n",
    "2. **One-Hot Encoding**\n",
    "   - 各カテゴリーを二値変数に変換\n",
    "   - 名義変数に適している\n",
    "   - 次元数が増加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリカルデータのサンプル\n",
    "df_cat = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'red', 'blue', 'green'],\n",
    "    'size': ['S', 'M', 'L', 'M', 'S', 'L'],\n",
    "    'grade': ['A', 'B', 'C', 'A', 'B', 'C']\n",
    "})\n",
    "\n",
    "print(\"元のデータ:\")\n",
    "print(df_cat)\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "df_label = df_cat.copy()\n",
    "df_label['color_encoded'] = le.fit_transform(df_cat['color'])\n",
    "\n",
    "print(\"\\nLabel Encoding:\")\n",
    "print(df_label[['color', 'color_encoded']])\n",
    "\n",
    "# One-Hot Encoding\n",
    "df_onehot = pd.get_dummies(df_cat, columns=['color'], prefix='color')\n",
    "\n",
    "print(\"\\nOne-Hot Encoding:\")\n",
    "print(df_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 特徴量エンジニアリング\n",
    "\n",
    "### 特徴量エンジニアリングとは\n",
    "\n",
    "既存の特徴量から新しい特徴量を作成し、モデルの性能を向上させる技術です。\n",
    "\n",
    "### 一般的な手法\n",
    "\n",
    "1. **多項式特徴量**\n",
    "   - x² や x₁×x₂ などの項を追加\n",
    "   - 非線形関係を捉える\n",
    "\n",
    "2. **ビニング（離散化）**\n",
    "   - 連続値を区間に分割\n",
    "   - 例: 年齢 → 若年層/中年層/高年層\n",
    "\n",
    "3. **集約統計量**\n",
    "   - グループごとの平均、合計、最大値など\n",
    "\n",
    "4. **時間特徴量**\n",
    "   - 日付から年、月、曜日などを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 多項式特徴量の生成\n",
    "X_simple = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "print(\"元の特徴量:\")\n",
    "print(X_simple)\n",
    "print(f\"形状: {X_simple.shape}\")\n",
    "\n",
    "# 2次の多項式特徴量を生成\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_simple)\n",
    "\n",
    "print(\"\\n多項式特徴量（2次）:\")\n",
    "print(X_poly)\n",
    "print(f\"形状: {X_poly.shape}\")\n",
    "print(f\"\\n特徴量名: {poly.get_feature_names_out(['x1', 'x2'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実際の効果を確認\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 非線形データ生成\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.randn(100) * 0.1\n",
    "\n",
    "# 線形回帰（通常）\n",
    "lr_simple = LinearRegression()\n",
    "lr_simple.fit(X, y)\n",
    "y_pred_simple = lr_simple.predict(X)\n",
    "r2_simple = r2_score(y, y_pred_simple)\n",
    "\n",
    "# 線形回帰（多項式特徴量付き）\n",
    "poly = PolynomialFeatures(degree=10)\n",
    "X_poly = poly.fit_transform(X)\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_poly, y)\n",
    "y_pred_poly = lr_poly.predict(X_poly)\n",
    "r2_poly = r2_score(y, y_pred_poly)\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X, y, alpha=0.6)\n",
    "plt.plot(X, y_pred_simple, 'r-', linewidth=2, label='Linear')\n",
    "plt.title(f'Linear Regression (R²={r2_simple:.3f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X, y, alpha=0.6)\n",
    "plt.plot(X, y_pred_poly, 'r-', linewidth=2, label='Polynomial')\n",
    "plt.title(f'Polynomial Features (R²={r2_poly:.3f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 次元削減\n",
    "\n",
    "### 次元の呪い\n",
    "\n",
    "特徴量が多すぎると:\n",
    "- 計算コストが増大\n",
    "- 過学習のリスクが増加\n",
    "- データが疎になる\n",
    "\n",
    "### 次元削減の手法\n",
    "\n",
    "1. **特徴選択（Feature Selection）**\n",
    "   - 重要な特徴量のみを選択\n",
    "   - 解釈性が保たれる\n",
    "\n",
    "2. **特徴抽出（Feature Extraction）**\n",
    "   - 新しい特徴量の組み合わせを作成\n",
    "   - PCA、t-SNEなど"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCAによる次元削減\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Irisデータセット読み込み\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"元のデータ形状: {X.shape}\")\n",
    "\n",
    "# PCAで2次元に削減\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(f\"削減後のデータ形状: {X_pca.shape}\")\n",
    "print(f\"\\n累積寄与率: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "print(f\"各主成分の寄与率: {pca.explained_variance_ratio_}\")\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, target_name in enumerate(iris.target_names):\n",
    "    plt.scatter(X_pca[y==i, 0], X_pca[y==i, 1], label=target_name, alpha=0.7)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(1, 3), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Variance Explained')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 完全な前処理パイプライン\n",
    "\n",
    "### 実践例\n",
    "\n",
    "実際のワークフローで前処理をどのように組み込むかを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# データ生成\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# データ分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# パイプライン構築\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=10)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# クロスバリデーション\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "\n",
    "print(f\"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# 学習と評価\n",
    "pipeline.fit(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. まとめ\n",
    "\n",
    "### 本ノートブックで学んだこと\n",
    "\n",
    "1. **データスケーリング**\n",
    "   - StandardScaler、MinMaxScaler、RobustScaler\n",
    "   - データリークの防止\n",
    "\n",
    "2. **欠損値の処理**\n",
    "   - 削除 vs 補完\n",
    "   - 平均値、中央値、k-NN補完\n",
    "\n",
    "3. **カテゴリカルエンコーディング**\n",
    "   - Label Encoding\n",
    "   - One-Hot Encoding\n",
    "\n",
    "4. **特徴量エンジニアリング**\n",
    "   - 多項式特徴量\n",
    "   - 新しい特徴量の作成\n",
    "\n",
    "5. **次元削減**\n",
    "   - PCA\n",
    "   - 特徴選択\n",
    "\n",
    "6. **パイプライン**\n",
    "   - 前処理とモデルの統合\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "- Notebook 03でモデルの評価方法を学ぶ\n",
    "- 実際のデータセットで前処理を実践\n",
    "- より高度な特徴量エンジニアリング手法を探求"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
