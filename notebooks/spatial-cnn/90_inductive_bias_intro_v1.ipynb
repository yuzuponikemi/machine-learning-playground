{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90. 帰納バイアス入門：なぜCNNは画像を理解できるのか\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックでは、以下を学びます：\n",
    "\n",
    "1. **帰納バイアス（Inductive Bias）**とは何か\n",
    "2. **No Free Lunch定理**と帰納バイアスの必要性\n",
    "3. **CNNが持つ帰納バイアス**の概要\n",
    "4. **帰納バイアスと汎化**の関係\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. [帰納バイアスとは](#section1)\n",
    "2. [No Free Lunch定理](#section2)\n",
    "3. [CNNの帰納バイアス](#section3)\n",
    "4. [帰納バイアスと汎化性能](#section4)\n",
    "5. [まとめ](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch\n",
    "import japanize_matplotlib\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1. 帰納バイアスとは\n",
    "\n",
    "### 定義\n",
    "\n",
    "**帰納バイアス（Inductive Bias）**とは：\n",
    "\n",
    "> 学習アルゴリズムが、見たことのないデータに対して予測を行う際に用いる**仮定・制約**のこと\n",
    "\n",
    "### なぜ必要なのか？\n",
    "\n",
    "有限のデータから無限の可能性のある関数を学習するためには、何らかの「事前の仮定」が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_inductive_bias_concept():\n",
    "    \"\"\"帰納バイアスの概念を可視化\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # データ点\n",
    "    np.random.seed(42)\n",
    "    x_data = np.array([1, 2, 3, 5, 6])\n",
    "    y_data = np.array([2.1, 4.2, 5.8, 10.1, 12.0])\n",
    "    \n",
    "    x_plot = np.linspace(0, 7, 100)\n",
    "    \n",
    "    # 1. データのみ（帰納バイアスなし）\n",
    "    ax = axes[0]\n",
    "    ax.scatter(x_data, y_data, c='blue', s=100, zorder=5, label='観測データ')\n",
    "    ax.set_title('データのみ\\n（帰納バイアスなし）', fontsize=14)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.text(4, 3, 'x=4での予測は？\\n無限の可能性...', fontsize=11, \n",
    "           ha='center', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "    ax.axvline(x=4, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 7)\n",
    "    ax.set_ylim(0, 15)\n",
    "    \n",
    "    # 2. 線形モデル（「データは直線に従う」という帰納バイアス）\n",
    "    ax = axes[1]\n",
    "    ax.scatter(x_data, y_data, c='blue', s=100, zorder=5)\n",
    "    \n",
    "    # 線形フィット\n",
    "    coeffs = np.polyfit(x_data, y_data, 1)\n",
    "    y_linear = np.polyval(coeffs, x_plot)\n",
    "    ax.plot(x_plot, y_linear, 'g-', linewidth=2, label='線形フィット')\n",
    "    \n",
    "    # x=4での予測\n",
    "    y_pred = np.polyval(coeffs, 4)\n",
    "    ax.scatter([4], [y_pred], c='red', s=150, marker='*', zorder=6)\n",
    "    \n",
    "    ax.set_title('線形モデル\\n「直線に従う」という仮定', fontsize=14)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.text(4, y_pred + 1.5, f'予測: {y_pred:.1f}', fontsize=11, ha='center', color='red')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 7)\n",
    "    ax.set_ylim(0, 15)\n",
    "    \n",
    "    # 3. 高次多項式（異なる帰納バイアス）\n",
    "    ax = axes[2]\n",
    "    ax.scatter(x_data, y_data, c='blue', s=100, zorder=5)\n",
    "    \n",
    "    # 4次多項式フィット\n",
    "    coeffs_poly = np.polyfit(x_data, y_data, 4)\n",
    "    y_poly = np.polyval(coeffs_poly, x_plot)\n",
    "    ax.plot(x_plot, y_poly, 'r-', linewidth=2, label='4次多項式')\n",
    "    \n",
    "    # x=4での予測\n",
    "    y_pred_poly = np.polyval(coeffs_poly, 4)\n",
    "    ax.scatter([4], [y_pred_poly], c='red', s=150, marker='*', zorder=6)\n",
    "    \n",
    "    ax.set_title('高次多項式\\n「複雑な曲線」という仮定', fontsize=14)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.text(4, y_pred_poly - 2, f'予測: {y_pred_poly:.1f}', fontsize=11, ha='center', color='red')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 7)\n",
    "    ax.set_ylim(0, 15)\n",
    "    \n",
    "    plt.suptitle('帰納バイアス：異なる仮定 → 異なる予測', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"線形モデルの x=4 での予測: {y_pred:.2f}\")\n",
    "    print(f\"4次多項式の x=4 での予測: {y_pred_poly:.2f}\")\n",
    "    print(\"\\n→ 同じデータでも、帰納バイアスによって予測が異なる！\")\n",
    "\n",
    "visualize_inductive_bias_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 帰納バイアスの例\n",
    "\n",
    "| モデル | 帰納バイアス |\n",
    "|--------|------------|\n",
    "| 線形回帰 | データは線形関係に従う |\n",
    "| 決定木 | 特徴空間は軸に平行な境界で分割できる |\n",
    "| k-NN | 近いデータは同じクラスに属する |\n",
    "| **CNN** | 画像には局所パターンがあり、それは位置に依存しない |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## 2. No Free Lunch定理\n",
    "\n",
    "### 定理の内容\n",
    "\n",
    "**No Free Lunch定理**（Wolpert & Macready, 1997）：\n",
    "\n",
    "> すべての可能な問題に対して平均すると、どの学習アルゴリズムも同じ性能を示す\n",
    "\n",
    "つまり、「万能のアルゴリズム」は存在しない！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_no_free_lunch():\n",
    "    \"\"\"No Free Lunch定理の直感的説明\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 問題1: 線形データ\n",
    "    ax = axes[0]\n",
    "    x1 = np.linspace(0, 5, 20)\n",
    "    y1 = 2 * x1 + 1 + np.random.normal(0, 0.5, 20)\n",
    "    \n",
    "    ax.scatter(x1, y1, c='blue', s=50)\n",
    "    \n",
    "    # 線形フィット\n",
    "    coeffs = np.polyfit(x1, y1, 1)\n",
    "    ax.plot(x1, np.polyval(coeffs, x1), 'g-', linewidth=2, label='線形モデル ✓')\n",
    "    \n",
    "    ax.set_title('問題1: 線形関係\\n線形モデルが良い', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 問題2: 周期的データ\n",
    "    ax = axes[1]\n",
    "    x2 = np.linspace(0, 4*np.pi, 40)\n",
    "    y2 = np.sin(x2) + np.random.normal(0, 0.1, 40)\n",
    "    \n",
    "    ax.scatter(x2, y2, c='blue', s=50)\n",
    "    \n",
    "    # 線形は失敗\n",
    "    coeffs = np.polyfit(x2, y2, 1)\n",
    "    ax.plot(x2, np.polyval(coeffs, x2), 'r--', linewidth=2, alpha=0.5, label='線形モデル ✗')\n",
    "    \n",
    "    # sinが正解\n",
    "    ax.plot(x2, np.sin(x2), 'g-', linewidth=2, label='周期モデル ✓')\n",
    "    \n",
    "    ax.set_title('問題2: 周期的関係\\n線形モデルは失敗', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 問題3: ランダムデータ\n",
    "    ax = axes[2]\n",
    "    x3 = np.random.rand(30) * 5\n",
    "    y3 = np.random.rand(30) * 5\n",
    "    \n",
    "    ax.scatter(x3, y3, c='blue', s=50)\n",
    "    \n",
    "    ax.set_title('問題3: ランダム\\nどのモデルも同じ（予測不可能）', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('No Free Lunch: 問題に合った帰納バイアスが必要', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_no_free_lunch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定理の含意\n",
    "\n",
    "1. **特定の問題には、それに適した帰納バイアスを持つアルゴリズムが必要**\n",
    "2. **帰納バイアスが問題に合っていれば、少ないデータで良い汎化が可能**\n",
    "3. **帰納バイアスが問題に合っていなければ、いくらデータがあっても性能が出ない**\n",
    "\n",
    "→ CNNが画像認識に優れているのは、画像の性質に合った帰納バイアスを持っているから！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 3. CNNの帰納バイアス\n",
    "\n",
    "CNNは主に3つの帰納バイアスを持っています：\n",
    "\n",
    "1. **局所性（Locality）**: 近くのピクセルは関連性が高い\n",
    "2. **平行移動等変性（Translation Equivariance）**: 同じパターンは画像のどこにあっても同じ方法で処理される\n",
    "3. **階層性（Hierarchy）**: 単純な特徴から複雑な特徴を段階的に構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cnn_inductive_biases():\n",
    "    \"\"\"CNNの3つの帰納バイアスを可視化\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # 1. 局所性\n",
    "    ax = axes[0]\n",
    "    \n",
    "    # 7x7グリッド\n",
    "    for i in range(8):\n",
    "        ax.axhline(y=i, color='lightgray', linewidth=0.5)\n",
    "        ax.axvline(x=i, color='lightgray', linewidth=0.5)\n",
    "    \n",
    "    # 中心ピクセル\n",
    "    rect = Rectangle((3, 3), 1, 1, linewidth=2, edgecolor='red', facecolor='red', alpha=0.5)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # 局所的な関係（近傍）\n",
    "    for i in range(2, 5):\n",
    "        for j in range(2, 5):\n",
    "            if not (i == 3 and j == 3):\n",
    "                rect = Rectangle((j, i), 1, 1, linewidth=1, \n",
    "                                 edgecolor='blue', facecolor='lightblue', alpha=0.3)\n",
    "                ax.add_patch(rect)\n",
    "    \n",
    "    # 遠いピクセル\n",
    "    rect = Rectangle((0, 0), 1, 1, linewidth=1, \n",
    "                     edgecolor='gray', facecolor='lightgray', alpha=0.3)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    ax.annotate('強い関係', xy=(3.5, 3.5), xytext=(5.5, 5.5),\n",
    "               arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "    ax.annotate('弱い関係', xy=(3.5, 3.5), xytext=(0.5, 0.5),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1, linestyle='--'))\n",
    "    \n",
    "    ax.set_xlim(-0.5, 7.5)\n",
    "    ax.set_ylim(-0.5, 7.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title('1. 局所性\\n「近くのピクセルは関連性が高い」', fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 2. 平行移動等変性\n",
    "    ax = axes[1]\n",
    "    \n",
    "    # 2つの画像（同じパターンが異なる位置）\n",
    "    img1 = np.zeros((7, 7))\n",
    "    img2 = np.zeros((7, 7))\n",
    "    \n",
    "    # パターン（L字）\n",
    "    pattern = np.array([[1, 0], [1, 0], [1, 1]])\n",
    "    \n",
    "    img1[1:4, 1:3] = pattern\n",
    "    img2[3:6, 4:6] = pattern\n",
    "    \n",
    "    # 上下に並べて表示\n",
    "    combined = np.vstack([img1, np.ones((1, 7)) * 0.5, img2])\n",
    "    ax.imshow(combined, cmap='Blues', vmin=0, vmax=1)\n",
    "    \n",
    "    ax.axhline(y=7, color='black', linewidth=2)\n",
    "    \n",
    "    ax.text(3.5, -0.5, '画像1', fontsize=11, ha='center')\n",
    "    ax.text(3.5, 8.5, '画像2', fontsize=11, ha='center')\n",
    "    ax.text(3.5, 15, '同じカーネルで\\n同じ特徴を検出', fontsize=11, ha='center',\n",
    "           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    ax.set_title('2. 平行移動等変性\\n「同じパターンは同じ方法で処理」', fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 3. 階層性\n",
    "    ax = axes[2]\n",
    "    \n",
    "    # 階層図\n",
    "    levels = ['ピクセル', 'エッジ', 'テクスチャ', 'パーツ', 'オブジェクト']\n",
    "    y_positions = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    \n",
    "    for y, level in zip(y_positions, levels):\n",
    "        box = FancyBboxPatch((0.3, y - 0.05), 0.4, 0.08, \n",
    "                             boxstyle=\"round,pad=0.02\",\n",
    "                             facecolor='lightgreen', edgecolor='green',\n",
    "                             transform=ax.transAxes)\n",
    "        ax.add_patch(box)\n",
    "        ax.text(0.5, y, level, fontsize=12, ha='center', va='center',\n",
    "               transform=ax.transAxes)\n",
    "    \n",
    "    # 矢印\n",
    "    for i in range(len(levels) - 1):\n",
    "        ax.annotate('', xy=(0.5, y_positions[i+1] - 0.07), \n",
    "                   xytext=(0.5, y_positions[i] + 0.05),\n",
    "                   arrowprops=dict(arrowstyle='->', color='black', lw=1.5),\n",
    "                   transform=ax.transAxes)\n",
    "    \n",
    "    ax.text(0.85, 0.5, '層を重ねて\\n抽象度UP', fontsize=11, \n",
    "           transform=ax.transAxes, ha='center', va='center',\n",
    "           bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('3. 階層性\\n「単純→複雑を段階的に構築」', fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('CNNの3つの帰納バイアス', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_cnn_inductive_biases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_cnn_biases_in_detail():\n",
    "    \"\"\"CNNの帰納バイアスの詳細説明\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"CNNの帰納バイアス：詳細\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    biases = [\n",
    "        (\"1. 局所性 (Locality)\",\n",
    "         \"- 畳み込みカーネルは局所的な領域のみを見る\",\n",
    "         \"- 遠く離れたピクセルは直接的には関係しないと仮定\",\n",
    "         \"- 受容野を通じて間接的に大域的な情報を得る\",\n",
    "         \"→ 画像では近くのピクセルが同じオブジェクトに属することが多い\"),\n",
    "        \n",
    "        (\"2. 平行移動等変性 (Translation Equivariance)\",\n",
    "         \"- 同じカーネルを画像全体でスライドさせる（重み共有）\",\n",
    "         \"- パターンが画像のどこにあっても同じ応答\",\n",
    "         \"- 入力がシフト → 出力も同じだけシフト\",\n",
    "         \"→ 猫は画像の左にいても右にいても『猫』\"),\n",
    "        \n",
    "        (\"3. 階層性 (Hierarchy)\",\n",
    "         \"- 層を重ねることで受容野が拡大\",\n",
    "         \"- 浅い層: 低レベル特徴（エッジ、色）\",\n",
    "         \"- 深い層: 高レベル特徴（オブジェクト）\",\n",
    "         \"→ 視覚システムの階層構造に類似\")\n",
    "    ]\n",
    "    \n",
    "    for name, *details in biases:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(\"-\" * 50)\n",
    "        for detail in details:\n",
    "            print(f\"  {detail}\")\n",
    "\n",
    "explain_cnn_biases_in_detail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### これらの帰納バイアスが画像に適している理由\n",
    "\n",
    "自然画像には以下の性質があり、CNNの帰納バイアスと一致します：\n",
    "\n",
    "1. **空間的局所性**: 近くのピクセルは同じオブジェクトに属することが多い\n",
    "2. **定常性**: 同じパターン（エッジ、テクスチャ）は画像のどこにでも現れうる\n",
    "3. **構成性**: 複雑な構造は単純な構造の組み合わせから成る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## 4. 帰納バイアスと汎化性能\n",
    "\n",
    "### 帰納バイアスの強さとトレードオフ\n",
    "\n",
    "- **強い帰納バイアス**: 少ないデータで良い汎化、ただし問題に合わないと性能低下\n",
    "- **弱い帰納バイアス**: 多くのデータが必要、ただしより多様な問題に対応可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bias_variance_tradeoff():\n",
    "    \"\"\"帰納バイアスとバイアス-バリアンストレードオフ\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 1. データ量と性能の関係\n",
    "    ax = axes[0]\n",
    "    \n",
    "    data_sizes = np.logspace(1, 6, 50)\n",
    "    \n",
    "    # 強い帰納バイアス（CNN風）\n",
    "    strong_bias = 0.95 - 0.7 * np.exp(-data_sizes / 1000)\n",
    "    \n",
    "    # 弱い帰納バイアス（MLP風）\n",
    "    weak_bias = 0.95 - 0.8 * np.exp(-data_sizes / 50000)\n",
    "    \n",
    "    ax.semilogx(data_sizes, strong_bias, 'b-', linewidth=2, label='強い帰納バイアス（CNN）')\n",
    "    ax.semilogx(data_sizes, weak_bias, 'r-', linewidth=2, label='弱い帰納バイアス（MLP）')\n",
    "    \n",
    "    ax.axhline(y=0.95, color='green', linestyle='--', alpha=0.5, label='理論的最大性能')\n",
    "    \n",
    "    ax.fill_between(data_sizes, strong_bias, weak_bias, \n",
    "                   where=strong_bias > weak_bias, alpha=0.2, color='blue',\n",
    "                   label='CNNが有利な領域')\n",
    "    ax.fill_between(data_sizes, strong_bias, weak_bias, \n",
    "                   where=strong_bias < weak_bias, alpha=0.2, color='red',\n",
    "                   label='MLPが有利な領域')\n",
    "    \n",
    "    ax.set_xlabel('データ量', fontsize=12)\n",
    "    ax.set_ylabel('テスト精度', fontsize=12)\n",
    "    ax.set_title('データ量 vs 汎化性能', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # 2. バイアス-バリアンストレードオフ\n",
    "    ax = axes[1]\n",
    "    \n",
    "    complexity = np.linspace(0, 10, 100)\n",
    "    \n",
    "    bias_squared = 0.8 * np.exp(-complexity)\n",
    "    variance = 0.05 * complexity ** 1.5\n",
    "    total_error = bias_squared + variance\n",
    "    \n",
    "    ax.plot(complexity, bias_squared, 'b-', linewidth=2, label='バイアス²')\n",
    "    ax.plot(complexity, variance, 'r-', linewidth=2, label='バリアンス')\n",
    "    ax.plot(complexity, total_error, 'k-', linewidth=2, label='総誤差')\n",
    "    \n",
    "    # 最適点\n",
    "    optimal_idx = np.argmin(total_error)\n",
    "    ax.axvline(x=complexity[optimal_idx], color='green', linestyle='--', alpha=0.5)\n",
    "    ax.scatter([complexity[optimal_idx]], [total_error[optimal_idx]], \n",
    "              c='green', s=100, zorder=5, label='最適な複雑さ')\n",
    "    \n",
    "    ax.set_xlabel('モデルの複雑さ（帰納バイアスの弱さ）', fontsize=12)\n",
    "    ax.set_ylabel('誤差', fontsize=12)\n",
    "    ax.set_title('バイアス-バリアンストレードオフ', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_bias_variance_tradeoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_parameters():\n",
    "    \"\"\"モデルのパラメータ効率比較\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"帰納バイアスとパラメータ効率\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 32x32x3の入力、64チャンネル出力の場合\n",
    "    input_size = 32 * 32 * 3  # = 3072\n",
    "    output_channels = 64\n",
    "    \n",
    "    # MLP（全結合）\n",
    "    # 32x32x3 → 32x32x64 の全結合\n",
    "    fc_params = input_size * (32 * 32 * output_channels)\n",
    "    \n",
    "    # CNN（3x3カーネル）\n",
    "    kernel_size = 3\n",
    "    input_channels = 3\n",
    "    cnn_params = kernel_size * kernel_size * input_channels * output_channels + output_channels  # +bias\n",
    "    \n",
    "    print(f\"\\n入力: 32×32×3 = {input_size}ピクセル\")\n",
    "    print(f\"出力: 32×32×64 = {32*32*64}値\")\n",
    "    \n",
    "    print(f\"\\n【全結合層（MLP）】\")\n",
    "    print(f\"  パラメータ数: {fc_params:,}\")\n",
    "    print(f\"  帰納バイアス: なし（全ピクセルが全出力に接続）\")\n",
    "    \n",
    "    print(f\"\\n【畳み込み層（CNN）】\")\n",
    "    print(f\"  パラメータ数: {cnn_params:,}\")\n",
    "    print(f\"  帰納バイアス: 局所性 + 重み共有\")\n",
    "    \n",
    "    print(f\"\\n【比較】\")\n",
    "    print(f\"  パラメータ削減率: {fc_params / cnn_params:,.0f}倍\")\n",
    "    print(f\"  → 帰納バイアスにより、{fc_params / cnn_params:,.0f}倍少ないパラメータで同等の処理！\")\n",
    "\n",
    "compare_model_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 5. まとめ\n",
    "\n",
    "### 学んだこと\n",
    "\n",
    "1. **帰納バイアスとは**\n",
    "   - 学習アルゴリズムが持つ事前の仮定・制約\n",
    "   - 有限データから汎化するために必要\n",
    "\n",
    "2. **No Free Lunch定理**\n",
    "   - 万能のアルゴリズムは存在しない\n",
    "   - 問題に合った帰納バイアスが重要\n",
    "\n",
    "3. **CNNの帰納バイアス**\n",
    "   - **局所性**: 近くのピクセルは関連性が高い\n",
    "   - **平行移動等変性**: 同じパターンは同じ方法で処理\n",
    "   - **階層性**: 単純→複雑を段階的に構築\n",
    "\n",
    "4. **帰納バイアスの効果**\n",
    "   - パラメータ効率の向上（数千倍の削減）\n",
    "   - 少ないデータでの良好な汎化\n",
    "   - 画像という問題領域に特化した設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_diagram():\n",
    "    \"\"\"まとめの図\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # 中央: 帰納バイアス\n",
    "    ax.add_patch(plt.Circle((0.5, 0.5), 0.12, color='gold', alpha=0.7))\n",
    "    ax.text(0.5, 0.5, '帰納\\nバイアス', fontsize=14, ha='center', va='center',\n",
    "           fontweight='bold')\n",
    "    \n",
    "    # 上: 画像の性質\n",
    "    ax.add_patch(FancyBboxPatch((0.3, 0.75), 0.4, 0.12, \n",
    "                                boxstyle=\"round,pad=0.02\",\n",
    "                                facecolor='lightblue', edgecolor='blue'))\n",
    "    ax.text(0.5, 0.81, '画像の性質', fontsize=12, ha='center', fontweight='bold')\n",
    "    ax.text(0.5, 0.77, '局所相関・定常性・構成性', fontsize=10, ha='center')\n",
    "    \n",
    "    # 下: 汎化性能\n",
    "    ax.add_patch(FancyBboxPatch((0.3, 0.15), 0.4, 0.12, \n",
    "                                boxstyle=\"round,pad=0.02\",\n",
    "                                facecolor='lightgreen', edgecolor='green'))\n",
    "    ax.text(0.5, 0.21, '汎化性能', fontsize=12, ha='center', fontweight='bold')\n",
    "    ax.text(0.5, 0.17, '少ないデータで高精度', fontsize=10, ha='center')\n",
    "    \n",
    "    # 左: CNNの構造\n",
    "    ax.add_patch(FancyBboxPatch((0.05, 0.4), 0.2, 0.2, \n",
    "                                boxstyle=\"round,pad=0.02\",\n",
    "                                facecolor='lightyellow', edgecolor='orange'))\n",
    "    ax.text(0.15, 0.55, 'CNNの構造', fontsize=11, ha='center', fontweight='bold')\n",
    "    ax.text(0.15, 0.47, '・局所カーネル', fontsize=9, ha='center')\n",
    "    ax.text(0.15, 0.43, '・重み共有', fontsize=9, ha='center')\n",
    "    \n",
    "    # 右: パラメータ効率\n",
    "    ax.add_patch(FancyBboxPatch((0.75, 0.4), 0.2, 0.2, \n",
    "                                boxstyle=\"round,pad=0.02\",\n",
    "                                facecolor='lavender', edgecolor='purple'))\n",
    "    ax.text(0.85, 0.55, 'パラメータ効率', fontsize=11, ha='center', fontweight='bold')\n",
    "    ax.text(0.85, 0.47, 'MLP比で', fontsize=9, ha='center')\n",
    "    ax.text(0.85, 0.43, '数千倍削減', fontsize=9, ha='center')\n",
    "    \n",
    "    # 矢印\n",
    "    ax.annotate('', xy=(0.5, 0.62), xytext=(0.5, 0.75),\n",
    "               arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "    ax.annotate('', xy=(0.5, 0.38), xytext=(0.5, 0.27),\n",
    "               arrowprops=dict(arrowstyle='<-', color='green', lw=2))\n",
    "    ax.annotate('', xy=(0.38, 0.5), xytext=(0.25, 0.5),\n",
    "               arrowprops=dict(arrowstyle='<-', color='orange', lw=2))\n",
    "    ax.annotate('', xy=(0.62, 0.5), xytext=(0.75, 0.5),\n",
    "               arrowprops=dict(arrowstyle='->', color='purple', lw=2))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('帰納バイアス：画像の性質とCNN設計の橋渡し', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "summary_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 次のノートブック\n",
    "\n",
    "次のノートブックでは、CNNの重要な帰納バイアスである**重み共有（Weight Sharing）**について詳しく学びます。\n",
    "\n",
    "- 重み共有の数学的定義\n",
    "- なぜパラメータが削減されるか\n",
    "- 重み共有がもたらす平行移動等変性"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
