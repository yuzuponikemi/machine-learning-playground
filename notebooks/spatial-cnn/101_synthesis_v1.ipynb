{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101. 総合演習：空間の帰納バイアスを統合する\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックでは、Unit 0.4全体を振り返り、以下を統合的に理解します：\n",
    "\n",
    "1. **畳み込み**の本質的理解\n",
    "2. **受容野**とネットワーク設計\n",
    "3. **帰納バイアス**の意味と限界\n",
    "4. **セグメンテーション**アーキテクチャの設計思想\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. [カリキュラムの振り返り](#section1)\n",
    "2. [核心概念の統合](#section2)\n",
    "3. [設計原則のまとめ](#section3)\n",
    "4. [チェックリスト](#section4)\n",
    "5. [次へのステップ](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1. カリキュラムの振り返り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_curriculum_map():\n",
    "    \"\"\"カリキュラム全体のマップを可視化\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(18, 12))\n",
    "    \n",
    "    sections = [\n",
    "        ('A. カーネル物理学\\n(80-85)', 0.1, 0.8, 'lightblue', \n",
    "         ['畳み込みの直感', '数学的定義', 'NumPy実装', '古典フィルタ', 'カーネル可視化']),\n",
    "        ('B. 受容野\\n(86-89)', 0.35, 0.8, 'lightgreen',\n",
    "         ['受容野入門', '深さと受容野', 'ダウンサンプリング', '3DGSとの類似性']),\n",
    "        ('C. 帰納バイアス\\n(90-95)', 0.6, 0.8, 'lightyellow',\n",
    "         ['帰納バイアス入門', '重み共有', '並進等変性', 'CNN vs MLP', 'CNNの限界', 'ViT/MLP-Mixer']),\n",
    "        ('D. セグメンテーション\\n(96-100)', 0.35, 0.35, 'lightcoral',\n",
    "         ['セマンティックセグメンテーション', 'U-Net', 'FPN', 'スキップ接続', '応用事例']),\n",
    "        ('E. 統合\\n(101-102)', 0.6, 0.35, 'lavender',\n",
    "         ['総合演習', '未来への展望']),\n",
    "    ]\n",
    "    \n",
    "    for name, x, y, color, topics in sections:\n",
    "        rect = plt.Rectangle((x, y - 0.15), 0.2, 0.25, \n",
    "                            facecolor=color, edgecolor='gray', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + 0.1, y + 0.08, name, ha='center', va='center', \n",
    "               fontsize=11, fontweight='bold')\n",
    "        \n",
    "        topic_text = '\\n'.join([f'• {t}' for t in topics[:4]])\n",
    "        if len(topics) > 4:\n",
    "            topic_text += f'\\n  + {len(topics)-4}項目'\n",
    "        ax.text(x + 0.1, y - 0.08, topic_text, ha='center', va='center', \n",
    "               fontsize=8)\n",
    "    \n",
    "    # 矢印（学習の流れ）\n",
    "    arrows = [\n",
    "        ((0.3, 0.8), (0.35, 0.8)),   # A → B\n",
    "        ((0.55, 0.8), (0.6, 0.8)),   # B → C\n",
    "        ((0.45, 0.65), (0.45, 0.5)), # B → D\n",
    "        ((0.7, 0.65), (0.7, 0.5)),   # C → E\n",
    "        ((0.55, 0.35), (0.6, 0.35)), # D → E\n",
    "    ]\n",
    "    \n",
    "    for start, end in arrows:\n",
    "        ax.annotate('', xy=end, xytext=start,\n",
    "                   arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    ax.set_xlim(0, 0.95)\n",
    "    ax.set_ylim(0.1, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Unit 0.4 カリキュラムマップ：空間の帰納バイアス', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\nvisualize_curriculum_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_key_concepts():\n",
    "    \"\"\"各セクションの核心概念をまとめる\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"Unit 0.4 核心概念のまとめ\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "【Section A: カーネル物理学】\n",
    "  核心: 畳み込み = 「周囲を見て自分の値を決める」演算\n",
    "  \n",
    "  • 数学的定義: (f * g)[n] = Σ f[k] g[n-k]\n",
    "  • 古典フィルタ: ぼかし、エッジ検出、シャープニング\n",
    "  • 実装: sliding_window_view, im2col で効率化\n",
    "\n",
    "【Section B: 受容野】\n",
    "  核心: 1ピクセルの決定に影響する入力領域\n",
    "  \n",
    "  • 漸化式: RF_n = RF_{n-1} + (k_n - 1) × 累積ストライド\n",
    "  • 深さとの関係: 層を深くすると受容野が指数的に拡大\n",
    "  • 3DGSとの類似: スプラットサイズ ≈ 受容野\n",
    "\n",
    "【Section C: 帰納バイアス】\n",
    "  核心: CNNは「画像には局所的なパターンがある」と仮定\n",
    "  \n",
    "  • 重み共有: 位置に依存しない特徴検出\n",
    "  • 並進等変性: 入力をずらすと出力もずれる\n",
    "  • 限界: 回転、長距離依存、テクスチャバイアス\n",
    "\n",
    "【Section D: セグメンテーション】\n",
    "  核心: ピクセルごとの分類 = 空間情報の完全活用\n",
    "  \n",
    "  • U-Net: エンコーダ-デコーダ + スキップ接続\n",
    "  • FPN: マルチスケール特徴ピラミッド\n",
    "  • スキップ接続: 加算型（ResNet）vs 連結型（U-Net）\n",
    "    \"\"\")\n",
    "\nsummarize_key_concepts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## 2. 核心概念の統合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_concept_connections():\n",
    "    \"\"\"概念間のつながりを可視化\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    \n",
    "    # 中心概念\n",
    "    concepts = {\n",
    "        '空間的帰納バイアス': (0.5, 0.5, 'gold', 0.12),\n",
    "        '局所性': (0.25, 0.75, 'lightblue', 0.08),\n",
    "        '重み共有': (0.75, 0.75, 'lightgreen', 0.08),\n",
    "        '受容野': (0.25, 0.25, 'lightcoral', 0.08),\n",
    "        'セグメンテーション': (0.75, 0.25, 'lavender', 0.08),\n",
    "    }\n",
    "    \n",
    "    for name, (x, y, color, radius) in concepts.items():\n",
    "        circle = plt.Circle((x, y), radius, facecolor=color, edgecolor='gray', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y, name, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 接続線\n",
    "    connections = [\n",
    "        ('局所性', '空間的帰納バイアス', '定義の一部'),\n",
    "        ('重み共有', '空間的帰納バイアス', '定義の一部'),\n",
    "        ('空間的帰納バイアス', '受容野', '設計に影響'),\n",
    "        ('空間的帰納バイアス', 'セグメンテーション', '応用'),\n",
    "        ('局所性', '受容野', '関連'),\n",
    "        ('重み共有', 'セグメンテーション', '効率化'),\n",
    "    ]\n",
    "    \n",
    "    for c1, c2, label in connections:\n",
    "        x1, y1, _, r1 = concepts[c1]\n",
    "        x2, y2, _, r2 = concepts[c2]\n",
    "        ax.plot([x1, x2], [y1, y2], 'k-', alpha=0.3, linewidth=2)\n",
    "        ax.text((x1+x2)/2, (y1+y2)/2, label, fontsize=8, ha='center', va='center',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('概念間のつながり', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\nvisualize_concept_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_unified_view():\n",
    "    \"\"\"統一的な視点からの説明\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"統一的視点：なぜCNNは画像に効くのか\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "【根本的な問い】\n",
    "  なぜCNNは画像認識で成功したのか？\n",
    "\n",
    "【答え：帰納バイアスが画像の性質と一致】\n",
    "\n",
    "  ┌─────────────────────────────────────────────────────┐\n",
    "  │ 画像の性質          →    CNNの設計原理            │\n",
    "  ├─────────────────────────────────────────────────────┤\n",
    "  │ 局所的なパターン    →    小さなカーネル            │\n",
    "  │ パターンは位置不変  →    重み共有                  │\n",
    "  │ 階層的な構造        →    深い層構造                │\n",
    "  │ スケール変化        →    ダウンサンプリング        │\n",
    "  └─────────────────────────────────────────────────────┘\n",
    "\n",
    "【受容野の役割】\n",
    "  • 浅い層: 小さな受容野 → 局所的な特徴（エッジ、テクスチャ）\n",
    "  • 深い層: 大きな受容野 → 大域的な特徴（物体全体）\n",
    "  \n",
    "  この階層構造が、画像の「部分→全体」の構造と一致\n",
    "\n",
    "【セグメンテーションへの発展】\n",
    "  • 分類: 画像 → 1つのラベル（大域的な受容野が必要）\n",
    "  • セグメンテーション: 画像 → ピクセルごとのラベル（局所と大域の両方）\n",
    "  \n",
    "  → U-Netのスキップ接続が「局所と大域」を橋渡し\n",
    "\n",
    "【限界と次世代】\n",
    "  • CNNの仮定が成り立たない場合（長距離依存など）\n",
    "  • → Vision Transformer: 全結合Attention\n",
    "  • → MLP-Mixer: チャンネルと空間を分離して処理\n",
    "    \"\"\")\n",
    "\nexplain_unified_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 3. 設計原則のまとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_principles():\n",
    "    \"\"\"CNNアーキテクチャ設計の原則\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"CNNアーキテクチャ設計の原則\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "【原則1: 受容野を意識する】\n",
    "  \n",
    "  目標: 最終層の受容野が入力の重要な部分をカバー\n",
    "  \n",
    "  計算式: RF_n = RF_{n-1} + (k_n - 1) × 累積ストライド\n",
    "  \n",
    "  例: 3×3カーネル5層、ストライド2が2回\n",
    "      RF = 1 + 2×1 + 2×1 + 2×2 + 2×2 + 2×4 = 1 + 2 + 2 + 4 + 4 + 8 = 21\n",
    "\n",
    "【原則2: ダウンサンプリングの戦略】\n",
    "  \n",
    "  • プーリング: 位置の微小なずれに頑健\n",
    "  • Strided Conv: 学習可能なダウンサンプリング\n",
    "  \n",
    "  一般的な戦略:\n",
    "  - 初期層: 解像度を維持（詳細な特徴を抽出）\n",
    "  - 中間層: 段階的にダウンサンプリング\n",
    "  - 深い層: 低解像度で大域的な特徴を処理\n",
    "\n",
    "【原則3: スキップ接続の活用】\n",
    "  \n",
    "  目的に応じて選択:\n",
    "  • 分類: ResNet型（加算）で勾配の流れを改善\n",
    "  • セグメンテーション: U-Net型（連結）で詳細を保持\n",
    "  • 両方を組み合わせることも可能\n",
    "\n",
    "【原則4: チャンネル数の設計】\n",
    "  \n",
    "  一般的なパターン:\n",
    "  • 解像度が半分になるとチャンネル数を2倍\n",
    "  • 計算量をほぼ一定に保つ\n",
    "  \n",
    "  例: 64 → 128 → 256 → 512（解像度: H → H/2 → H/4 → H/8）\n",
    "\n",
    "【原則5: 正則化と安定化】\n",
    "  \n",
    "  • BatchNorm: 各層の出力を正規化\n",
    "  • Dropout: 過学習を防止\n",
    "  • Weight Decay: 重みの大きさを制限\n",
    "    \"\"\")\n",
    "\ndesign_principles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_architecture_patterns():\n",
    "    \"\"\"代表的なアーキテクチャパターン\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # VGG型\n",
    "    ax = axes[0]\n",
    "    layers = [(0.5, 0.9, 0.3, 0.08), (0.5, 0.75, 0.3, 0.08), (0.5, 0.6, 0.25, 0.08),\n",
    "              (0.5, 0.45, 0.2, 0.08), (0.5, 0.3, 0.15, 0.08), (0.5, 0.15, 0.1, 0.08)]\n",
    "    \n",
    "    for x, y, w, h in layers:\n",
    "        rect = plt.Rectangle((x - w/2, y - h/2), w, h, facecolor='lightblue', edgecolor='blue')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('VGG型\\n（シンプルな積み重ね）', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # ResNet型\n",
    "    ax = axes[1]\n",
    "    for x, y, w, h in layers:\n",
    "        rect = plt.Rectangle((x - w/2, y - h/2), w, h, facecolor='lightgreen', edgecolor='green')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # スキップ接続\n",
    "    for i in range(0, len(layers)-1, 2):\n",
    "        ax.annotate('', xy=(0.8, layers[i+1][1]), xytext=(0.8, layers[i][1]),\n",
    "                   arrowprops=dict(arrowstyle='->', color='red', lw=2,\n",
    "                                  connectionstyle='arc3,rad=0.3'))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('ResNet型\\n（スキップ接続で深く）', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # U-Net型\n",
    "    ax = axes[2]\n",
    "    encoder = [(0.2, 0.85, 0.15, 0.08), (0.2, 0.65, 0.12, 0.08), (0.2, 0.45, 0.09, 0.08)]\n",
    "    decoder = [(0.7, 0.45, 0.09, 0.08), (0.7, 0.65, 0.12, 0.08), (0.7, 0.85, 0.15, 0.08)]\n",
    "    bottleneck = [(0.45, 0.25, 0.06, 0.08)]\n",
    "    \n",
    "    for x, y, w, h in encoder:\n",
    "        rect = plt.Rectangle((x - w/2, y - h/2), w, h, facecolor='lightblue', edgecolor='blue')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    for x, y, w, h in decoder:\n",
    "        rect = plt.Rectangle((x - w/2, y - h/2), w, h, facecolor='lightgreen', edgecolor='green')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    for x, y, w, h in bottleneck:\n",
    "        rect = plt.Rectangle((x - w/2, y - h/2), w, h, facecolor='coral', edgecolor='red')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # スキップ接続\n",
    "    for enc, dec in zip(encoder, decoder):\n",
    "        ax.annotate('', xy=(dec[0] - dec[2]/2, dec[1]), \n",
    "                   xytext=(enc[0] + enc[2]/2, enc[1]),\n",
    "                   arrowprops=dict(arrowstyle='->', color='orange', lw=2))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('U-Net型\\n（エンコーダ-デコーダ）', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('代表的なCNNアーキテクチャパターン', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\nvisualize_architecture_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## 4. 理解度チェックリスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def understanding_checklist():\n",
    "    \"\"\"理解度チェックリスト\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"Unit 0.4 理解度チェックリスト\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    checklist = [\n",
    "        (\"カーネル物理学\", [\n",
    "            \"畳み込みを『周囲を見て自分の値を決める演算』と説明できる\",\n",
    "            \"畳み込みと相関の違いを説明できる\",\n",
    "            \"Sobel、Gaussianフィルタの役割を説明できる\",\n",
    "            \"im2colの仕組みを理解している\",\n",
    "        ]),\n",
    "        (\"受容野\", [\n",
    "            \"受容野の定義を説明できる\",\n",
    "            \"受容野の漸化式を使って計算できる\",\n",
    "            \"プーリングとStrided Convの違いを説明できる\",\n",
    "            \"受容野と3DGSスプラットの類似性を理解している\",\n",
    "        ]),\n",
    "        (\"帰納バイアス\", [\n",
    "            \"帰納バイアスの意味を説明できる\",\n",
    "            \"重み共有のメリットを説明できる\",\n",
    "            \"並進等変性と並進不変性の違いを説明できる\",\n",
    "            \"CNNの限界（回転、長距離依存）を説明できる\",\n",
    "        ]),\n",
    "        (\"セグメンテーション\", [\n",
    "            \"セマンティック/インスタンス/パノプティックの違いを説明できる\",\n",
    "            \"U-Netの構造とスキップ接続の役割を説明できる\",\n",
    "            \"FPNの目的と構造を説明できる\",\n",
    "            \"ResNet型とU-Net型のスキップ接続の違いを説明できる\",\n",
    "        ]),\n",
    "    ]\n",
    "    \n",
    "    for section, items in checklist:\n",
    "        print(f\"\\n【{section}】\")\n",
    "        for item in items:\n",
    "            print(f\"  □ {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"全ての項目にチェックが入れば、Unit 0.4の学習は完了です！\")\n",
    "\nunderstanding_checklist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quiz():\n",
    "    \"\"\"簡単なクイズ\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"クイズ：理解度を確認しよう\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    questions = [\n",
    "        (\"Q1: 3×3カーネルを5層重ねた時の受容野は？（ストライド全て1）\",\n",
    "         \"A1: RF = 1 + 2×5 = 11ピクセル\"),\n",
    "        (\"Q2: CNNの『重み共有』によって得られる最大のメリットは？\",\n",
    "         \"A2: パラメータ数の大幅削減（位置に関係なく同じ特徴を検出）\"),\n",
    "        (\"Q3: U-Netのスキップ接続が『連結』を使う理由は？\",\n",
    "         \"A3: 高解像度の詳細情報を保持するため（加算だと情報が混ざる）\"),\n",
    "        (\"Q4: Vision TransformerがCNNより優れている点は？\",\n",
    "         \"A4: 長距離依存関係を直接モデル化できる（グローバルな注意機構）\"),\n",
    "        (\"Q5: FPNの『ラテラル接続』の役割は？\",\n",
    "         \"A5: 異なる解像度の特徴を統合（1×1畳み込みでチャンネル数を調整）\"),\n",
    "    ]\n",
    "    \n",
    "    for q, a in questions:\n",
    "        print(f\"\\n{q}\")\n",
    "        print(f\"  → {a}\")\n",
    "\nquiz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 5. 次へのステップ\n",
    "\n",
    "### Unit 0.4で学んだこと\n",
    "\n",
    "このユニットでは、CNNの空間的帰納バイアスについて深く学びました。\n",
    "\n",
    "- **畳み込み**は「局所的なパターンを効率的に検出する」演算\n",
    "- **受容野**は「どれだけの入力領域を見ているか」を表す\n",
    "- **帰納バイアス**は「事前知識を構造に埋め込む」設計思想\n",
    "- **セグメンテーション**は「ピクセル単位の空間理解」を実現\n",
    "\n",
    "### 次のノートブックへ\n",
    "\n",
    "最後のノートブック（102）では、未来への展望として：\n",
    "- CNNからTransformerへの進化\n",
    "- 3D理解（NeRF、3DGS）への接続\n",
    "- NeoVerseプロジェクトとの関連\n",
    "\n",
    "を扱います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_message():\n",
    "    \"\"\"最終メッセージ\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"おめでとうございます！\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "Unit 0.4「空間の帰納バイアス：CNN & 局所性の科学」の\n",
    "総合演習が完了しました。\n",
    "\n",
    "【習得したスキル】\n",
    "  ✓ 畳み込みの直感的・数学的理解\n",
    "  ✓ 受容野の計算と設計への応用\n",
    "  ✓ 帰納バイアスの概念と限界の理解\n",
    "  ✓ セグメンテーションアーキテクチャの設計思想\n",
    "\n",
    "【次のステップ】\n",
    "  → ノートブック102: 未来への展望\n",
    "  → 3DGS/NeRFへの架け橋\n",
    "  → NeoVerseプロジェクトの基礎完成\n",
    "\n",
    "あなたは今、深層学習の空間理解における\n",
    "確かな基礎を身につけました。\n",
    "    \"\"\")\n",
    "\nfinal_message()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
