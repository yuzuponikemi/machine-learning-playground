{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95. CNNを超えて：Vision TransformerとMLP-Mixer\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックでは、以下を学びます：\n",
    "\n",
    "1. **Vision Transformer (ViT)**の基本概念\n",
    "2. **Self-Attention**による大域的な依存関係のモデリング\n",
    "3. **MLP-Mixer**：MLPだけで画像認識\n",
    "4. **帰納バイアスのスペクトル**\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. [CNNの限界の振り返り](#section1)\n",
    "2. [Vision Transformer](#section2)\n",
    "3. [MLP-Mixer](#section3)\n",
    "4. [帰納バイアスの比較](#section4)\n",
    "5. [まとめ](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch, FancyArrowPatch\n",
    "import japanize_matplotlib\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1. CNNの限界の振り返り\n",
    "\n",
    "CNNの主な制約：\n",
    "1. **局所的な受容野**：長距離依存関係を捉えにくい\n",
    "2. **固定的な帰納バイアス**：平行移動のみ等変\n",
    "3. **階層的な処理**：大域的な情報は深い層でのみ利用可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_architecture_evolution():\n",
    "    \"\"\"アーキテクチャの進化を可視化\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    \n",
    "    # タイムライン\n",
    "    architectures = [\n",
    "        (2012, 'AlexNet', 'CNN', 'lightblue'),\n",
    "        (2014, 'VGGNet', 'CNN', 'lightblue'),\n",
    "        (2015, 'ResNet', 'CNN', 'lightblue'),\n",
    "        (2017, 'Transformer\\n(NLP)', 'Attention', 'lightyellow'),\n",
    "        (2020, 'ViT', 'Attention', 'lightgreen'),\n",
    "        (2021, 'MLP-Mixer', 'MLP', 'lightcoral'),\n",
    "        (2022, 'ConvNeXt', 'CNN+Modern', 'lightblue'),\n",
    "    ]\n",
    "    \n",
    "    for year, name, arch_type, color in architectures:\n",
    "        x = (year - 2012) / 10\n",
    "        box = FancyBboxPatch((x - 0.04, 0.3), 0.08, 0.4,\n",
    "                             boxstyle=\"round,pad=0.02\",\n",
    "                             facecolor=color, edgecolor='gray',\n",
    "                             transform=ax.transAxes)\n",
    "        ax.add_patch(box)\n",
    "        ax.text(x, 0.75, name, ha='center', va='center', fontsize=10,\n",
    "               transform=ax.transAxes, fontweight='bold')\n",
    "        ax.text(x, 0.5, arch_type, ha='center', va='center', fontsize=9,\n",
    "               transform=ax.transAxes)\n",
    "        ax.text(x, 0.2, str(year), ha='center', va='center', fontsize=10,\n",
    "               transform=ax.transAxes)\n",
    "    \n",
    "    # 凡例\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='lightblue', label='CNN系'),\n",
    "        Patch(facecolor='lightyellow', label='Transformer (NLP)'),\n",
    "        Patch(facecolor='lightgreen', label='Vision Transformer'),\n",
    "        Patch(facecolor='lightcoral', label='MLP系'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('画像認識アーキテクチャの進化', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_architecture_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## 2. Vision Transformer (ViT)\n",
    "\n",
    "### 基本アイデア\n",
    "\n",
    "1. 画像を**パッチ**に分割\n",
    "2. 各パッチを**トークン**として扱う\n",
    "3. **Self-Attention**で全パッチ間の関係をモデリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vit_architecture():\n",
    "    \"\"\"ViTのアーキテクチャを可視化\"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    # 1. 入力画像\n",
    "    ax = axes[0]\n",
    "    img = np.random.rand(8, 8)\n",
    "    ax.imshow(img, cmap='viridis')\n",
    "    # パッチ分割を表示\n",
    "    for i in range(0, 9, 2):\n",
    "        ax.axhline(y=i-0.5, color='red', linewidth=2)\n",
    "        ax.axvline(x=i-0.5, color='red', linewidth=2)\n",
    "    ax.set_title('1. 入力画像\\n（パッチに分割）', fontsize=12)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 2. パッチのフラット化\n",
    "    ax = axes[1]\n",
    "    patches = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            patch = img[i*2:(i+1)*2, j*2:(j+1)*2]\n",
    "            patches.append(patch.flatten())\n",
    "    patches = np.array(patches)\n",
    "    \n",
    "    ax.imshow(patches, cmap='viridis', aspect='auto')\n",
    "    ax.set_xlabel('パッチ内の値')\n",
    "    ax.set_ylabel('パッチ番号')\n",
    "    ax.set_title('2. パッチをフラット化\\n（16パッチ → 16トークン）', fontsize=12)\n",
    "    \n",
    "    # 3. Self-Attention\n",
    "    ax = axes[2]\n",
    "    # アテンションマップ（ランダム）\n",
    "    np.random.seed(42)\n",
    "    attention = np.random.rand(16, 16)\n",
    "    attention = attention / attention.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    ax.imshow(attention, cmap='Blues')\n",
    "    ax.set_xlabel('Key パッチ')\n",
    "    ax.set_ylabel('Query パッチ')\n",
    "    ax.set_title('3. Self-Attention\\n（全パッチ間の関係）', fontsize=12)\n",
    "    \n",
    "    # 4. 特徴の意味\n",
    "    ax = axes[3]\n",
    "    ax.text(0.5, 0.8, 'Self-Attentionの利点', fontsize=14, \n",
    "           ha='center', transform=ax.transAxes, fontweight='bold')\n",
    "    \n",
    "    benefits = [\n",
    "        '• 長距離依存関係を直接モデル化',\n",
    "        '• 大域的なコンテキストを即座に取得',\n",
    "        '• 動的な特徴の重み付け',\n",
    "        '• 入力依存の受容野',\n",
    "    ]\n",
    "    \n",
    "    for i, benefit in enumerate(benefits):\n",
    "        ax.text(0.1, 0.6 - i*0.15, benefit, fontsize=11, \n",
    "               transform=ax.transAxes)\n",
    "    \n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Vision Transformer (ViT) の仕組み', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_vit_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_self_attention():\n",
    "    \"\"\"Self-Attentionの説明\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Self-Attention の仕組み\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\"\"\n",
    "【数式】\n",
    "Attention(Q, K, V) = softmax(QK^T / √d_k) V\n",
    "\n",
    "- Q (Query): 「何を探しているか」\n",
    "- K (Key): 「何を持っているか」\n",
    "- V (Value): 「実際の値」\n",
    "- d_k: キーの次元（スケーリング用）\n",
    "\n",
    "【直感的理解】\n",
    "1. 各パッチが「どの他のパッチに注目すべきか」を学習\n",
    "2. QK^T で類似度を計算\n",
    "3. softmax で正規化（重みの合計=1）\n",
    "4. 重み付き和で出力を計算\n",
    "\n",
    "【CNNとの違い】\n",
    "CNN:  各位置は固定された近傍のみを見る（局所的）\n",
    "ViT:  各位置は全ての位置を見れる（大域的）\n",
    "    \"\"\")\n",
    "\n",
    "explain_self_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cnn_vs_vit_receptive_field():\n",
    "    \"\"\"CNNとViTの受容野比較\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # CNN\n",
    "    ax = axes[0]\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            rect = Rectangle((j, 7-i), 0.9, 0.9, facecolor='white', edgecolor='gray')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # 中心のニューロンの受容野（3x3 × 2層 = 5x5）\n",
    "    center = (3.5, 3.5)\n",
    "    for i in range(2, 6):\n",
    "        for j in range(2, 6):\n",
    "            rect = Rectangle((j, 7-i), 0.9, 0.9, facecolor='lightblue', \n",
    "                             edgecolor='blue', alpha=0.5)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # 中心\n",
    "    rect = Rectangle((3, 4), 0.9, 0.9, facecolor='red', edgecolor='red')\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim(-0.5, 8.5)\n",
    "    ax.set_ylim(-0.5, 8.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('CNN: 局所的な受容野\\n（深い層で徐々に拡大）', fontsize=12)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # ViT\n",
    "    ax = axes[1]\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            color = 'lightblue' if not (i == 3 and j == 3) else 'red'\n",
    "            rect = Rectangle((j, 7-i), 0.9, 0.9, facecolor=color, \n",
    "                             edgecolor='blue' if color == 'lightblue' else 'red',\n",
    "                             alpha=0.5 if color == 'lightblue' else 1)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim(-0.5, 8.5)\n",
    "    ax.set_ylim(-0.5, 8.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('ViT: 大域的な受容野\\n（最初の層から全体を見れる）', fontsize=12)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('受容野の比較：CNN vs ViT', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_cnn_vs_vit_receptive_field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 3. MLP-Mixer\n",
    "\n",
    "### 驚きの発見\n",
    "\n",
    "畳み込みもAttentionも使わず、**MLPだけ**で画像認識が可能！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mlp_mixer():\n",
    "    \"\"\"MLP-Mixerの仕組み\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # 1. パッチの配列\n",
    "    ax = axes[0]\n",
    "    # パッチ×特徴 の行列\n",
    "    data = np.random.rand(9, 6)  # 9パッチ、6特徴\n",
    "    ax.imshow(data, cmap='viridis', aspect='auto')\n",
    "    ax.set_xlabel('特徴次元')\n",
    "    ax.set_ylabel('パッチ')\n",
    "    ax.set_title('入力: パッチ × 特徴 行列', fontsize=12)\n",
    "    \n",
    "    # 2. Token Mixing\n",
    "    ax = axes[1]\n",
    "    ax.text(0.5, 0.85, 'Token Mixing MLP', fontsize=14, \n",
    "           ha='center', transform=ax.transAxes, fontweight='bold')\n",
    "    \n",
    "    ax.text(0.5, 0.65, '列方向（パッチ間）に\\nMLPを適用', fontsize=12, \n",
    "           ha='center', transform=ax.transAxes)\n",
    "    \n",
    "    # 矢印で方向を示す\n",
    "    ax.annotate('', xy=(0.3, 0.35), xytext=(0.3, 0.5),\n",
    "               arrowprops=dict(arrowstyle='->', color='blue', lw=3),\n",
    "               transform=ax.transAxes)\n",
    "    ax.text(0.35, 0.42, 'パッチ間の\\n情報交換', fontsize=10, transform=ax.transAxes)\n",
    "    \n",
    "    ax.text(0.5, 0.15, 'Channel Mixing MLP', fontsize=14, \n",
    "           ha='center', transform=ax.transAxes, fontweight='bold')\n",
    "    \n",
    "    ax.annotate('', xy=(0.7, 0.25), xytext=(0.55, 0.25),\n",
    "               arrowprops=dict(arrowstyle='->', color='red', lw=3),\n",
    "               transform=ax.transAxes)\n",
    "    ax.text(0.6, 0.28, '特徴間の\\n情報交換', fontsize=10, transform=ax.transAxes)\n",
    "    \n",
    "    ax.axis('off')\n",
    "    ax.set_title('MLP-Mixer の2種類のMLP', fontsize=12)\n",
    "    \n",
    "    # 3. 比較表\n",
    "    ax = axes[2]\n",
    "    comparison = [\n",
    "        ('', 'CNN', 'ViT', 'MLP-Mixer'),\n",
    "        ('空間的混合', '畳み込み', 'Self-Att.', 'Token MLP'),\n",
    "        ('チャンネル混合', '1×1 Conv', 'MLP', 'Channel MLP'),\n",
    "        ('帰納バイアス', '強い', '弱い', '中程度'),\n",
    "        ('長距離依存', '層で成長', '即座', '即座'),\n",
    "    ]\n",
    "    \n",
    "    table = ax.table(cellText=comparison, loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.8)\n",
    "    \n",
    "    # ヘッダーを太字に\n",
    "    for i in range(4):\n",
    "        table[(0, i)].set_text_props(fontweight='bold')\n",
    "    \n",
    "    ax.axis('off')\n",
    "    ax.set_title('アーキテクチャ比較', fontsize=12)\n",
    "    \n",
    "    plt.suptitle('MLP-Mixer: 畳み込みもAttentionも使わない', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_mlp_mixer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## 4. 帰納バイアスの比較\n",
    "\n",
    "各アーキテクチャは異なる帰納バイアスを持ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_inductive_bias_spectrum():\n",
    "    \"\"\"帰納バイアスのスペクトル\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # スペクトル（帰納バイアスの強さ）\n",
    "    architectures = [\n",
    "        ('MLP\\n(全結合)', 0.1, 'lightcoral'),\n",
    "        ('ViT\\n(パッチ分割のみ)', 0.3, 'lightgreen'),\n",
    "        ('MLP-Mixer\\n(パッチ + 位置)', 0.4, 'lightyellow'),\n",
    "        ('CNN\\n(局所性 + 重み共有)', 0.7, 'lightblue'),\n",
    "        ('G-CNN\\n(回転等変)', 0.85, 'lavender'),\n",
    "    ]\n",
    "    \n",
    "    for name, bias, color in architectures:\n",
    "        ax.barh(name, bias, color=color, edgecolor='gray', height=0.6)\n",
    "        ax.text(bias + 0.02, name, f'{bias:.1f}', va='center', fontsize=11)\n",
    "    \n",
    "    ax.set_xlabel('帰納バイアスの強さ', fontsize=12)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_title('アーキテクチャ別の帰納バイアス', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 注釈\n",
    "    ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.text(0.25, -0.8, '← 弱い帰納バイアス\\n（より多くのデータが必要）', \n",
    "           ha='center', fontsize=10, transform=ax.transData)\n",
    "    ax.text(0.75, -0.8, '強い帰納バイアス →\\n（少ないデータで学習可能）', \n",
    "           ha='center', fontsize=10, transform=ax.transData)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_inductive_bias_spectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_data_efficiency():\n",
    "    \"\"\"データ効率の比較\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    data_sizes = np.logspace(4, 8, 50)  # 10K to 100M\n",
    "    \n",
    "    # 仮想的な性能曲線\n",
    "    cnn_perf = 0.9 * (1 - np.exp(-data_sizes / 1e6))\n",
    "    vit_perf = 0.95 * (1 - np.exp(-data_sizes / 1e7))\n",
    "    mlp_perf = 0.85 * (1 - np.exp(-data_sizes / 5e7))\n",
    "    \n",
    "    ax.semilogx(data_sizes, cnn_perf, 'b-', label='CNN', linewidth=2)\n",
    "    ax.semilogx(data_sizes, vit_perf, 'g-', label='ViT', linewidth=2)\n",
    "    ax.semilogx(data_sizes, mlp_perf, 'r-', label='MLP', linewidth=2)\n",
    "    \n",
    "    # 交差点を強調\n",
    "    ax.axvline(x=1e7, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.text(1e7, 0.3, 'CNNとViTが\\n交差する領域', fontsize=10, ha='center')\n",
    "    \n",
    "    ax.set_xlabel('訓練データ数', fontsize=12)\n",
    "    ax.set_ylabel('性能', fontsize=12)\n",
    "    ax.set_title('データ量と性能の関係（概念図）', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"観察:\")\n",
    "    print(\"- 少ないデータ: 強い帰納バイアスを持つCNNが有利\")\n",
    "    print(\"- 大量データ: 弱い帰納バイアスのViTが最終的に高性能\")\n",
    "    print(\"- トレードオフ: 帰納バイアスの強さ vs データ効率 vs 最終性能\")\n",
    "\n",
    "compare_data_efficiency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 5. まとめ\n",
    "\n",
    "### アーキテクチャの選択指針\n",
    "\n",
    "| 状況 | 推奨アーキテクチャ |\n",
    "|------|------------------|\n",
    "| 少ないデータ | CNN |\n",
    "| 大量のデータ | ViT |\n",
    "| 長距離依存が重要 | ViT / MLP-Mixer |\n",
    "| 回転不変性が必要 | G-CNN / データ拡張 |\n",
    "| 推論速度重視 | CNN / MLP-Mixer |\n",
    "\n",
    "### 学んだこと\n",
    "\n",
    "1. **ViT**: Self-Attentionで大域的な依存関係をモデル化\n",
    "2. **MLP-Mixer**: 畳み込みもAttentionも不要という驚きの発見\n",
    "3. **帰納バイアスはトレードオフ**: 強すぎても弱すぎても問題\n",
    "4. **問題とデータに応じた選択**が重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Cの終わり\n",
    "\n",
    "これでSection C（帰納バイアスの科学）は終了です。\n",
    "\n",
    "次のSection Dでは、**空間知性の応用**（セマンティックセグメンテーション、U-Netなど）について学びます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
