{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. カーネルの可視化と解釈\n",
    "\n",
    "**Unit 0.4: 空間の帰納バイアス - CNN & 局所性の科学**\n",
    "\n",
    "---\n",
    "\n",
    "## このノートブックの位置づけ\n",
    "\n",
    "古典的フィルタを学んだ今、**CNNが実際に学習するカーネル**を覗いてみましょう。学習済みネットワークの第1層は、Gaborフィルタに似た構造を示すことが知られています。\n",
    "\n",
    "```\n",
    "Unit 0.4 学習マップ\n",
    "==================\n",
    "\n",
    "Section A: 畳み込みの世界への入口\n",
    "├── 80. 畳み込みとは何か（直感編）  ✅\n",
    "├── 81. 畳み込みの数学的定義  ✅\n",
    "├── 82. NumPyスクラッチ実装（基礎編）  ✅\n",
    "├── 83. NumPyスクラッチ実装（高速化編）  ✅\n",
    "├── 84. 古典的フィルタの解剖学  ✅\n",
    "└── 85. カーネルの可視化と解釈  ← 今ここ（Section A 最終）\n",
    "\n",
    "→ 次は Section B: 受容野と階層的抽象化\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習目標\n",
    "\n",
    "1. **学習済みCNNの第1層カーネルを可視化できる**\n",
    "2. **Gaborフィルタとの類似性を理解する**\n",
    "3. **深い層のカーネルが解釈困難な理由を説明できる**\n",
    "4. **特徴マップの可視化を通じてCNNの「見方」を理解する**\n",
    "5. **学習前後のカーネルの変化を観察できる**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [学習済みCNNの中身を覗く](#1-学習済みcnnの中身を覗く)\n",
    "2. [第1層のカーネル：何を見ているか](#2-第1層のカーネル何を見ているか)\n",
    "3. [深い層のカーネル：解釈の困難さ](#3-深い層のカーネル解釈の困難さ)\n",
    "4. [特徴マップの可視化](#4-特徴マップの可視化)\n",
    "5. [学習前 vs 学習後の比較](#5-学習前-vs-学習後の比較)\n",
    "6. [3DGSへの接続：学習されるガウシアン](#6-3dgsへの接続学習されるガウシアン)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 環境セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorchのインポート\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torchvision import models, transforms\n",
    "    from torchvision.models import VGG16_Weights, ResNet18_Weights\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorchが利用できません。一部の機能が制限されます。\")\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"環境準備完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 学習済みCNNの中身を覗く\n",
    "\n",
    "ImageNetで訓練された有名なCNNモデル（VGG、ResNet）のカーネルを取り出して可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # VGG16の読み込み（学習済み）\n",
    "    vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "    vgg16.eval()\n",
    "    \n",
    "    # ResNet18の読み込み（学習済み）\n",
    "    resnet18 = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    resnet18.eval()\n",
    "    \n",
    "    print(\"モデル読み込み完了\")\n",
    "    print(f\"\\nVGG16 第1層: {vgg16.features[0]}\")\n",
    "    print(f\"ResNet18 第1層: {resnet18.conv1}\")\n",
    "else:\n",
    "    print(\"PyTorchが必要です。このセクションはスキップされます。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # VGG16の構造を確認\n",
    "    print(\"VGG16 の畳み込み層一覧:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    conv_layers = []\n",
    "    for idx, layer in enumerate(vgg16.features):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            conv_layers.append((idx, layer))\n",
    "            print(f\"Layer {idx}: Conv2d(in={layer.in_channels}, out={layer.out_channels}, \"\n",
    "                  f\"kernel={layer.kernel_size}, stride={layer.stride})\")\n",
    "    \n",
    "    print(f\"\\n畳み込み層の総数: {len(conv_layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 第1層のカーネル：何を見ているか\n",
    "\n",
    "CNNの第1層は、入力画像（RGB）から直接特徴を抽出します。このカーネルは人間にも解釈可能なパターンを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_first_layer_kernels(model, model_name, layer_attr):\n",
    "    \"\"\"\n",
    "    第1層のカーネルを可視化\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        PyTorchモデル\n",
    "    model_name : str\n",
    "        モデル名（表示用）\n",
    "    layer_attr : str\n",
    "        第1層のアトリビュート名（例：'features[0]', 'conv1'）\n",
    "    \"\"\"\n",
    "    # 第1層の重みを取得\n",
    "    first_layer = eval(f\"model.{layer_attr}\")\n",
    "    weights = first_layer.weight.data.cpu().numpy()\n",
    "    \n",
    "    # 形状: (out_channels, in_channels, H, W)\n",
    "    n_filters = weights.shape[0]\n",
    "    n_channels = weights.shape[1]\n",
    "    \n",
    "    print(f\"{model_name} 第1層カーネル:\")\n",
    "    print(f\"  形状: {weights.shape}\")\n",
    "    print(f\"  フィルタ数: {n_filters}\")\n",
    "    print(f\"  入力チャネル: {n_channels} (RGB)\")\n",
    "    print(f\"  カーネルサイズ: {weights.shape[2]}×{weights.shape[3]}\")\n",
    "    \n",
    "    # 可視化（最初の64個のフィルタ）\n",
    "    n_show = min(64, n_filters)\n",
    "    n_cols = 8\n",
    "    n_rows = (n_show + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 2))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_show):\n",
    "        # RGBカーネルを可視化用に正規化\n",
    "        kernel = weights[i]  # shape: (3, H, W)\n",
    "        \n",
    "        # チャネルを最後に移動 (H, W, 3)\n",
    "        kernel = np.transpose(kernel, (1, 2, 0))\n",
    "        \n",
    "        # 0-1に正規化\n",
    "        kernel = (kernel - kernel.min()) / (kernel.max() - kernel.min() + 1e-8)\n",
    "        \n",
    "        axes[i].imshow(kernel)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'#{i}', fontsize=8)\n",
    "    \n",
    "    # 余ったaxesを非表示\n",
    "    for i in range(n_show, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} 第1層カーネル（{n_show}個を表示）', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # VGG16の第1層カーネルを可視化\n",
    "    vgg_kernels = visualize_first_layer_kernels(vgg16, \"VGG16\", \"features[0]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # ResNet18の第1層カーネルを可視化\n",
    "    resnet_kernels = visualize_first_layer_kernels(resnet18, \"ResNet18\", \"conv1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Gaborフィルタとの類似性\n",
    "\n",
    "CNNの第1層カーネルは、**Gaborフィルタ**に似たパターンを示すことが知られています。Gaborフィルタは、人間の視覚野の単純細胞の応答をモデル化したものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_kernel(size, sigma, theta, lambd, gamma, psi):\n",
    "    \"\"\"\n",
    "    Gaborフィルタカーネルを生成\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    size : int\n",
    "        カーネルサイズ\n",
    "    sigma : float\n",
    "        ガウシアンエンベロープの標準偏差\n",
    "    theta : float\n",
    "        方向（ラジアン）\n",
    "    lambd : float\n",
    "        正弦波の波長\n",
    "    gamma : float\n",
    "        空間アスペクト比\n",
    "    psi : float\n",
    "        位相オフセット\n",
    "    \"\"\"\n",
    "    center = size // 2\n",
    "    x = np.arange(size) - center\n",
    "    y = np.arange(size) - center\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # 回転\n",
    "    x_theta = X * np.cos(theta) + Y * np.sin(theta)\n",
    "    y_theta = -X * np.sin(theta) + Y * np.cos(theta)\n",
    "    \n",
    "    # Gabor関数\n",
    "    gaussian = np.exp(-(x_theta**2 + gamma**2 * y_theta**2) / (2 * sigma**2))\n",
    "    sinusoid = np.cos(2 * np.pi * x_theta / lambd + psi)\n",
    "    \n",
    "    return gaussian * sinusoid\n",
    "\n",
    "# 様々な方向のGaborフィルタを生成\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "\n",
    "orientations = np.linspace(0, np.pi, 8, endpoint=False)\n",
    "\n",
    "for i, theta in enumerate(orientations):\n",
    "    # 実部（偶対称）\n",
    "    gabor_real = gabor_kernel(11, sigma=2.0, theta=theta, lambd=4.0, gamma=0.5, psi=0)\n",
    "    axes[0, i].imshow(gabor_real, cmap='RdBu')\n",
    "    axes[0, i].set_title(f'{np.degrees(theta):.0f}°', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # 虚部（奇対称）\n",
    "    gabor_imag = gabor_kernel(11, sigma=2.0, theta=theta, lambd=4.0, gamma=0.5, psi=np.pi/2)\n",
    "    axes[1, i].imshow(gabor_imag, cmap='RdBu')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('偶対称', fontsize=10)\n",
    "axes[1, 0].set_ylabel('奇対称', fontsize=10)\n",
    "\n",
    "plt.suptitle('Gaborフィルタバンク（様々な方向）', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGaborフィルタの特徴:\")\n",
    "print(\"- 特定の方向と周波数に選択的に応答\")\n",
    "print(\"- 人間の視覚野の単純細胞に類似\")\n",
    "print(\"- CNNの第1層は、学習によってこれに近いパターンを獲得する\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # VGGカーネルとGaborの比較\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    \n",
    "    # 上段：VGG16の特徴的なカーネル（手動で選択）\n",
    "    selected_indices = [0, 3, 7, 12, 18, 24, 32, 45]  # エッジ検出的なもの\n",
    "    \n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        kernel = vgg_kernels[idx]\n",
    "        # グレースケール化（3チャネルの平均）\n",
    "        kernel_gray = kernel.mean(axis=0)\n",
    "        axes[0, i].imshow(kernel_gray, cmap='RdBu')\n",
    "        axes[0, i].set_title(f'VGG #{idx}', fontsize=9)\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # 下段：類似のGaborフィルタ\n",
    "    gabor_params = [\n",
    "        (0, 4.0), (np.pi/4, 4.0), (np.pi/2, 4.0), (3*np.pi/4, 4.0),\n",
    "        (0, 6.0), (np.pi/4, 6.0), (np.pi/2, 6.0), (3*np.pi/4, 6.0),\n",
    "    ]\n",
    "    \n",
    "    for i, (theta, lambd) in enumerate(gabor_params):\n",
    "        gabor = gabor_kernel(3, sigma=1.0, theta=theta, lambd=lambd, gamma=0.5, psi=0)\n",
    "        axes[1, i].imshow(gabor, cmap='RdBu')\n",
    "        axes[1, i].set_title(f'Gabor {np.degrees(theta):.0f}°', fontsize=9)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('VGG16学習済み', fontsize=10)\n",
    "    axes[1, 0].set_ylabel('Gabor（参考）', fontsize=10)\n",
    "    \n",
    "    plt.suptitle('学習されたカーネル vs Gaborフィルタ', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 深い層のカーネル：解釈の困難さ\n",
    "\n",
    "第2層以降のカーネルは、直接可視化しても解釈が困難です。なぜでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # 各層のカーネルサイズを確認\n",
    "    print(\"VGG16 畳み込み層のカーネル情報:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'層':>10} {'入力Ch':>10} {'出力Ch':>10} {'カーネル':>10} {'パラメータ数':>15}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for idx, layer in conv_layers[:8]:  # 最初の8層\n",
    "        w = layer.weight.data\n",
    "        n_params = w.numel()\n",
    "        print(f\"Layer {idx:>3} {w.shape[1]:>10} {w.shape[0]:>10} \"\n",
    "              f\"{w.shape[2]}×{w.shape[3]:>6} {n_params:>15,}\")\n",
    "    \n",
    "    print(\"\\n注目ポイント:\")\n",
    "    print(\"- 第1層: 3チャネル（RGB）→ 人間が解釈可能\")\n",
    "    print(\"- 第2層以降: 64チャネル以上 → 64次元空間のパターン\")\n",
    "    print(\"- 深くなるほど、カーネルは『抽象的な特徴の組み合わせ』を学習\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # 第2層のカーネルを可視化（試み）\n",
    "    \n",
    "    layer2 = vgg16.features[2]  # 2番目の畳み込み層\n",
    "    weights2 = layer2.weight.data.cpu().numpy()\n",
    "    \n",
    "    print(f\"第2層カーネルの形状: {weights2.shape}\")\n",
    "    print(f\"  → {weights2.shape[0]}個のフィルタ、各フィルタは{weights2.shape[1]}チャネル\")\n",
    "    \n",
    "    # 1つのフィルタ（64チャネル）を可視化\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "    \n",
    "    filter_idx = 0  # 最初のフィルタ\n",
    "    kernel = weights2[filter_idx]  # shape: (64, 3, 3)\n",
    "    \n",
    "    for i in range(64):\n",
    "        row, col = i // 8, i % 8\n",
    "        channel_kernel = kernel[i]  # shape: (3, 3)\n",
    "        \n",
    "        # 正規化\n",
    "        vmax = np.abs(channel_kernel).max()\n",
    "        axes[row, col].imshow(channel_kernel, cmap='RdBu', vmin=-vmax, vmax=vmax)\n",
    "        axes[row, col].axis('off')\n",
    "        axes[row, col].set_title(f'Ch{i}', fontsize=7)\n",
    "    \n",
    "    plt.suptitle(f'第2層フィルタ#{filter_idx}の64チャネルカーネル\\n（各3×3が異なる入力チャネルに対応）', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n解釈の困難さ:\")\n",
    "    print(\"- 64個の3×3カーネルの『組み合わせ』が1つの特徴検出器\")\n",
    "    print(\"- 人間には直感的に理解できない\")\n",
    "    print(\"- だからこそ、特徴マップの可視化が重要\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 特徴マップの可視化\n",
    "\n",
    "カーネルそのものではなく、**カーネルが画像に対して何に反応するか**を見ることで、CNNの動作を理解できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_image(size=224):\n",
    "    \"\"\"テスト用のサンプル画像を作成\"\"\"\n",
    "    img = np.zeros((size, size, 3))\n",
    "    \n",
    "    # 背景（グラデーション）\n",
    "    for c in range(3):\n",
    "        x = np.linspace(0, 1, size)\n",
    "        y = np.linspace(0, 1, size)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        img[:, :, c] = 0.2 * X + 0.1 * Y + 0.1\n",
    "    \n",
    "    # 赤い四角形\n",
    "    img[30:80, 30:80, 0] = 0.9\n",
    "    img[30:80, 30:80, 1] = 0.2\n",
    "    img[30:80, 30:80, 2] = 0.2\n",
    "    \n",
    "    # 緑の円\n",
    "    yy, xx = np.ogrid[:size, :size]\n",
    "    center = (150, 60)\n",
    "    radius = 30\n",
    "    mask = (xx - center[0])**2 + (yy - center[1])**2 <= radius**2\n",
    "    img[mask, 0] = 0.2\n",
    "    img[mask, 1] = 0.8\n",
    "    img[mask, 2] = 0.2\n",
    "    \n",
    "    # 青い三角形\n",
    "    for i in range(60):\n",
    "        start = 130 + i // 2\n",
    "        end = 190 - i // 2\n",
    "        if 100 + i < size:\n",
    "            img[100 + i, start:end, 0] = 0.2\n",
    "            img[100 + i, start:end, 1] = 0.2\n",
    "            img[100 + i, start:end, 2] = 0.9\n",
    "    \n",
    "    # 白黒のエッジ\n",
    "    img[180:200, 30:120, :] = 0.9\n",
    "    img[180:200, 120:210, :] = 0.1\n",
    "    \n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "sample_img = create_sample_image()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(sample_img)\n",
    "plt.title('テスト画像（様々な色・形・エッジ）', fontsize=12)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    def get_feature_maps(model, image, layer_indices):\n",
    "        \"\"\"\n",
    "        指定した層の特徴マップを取得\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : VGG model\n",
    "        image : ndarray (H, W, 3)\n",
    "            0-1の範囲の画像\n",
    "        layer_indices : list\n",
    "            特徴マップを取得する層のインデックス\n",
    "        \"\"\"\n",
    "        # 前処理\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # NumPy → Tensor\n",
    "        img_tensor = transform(image.astype(np.float32)).unsqueeze(0)\n",
    "        \n",
    "        # 各層の出力を記録\n",
    "        feature_maps = {}\n",
    "        x = img_tensor\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx, layer in enumerate(model.features):\n",
    "                x = layer(x)\n",
    "                if idx in layer_indices:\n",
    "                    feature_maps[idx] = x.squeeze().cpu().numpy()\n",
    "        \n",
    "        return feature_maps\n",
    "    \n",
    "    # 特徴マップを取得（第1, 2, 5, 10層）\n",
    "    layer_indices = [0, 2, 5, 10]\n",
    "    feature_maps = get_feature_maps(vgg16, sample_img, layer_indices)\n",
    "    \n",
    "    print(\"取得した特徴マップ:\")\n",
    "    for idx, fmap in feature_maps.items():\n",
    "        print(f\"  Layer {idx}: shape {fmap.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # 特徴マップの可視化\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = GridSpec(4, 9, figure=fig)\n",
    "    \n",
    "    # 元画像\n",
    "    ax_orig = fig.add_subplot(gs[:2, :2])\n",
    "    ax_orig.imshow(sample_img)\n",
    "    ax_orig.set_title('入力画像', fontsize=12)\n",
    "    ax_orig.axis('off')\n",
    "    \n",
    "    # 各層の特徴マップ\n",
    "    layer_names = ['Layer 0\\n(Conv1)', 'Layer 2\\n(Conv2)', 'Layer 5\\n(Conv3)', 'Layer 10\\n(Conv5)']\n",
    "    \n",
    "    for row, (layer_idx, fmap) in enumerate(feature_maps.items()):\n",
    "        # 層名\n",
    "        ax_label = fig.add_subplot(gs[row, 2])\n",
    "        ax_label.text(0.5, 0.5, layer_names[row], fontsize=11, ha='center', va='center')\n",
    "        ax_label.axis('off')\n",
    "        \n",
    "        # 最初の6チャネルを表示\n",
    "        for ch in range(min(6, fmap.shape[0])):\n",
    "            ax = fig.add_subplot(gs[row, 3 + ch])\n",
    "            ax.imshow(fmap[ch], cmap='viridis')\n",
    "            ax.set_title(f'Ch{ch}', fontsize=9)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('VGG16 各層の特徴マップ（各行：1つの層、各列：異なるチャネル）', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n観察ポイント:\")\n",
    "    print(\"- 浅い層：エッジ、色、テクスチャなど低レベル特徴に反応\")\n",
    "    print(\"- 深い層：より抽象的な特徴（物体の部品など）に反応\")\n",
    "    print(\"- 解像度が徐々に低下（プーリングの効果）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # 特定のチャネルが何に反応しているか詳しく見る\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(16, 7))\n",
    "    \n",
    "    layer0_fmap = feature_maps[0]  # 第1層の特徴マップ\n",
    "    \n",
    "    # 最も活性化が強いチャネルと弱いチャネルを見つける\n",
    "    channel_activations = [layer0_fmap[ch].max() for ch in range(layer0_fmap.shape[0])]\n",
    "    sorted_channels = np.argsort(channel_activations)[::-1]\n",
    "    \n",
    "    # 上段：活性化が強いチャネル\n",
    "    axes[0, 0].imshow(sample_img)\n",
    "    axes[0, 0].set_title('入力画像', fontsize=11)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    for i, ch in enumerate(sorted_channels[:4]):\n",
    "        axes[0, i+1].imshow(layer0_fmap[ch], cmap='hot')\n",
    "        axes[0, i+1].set_title(f'Ch{ch} (高活性)', fontsize=11)\n",
    "        axes[0, i+1].axis('off')\n",
    "    \n",
    "    # 下段：対応するカーネル\n",
    "    axes[1, 0].axis('off')\n",
    "    axes[1, 0].text(0.5, 0.5, '対応する\\nカーネル', fontsize=12, ha='center', va='center')\n",
    "    \n",
    "    for i, ch in enumerate(sorted_channels[:4]):\n",
    "        kernel = vgg_kernels[ch]\n",
    "        kernel_vis = np.transpose(kernel, (1, 2, 0))\n",
    "        kernel_vis = (kernel_vis - kernel_vis.min()) / (kernel_vis.max() - kernel_vis.min())\n",
    "        \n",
    "        # 拡大表示\n",
    "        kernel_large = np.repeat(np.repeat(kernel_vis, 20, axis=0), 20, axis=1)\n",
    "        axes[1, i+1].imshow(kernel_large)\n",
    "        axes[1, i+1].set_title(f'Filter #{ch}', fontsize=11)\n",
    "        axes[1, i+1].axis('off')\n",
    "    \n",
    "    plt.suptitle('特徴マップとカーネルの対応関係', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 学習前 vs 学習後の比較\n",
    "\n",
    "ランダム初期化されたカーネルと、学習後のカーネルを比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # 未学習のVGG16を作成\n",
    "    vgg16_untrained = models.vgg16(weights=None)\n",
    "    \n",
    "    # 第1層カーネルを取得\n",
    "    untrained_kernels = vgg16_untrained.features[0].weight.data.cpu().numpy()\n",
    "    trained_kernels = vgg_kernels\n",
    "    \n",
    "    # 比較可視化\n",
    "    fig, axes = plt.subplots(4, 16, figsize=(20, 5))\n",
    "    \n",
    "    # 上2行：未学習\n",
    "    for i in range(32):\n",
    "        row = i // 16\n",
    "        col = i % 16\n",
    "        kernel = untrained_kernels[i]\n",
    "        kernel_vis = np.transpose(kernel, (1, 2, 0))\n",
    "        kernel_vis = (kernel_vis - kernel_vis.min()) / (kernel_vis.max() - kernel_vis.min() + 1e-8)\n",
    "        axes[row, col].imshow(kernel_vis)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # 下2行：学習済み\n",
    "    for i in range(32):\n",
    "        row = 2 + i // 16\n",
    "        col = i % 16\n",
    "        kernel = trained_kernels[i]\n",
    "        kernel_vis = np.transpose(kernel, (1, 2, 0))\n",
    "        kernel_vis = (kernel_vis - kernel_vis.min()) / (kernel_vis.max() - kernel_vis.min() + 1e-8)\n",
    "        axes[row, col].imshow(kernel_vis)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('未学習', fontsize=12)\n",
    "    axes[2, 0].set_ylabel('学習済み', fontsize=12)\n",
    "    \n",
    "    plt.suptitle('ランダム初期化 vs 学習済みカーネル（VGG16 第1層）', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n観察:\")\n",
    "    print(\"- 未学習: ランダムなノイズパターン（構造なし）\")\n",
    "    print(\"- 学習済み: 明確な構造（エッジ、色のパターン）\")\n",
    "    print(\"- 学習によって『意味のある』フィルタが創発する\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    # カーネルの統計的特性の比較\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 重みの分布\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(untrained_kernels.flatten(), bins=50, alpha=0.7, label='未学習', density=True)\n",
    "    ax1.hist(trained_kernels.flatten(), bins=50, alpha=0.7, label='学習済み', density=True)\n",
    "    ax1.set_xlabel('重み値')\n",
    "    ax1.set_ylabel('密度')\n",
    "    ax1.set_title('重みの分布', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 各フィルタの標準偏差\n",
    "    ax2 = axes[1]\n",
    "    untrained_stds = [untrained_kernels[i].std() for i in range(64)]\n",
    "    trained_stds = [trained_kernels[i].std() for i in range(64)]\n",
    "    ax2.scatter(range(64), untrained_stds, alpha=0.7, label='未学習')\n",
    "    ax2.scatter(range(64), trained_stds, alpha=0.7, label='学習済み')\n",
    "    ax2.set_xlabel('フィルタ番号')\n",
    "    ax2.set_ylabel('標準偏差')\n",
    "    ax2.set_title('各フィルタの重みの標準偏差', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # フィルタ間の相関\n",
    "    ax3 = axes[2]\n",
    "    trained_flat = trained_kernels.reshape(64, -1)\n",
    "    correlation = np.corrcoef(trained_flat)\n",
    "    im = ax3.imshow(correlation, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    ax3.set_xlabel('フィルタ番号')\n",
    "    ax3.set_ylabel('フィルタ番号')\n",
    "    ax3.set_title('学習済みフィルタ間の相関', fontsize=12)\n",
    "    plt.colorbar(im, ax=ax3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n統計的特徴:\")\n",
    "    print(f\"- 未学習の重みの標準偏差: {np.std(untrained_kernels):.4f}\")\n",
    "    print(f\"- 学習済みの重みの標準偏差: {np.std(trained_kernels):.4f}\")\n",
    "    print(\"- 学習済みフィルタは多様性がある（相関が低い）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 3DGSへの接続：学習されるガウシアン\n",
    "\n",
    "CNNのカーネルと3D Gaussian Splattingの類似性を考察します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNと3DGSの類似性\n",
    "\n",
    "comparison = \"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║              CNNカーネル vs 3Dガウシアン：学習される表現                       ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║  【共通点】                                                                    ║\n",
    "║                                                                               ║\n",
    "║  1. 局所的な重み付け                                                          ║\n",
    "║     CNN: 3×3などのカーネルで近傍ピクセルを重み付け                            ║\n",
    "║     3DGS: ガウシアン関数で3D空間の近傍点を重み付け                            ║\n",
    "║                                                                               ║\n",
    "║  2. 学習によるパラメータ最適化                                                ║\n",
    "║     CNN: バックプロパゲーションでカーネル重みを更新                           ║\n",
    "║     3DGS: 勾配降下でガウシアンの位置・形状・色を更新                          ║\n",
    "║                                                                               ║\n",
    "║  3. 階層的/多スケール表現                                                      ║\n",
    "║     CNN: 深い層ほど大きな受容野（抽象的特徴）                                 ║\n",
    "║     3DGS: 異なるサイズのガウシアンで細部〜大域を表現                          ║\n",
    "║                                                                               ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║  【相違点】                                                                    ║\n",
    "║                                                                               ║\n",
    "║  • CNN: 離散的なグリッド上で動作、カーネルは全画像で共有                      ║\n",
    "║  • 3DGS: 連続的な3D空間、各ガウシアンは固有のパラメータ                       ║\n",
    "║                                                                               ║\n",
    "║  • CNN: 特徴抽出（認識タスク向け）                                            ║\n",
    "║  • 3DGS: シーン表現（レンダリング向け）                                       ║\n",
    "║                                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ガウシアンの「学習」を模擬的に可視化\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# ターゲット画像（単純な形状）\n",
    "target = np.zeros((64, 64))\n",
    "target[20:45, 20:45] = 1.0\n",
    "\n",
    "# ランダムなガウシアンの集合（初期状態）\n",
    "np.random.seed(42)\n",
    "n_gaussians = 50\n",
    "\n",
    "def render_gaussians(positions, scales, intensities, size=64):\n",
    "    \"\"\"ガウシアンの集合を画像にレンダリング\"\"\"\n",
    "    img = np.zeros((size, size))\n",
    "    y, x = np.ogrid[:size, :size]\n",
    "    \n",
    "    for pos, scale, intensity in zip(positions, scales, intensities):\n",
    "        gaussian = np.exp(-((x - pos[0])**2 + (y - pos[1])**2) / (2 * scale**2))\n",
    "        img += intensity * gaussian\n",
    "    \n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "# 初期状態（ランダム）\n",
    "init_positions = np.random.rand(n_gaussians, 2) * 64\n",
    "init_scales = np.random.rand(n_gaussians) * 5 + 2\n",
    "init_intensities = np.random.rand(n_gaussians) * 0.5\n",
    "\n",
    "# 「学習後」状態（ターゲットに合わせて配置）\n",
    "learned_positions = []\n",
    "learned_scales = []\n",
    "learned_intensities = []\n",
    "\n",
    "for _ in range(n_gaussians):\n",
    "    if np.random.rand() < 0.7:  # 70%を四角形内に配置\n",
    "        pos = [np.random.uniform(20, 45), np.random.uniform(20, 45)]\n",
    "        intensity = np.random.uniform(0.3, 0.6)\n",
    "    else:  # 30%を外に（背景）\n",
    "        pos = [np.random.uniform(0, 64), np.random.uniform(0, 64)]\n",
    "        intensity = np.random.uniform(-0.1, 0.1)\n",
    "    learned_positions.append(pos)\n",
    "    learned_scales.append(np.random.uniform(2, 4))\n",
    "    learned_intensities.append(intensity)\n",
    "\n",
    "learned_positions = np.array(learned_positions)\n",
    "learned_scales = np.array(learned_scales)\n",
    "learned_intensities = np.array(learned_intensities)\n",
    "\n",
    "# レンダリング\n",
    "init_render = render_gaussians(init_positions, init_scales, init_intensities)\n",
    "learned_render = render_gaussians(learned_positions, learned_scales, learned_intensities)\n",
    "\n",
    "# 可視化\n",
    "axes[0].imshow(target, cmap='gray', vmin=0, vmax=1)\n",
    "axes[0].set_title('ターゲット', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(init_render, cmap='gray', vmin=0, vmax=1)\n",
    "axes[1].scatter(init_positions[:, 0], init_positions[:, 1], c='red', s=10, alpha=0.5)\n",
    "axes[1].set_title('初期状態\\n（ランダムなガウシアン）', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(learned_render, cmap='gray', vmin=0, vmax=1)\n",
    "axes[2].scatter(learned_positions[:, 0], learned_positions[:, 1], c='red', s=10, alpha=0.5)\n",
    "axes[2].set_title('学習後\\n（最適化されたガウシアン）', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(np.abs(target - learned_render), cmap='hot')\n",
    "axes[3].set_title('誤差', fontsize=12)\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.suptitle('ガウシアンの「学習」による表現獲得（3DGSの2D簡易版）', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n共通する本質:\")\n",
    "print(\"- 初期状態はランダム（意味のない構造）\")\n",
    "print(\"- 学習によってターゲットを表現できる構造を獲得\")\n",
    "print(\"- CNNもGSも、データから『表現』を学習する\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## まとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    Section A 完了：畳み込みの世界への入口                      ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║  【学んだこと】                                                                ║\n",
    "║                                                                               ║\n",
    "║  80. 畳み込みの直感的理解                                                     ║\n",
    "║      → 「周囲を見て自分の値を決める」操作                                     ║\n",
    "║                                                                               ║\n",
    "║  81. 畳み込みの数学的定義                                                     ║\n",
    "║      → 数式、パディング、ストライドの意味                                     ║\n",
    "║                                                                               ║\n",
    "║  82-83. NumPyスクラッチ実装                                                   ║\n",
    "║      → forループ版から高速ベクトル化版へ                                      ║\n",
    "║                                                                               ║\n",
    "║  84. 古典的フィルタ                                                           ║\n",
    "║      → 平滑化、エッジ検出、シャープ化の原理                                   ║\n",
    "║                                                                               ║\n",
    "║  85. カーネルの可視化と解釈                                                   ║\n",
    "║      → 学習済みCNNのカーネル、Gaborフィルタとの類似性                         ║\n",
    "║                                                                               ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║  【次のSection B: 受容野と階層的抽象化】                                       ║\n",
    "║                                                                               ║\n",
    "║  86. 受容野入門：「視野」という概念                                           ║\n",
    "║  87. 層を重ねる：深さが生む抽象化                                             ║\n",
    "║  88. ダウンサンプリング：効率的に視野を広げる                                 ║\n",
    "║  89. 受容野と3DGS：空間的影響範囲の対比                                       ║\n",
    "║                                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 次のステップ\n",
    "\n",
    "**Section B: 受容野と階層的抽象化** に進みます。\n",
    "\n",
    "次のノートブック **86. 受容野入門** では：\n",
    "\n",
    "- 受容野（Receptive Field）の直感的理解\n",
    "- 1つのピクセルが「世界のどこを見ているか」\n",
    "- 受容野の計算と可視化ツールの作成\n",
    "\n",
    "畳み込みの「局所性」がどのように「大域的な理解」につながるかを探求します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
