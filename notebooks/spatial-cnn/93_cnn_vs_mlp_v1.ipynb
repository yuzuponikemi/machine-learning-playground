{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 93. CNN vs MLP：帰納バイアスの効果を実証\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックでは、以下を学びます：\n",
    "\n",
    "1. **MLP（多層パーセプトロン）**の構造と特性\n",
    "2. **CNN vs MLP**のパラメータ効率比較\n",
    "3. **実験による性能比較**\n",
    "4. **帰納バイアスの効果**の実証\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. [MLPの復習](#section1)\n",
    "2. [構造の比較](#section2)\n",
    "3. [MNIST実験](#section3)\n",
    "4. [位置シフト実験](#section4)\n",
    "5. [まとめ](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1. MLPの復習\n",
    "\n",
    "### MLP（多層パーセプトロン）とは\n",
    "\n",
    "- 全結合層のみで構成されるニューラルネットワーク\n",
    "- 各ニューロンは前層の**すべての**ニューロンと接続\n",
    "- 空間的な構造を考慮しない（画像を1次元ベクトルとして扱う）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mlp_vs_cnn_structure():\n",
    "    \"\"\"MLPとCNNの構造比較\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # MLP\n",
    "    ax = axes[0]\n",
    "    \n",
    "    # 入力層（flatten済み）\n",
    "    input_neurons = 6\n",
    "    for i in range(input_neurons):\n",
    "        circle = plt.Circle((0.2, 0.9 - i*0.12), 0.03, color='lightblue', ec='blue')\n",
    "        ax.add_patch(circle)\n",
    "    ax.text(0.2, 0.97, '入力\\n(flatten)', ha='center', fontsize=10)\n",
    "    \n",
    "    # 隠れ層\n",
    "    hidden_neurons = 4\n",
    "    for i in range(hidden_neurons):\n",
    "        circle = plt.Circle((0.5, 0.8 - i*0.15), 0.03, color='lightyellow', ec='orange')\n",
    "        ax.add_patch(circle)\n",
    "    ax.text(0.5, 0.92, '隠れ層', ha='center', fontsize=10)\n",
    "    \n",
    "    # 出力層\n",
    "    output_neurons = 3\n",
    "    for i in range(output_neurons):\n",
    "        circle = plt.Circle((0.8, 0.75 - i*0.15), 0.03, color='lightgreen', ec='green')\n",
    "        ax.add_patch(circle)\n",
    "    ax.text(0.8, 0.92, '出力', ha='center', fontsize=10)\n",
    "    \n",
    "    # 全結合の接続（一部）\n",
    "    for i in range(input_neurons):\n",
    "        for j in range(hidden_neurons):\n",
    "            ax.plot([0.23, 0.47], [0.9 - i*0.12, 0.8 - j*0.15], \n",
    "                   'gray', alpha=0.2, linewidth=0.5)\n",
    "    \n",
    "    for i in range(hidden_neurons):\n",
    "        for j in range(output_neurons):\n",
    "            ax.plot([0.53, 0.77], [0.8 - i*0.15, 0.75 - j*0.15], \n",
    "                   'gray', alpha=0.2, linewidth=0.5)\n",
    "    \n",
    "    ax.text(0.5, 0.1, '全結合：すべてのニューロンが接続\\n→ 空間構造を無視', \n",
    "           ha='center', fontsize=11, bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('MLP（多層パーセプトロン）', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # CNN\n",
    "    ax = axes[1]\n",
    "    \n",
    "    # 入力（2Dグリッド）\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            rect = plt.Rectangle((0.1 + j*0.06, 0.7 - i*0.06), 0.05, 0.05,\n",
    "                                 facecolor='lightblue', edgecolor='blue')\n",
    "            ax.add_patch(rect)\n",
    "    ax.text(0.19, 0.82, '入力\\n(2D)', ha='center', fontsize=10)\n",
    "    \n",
    "    # カーネル\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            rect = plt.Rectangle((0.38 + j*0.04, 0.62 - i*0.04), 0.03, 0.03,\n",
    "                                 facecolor='lightyellow', edgecolor='orange', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "    ax.text(0.42, 0.72, 'カーネル\\n(共有)', ha='center', fontsize=10)\n",
    "    \n",
    "    # 出力（特徴マップ）\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            rect = plt.Rectangle((0.65 + j*0.06, 0.65 - i*0.06), 0.05, 0.05,\n",
    "                                 facecolor='lightgreen', edgecolor='green')\n",
    "            ax.add_patch(rect)\n",
    "    ax.text(0.71, 0.82, '特徴マップ\\n(2D)', ha='center', fontsize=10)\n",
    "    \n",
    "    # 局所接続を示す\n",
    "    ax.annotate('', xy=(0.35, 0.6), xytext=(0.25, 0.6),\n",
    "               arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "    ax.annotate('', xy=(0.62, 0.6), xytext=(0.52, 0.6),\n",
    "               arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "    \n",
    "    ax.text(0.5, 0.1, '局所接続 + 重み共有\\n→ 空間構造を活用', \n",
    "           ha='center', fontsize=11, bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('CNN（畳み込みニューラルネットワーク）', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('MLP vs CNN の構造比較', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_mlp_vs_cnn_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## 2. 構造の比較\n",
    "\n",
    "### モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"シンプルなMLP\"\"\"\n",
    "    def __init__(self, input_size=784, hidden_size=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"シンプルなCNN\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))  # 28->14\n",
    "        x = self.pool(self.relu(self.conv2(x)))  # 14->7\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# パラメータ数を比較\n",
    "mlp = MLP()\n",
    "cnn = SimpleCNN()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"パラメータ数の比較\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MLP:  {count_parameters(mlp):,}\")\n",
    "print(f\"CNN:  {count_parameters(cnn):,}\")\n",
    "print(f\"比率: MLP/CNN = {count_parameters(mlp)/count_parameters(cnn):.2f}倍\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_layers():\n",
    "    \"\"\"各層のパラメータ数を分析\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"層ごとのパラメータ数\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n【MLP】\")\n",
    "    for name, param in mlp.named_parameters():\n",
    "        print(f\"  {name}: {param.shape} = {param.numel():,}\")\n",
    "    \n",
    "    print(\"\\n【CNN】\")\n",
    "    for name, param in cnn.named_parameters():\n",
    "        print(f\"  {name}: {param.shape} = {param.numel():,}\")\n",
    "\n",
    "analyze_model_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 3. MNIST実験\n",
    "\n",
    "MNISTデータセットでMLPとCNNを比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# MNISTをダウンロード\n",
    "try:\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    # 小さいサブセットで実験（高速化のため）\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, range(5000))\n",
    "    test_subset = torch.utils.data.Subset(test_dataset, range(1000))\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    print(f\"訓練データ: {len(train_subset)}\")\n",
    "    print(f\"テストデータ: {len(test_subset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"データのダウンロードに失敗: {e}\")\n",
    "    print(\"合成データで代用します\")\n",
    "    \n",
    "    # 合成データ\n",
    "    X_train = torch.randn(5000, 1, 28, 28)\n",
    "    y_train = torch.randint(0, 10, (5000,))\n",
    "    X_test = torch.randn(1000, 1, 28, 28)\n",
    "    y_test = torch.randint(0, 10, (1000,))\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=5, lr=0.001):\n",
    "    \"\"\"モデルを訓練\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 訓練\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        \n",
    "        # テスト\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        test_acc = 100 * correct / total\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss={train_losses[-1]:.4f}, Acc={test_acc:.2f}%\")\n",
    "    \n",
    "    return train_losses, test_accs\n",
    "\n",
    "# MLPの訓練\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MLPの訓練\")\n",
    "print(\"=\"*50)\n",
    "mlp = MLP().to(device)\n",
    "mlp_losses, mlp_accs = train_model(mlp, train_loader, test_loader, epochs=10)\n",
    "\n",
    "# CNNの訓練\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CNNの訓練\")\n",
    "print(\"=\"*50)\n",
    "cnn = SimpleCNN().to(device)\n",
    "cnn_losses, cnn_accs = train_model(cnn, train_loader, test_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_comparison():\n",
    "    \"\"\"訓練結果の比較\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 損失\n",
    "    axes[0].plot(mlp_losses, 'b-o', label='MLP')\n",
    "    axes[0].plot(cnn_losses, 'r-s', label='CNN')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('訓練損失')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 精度\n",
    "    axes[1].plot(mlp_accs, 'b-o', label='MLP')\n",
    "    axes[1].plot(cnn_accs, 'r-s', label='CNN')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('テスト精度')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('MLP vs CNN の学習曲線', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n最終テスト精度:\")\n",
    "    print(f\"  MLP: {mlp_accs[-1]:.2f}%\")\n",
    "    print(f\"  CNN: {cnn_accs[-1]:.2f}%\")\n",
    "\n",
    "plot_training_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## 4. 位置シフト実験\n",
    "\n",
    "平行移動に対する頑健性を比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_shift_robustness(model, test_loader, shift_amounts):\n",
    "    \"\"\"シフトに対する頑健性をテスト\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for shift in shift_amounts:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                # 画像をシフト\n",
    "                if shift != 0:\n",
    "                    data = torch.roll(data, shifts=shift, dims=2)  # 水平シフト\n",
    "                \n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        results.append(acc)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# シフト量のリスト\n",
    "shift_amounts = list(range(-10, 11, 2))\n",
    "\n",
    "# テスト\n",
    "mlp_shift_accs = test_shift_robustness(mlp, test_loader, shift_amounts)\n",
    "cnn_shift_accs = test_shift_robustness(cnn, test_loader, shift_amounts)\n",
    "\n",
    "# 結果をプロット\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(shift_amounts, mlp_shift_accs, 'b-o', label='MLP', linewidth=2, markersize=8)\n",
    "plt.plot(shift_amounts, cnn_shift_accs, 'r-s', label='CNN', linewidth=2, markersize=8)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('水平シフト量（ピクセル）', fontsize=12)\n",
    "plt.ylabel('精度 (%)', fontsize=12)\n",
    "plt.title('位置シフトに対する頑健性\\nCNNは重み共有により位置変化に強い', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nシフトに対する精度低下:\")\n",
    "print(f\"  MLP: シフト0→±10で {mlp_shift_accs[5]:.1f}% → {mlp_shift_accs[0]:.1f}%\")\n",
    "print(f\"  CNN: シフト0→±10で {cnn_shift_accs[5]:.1f}% → {cnn_shift_accs[0]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 5. まとめ\n",
    "\n",
    "### 実験結果\n",
    "\n",
    "| 観点 | MLP | CNN |\n",
    "|------|-----|-----|\n",
    "| パラメータ数 | 多い | 少ない |\n",
    "| 学習速度 | 遅い | 速い |\n",
    "| 最終精度 | 低い | 高い |\n",
    "| シフト頑健性 | 低い | 高い |\n",
    "\n",
    "### 帰納バイアスの効果\n",
    "\n",
    "CNNの帰納バイアス（局所性、重み共有、平行移動等変性）は：\n",
    "- パラメータ効率を向上\n",
    "- 学習を高速化\n",
    "- 汎化性能を向上\n",
    "- 位置変化への頑健性を付与"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 次のノートブック\n",
    "\n",
    "次のノートブックでは、**CNNが苦手なケース**について学びます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
