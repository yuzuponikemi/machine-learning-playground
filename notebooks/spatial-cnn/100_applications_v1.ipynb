{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100. 空間的CNNの実応用：ケーススタディ\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックでは、以下を学びます：\n",
    "\n",
    "1. **医療画像解析**での空間的CNN\n",
    "2. **自動運転**におけるセグメンテーション\n",
    "3. **衛星画像解析**の応用\n",
    "4. **3D再構成**とNeRF/3DGSへの接続\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. [医療画像解析](#section1)\n",
    "2. [自動運転](#section2)\n",
    "3. [衛星画像解析](#section3)\n",
    "4. [3D再構成への応用](#section4)\n",
    "5. [まとめ](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1. 医療画像解析\n",
    "\n",
    "### U-Netの誕生と医療分野への貢献\n",
    "\n",
    "U-Netは2015年、医療画像セグメンテーションのために開発されました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_medical_applications():\n",
    "    \"\"\"医療画像解析の応用例を可視化\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    applications = [\n",
    "        ('細胞セグメンテーション', 'U-Net（オリジナル）', '顕微鏡画像から細胞境界を検出'),\n",
    "        ('腫瘍検出', 'CNN + セグメンテーション', 'CT/MRIから腫瘍領域を特定'),\n",
    "        ('網膜血管検出', 'U-Net + Attention', '眼底画像から血管構造を抽出'),\n",
    "        ('臓器セグメンテーション', '3D U-Net', 'CTスキャンから臓器を3D分割'),\n",
    "        ('骨折検出', 'CNN + 分類', 'X線画像から骨折を検出'),\n",
    "        ('皮膚病変分類', 'CNN + セグメンテーション', '皮膚画像から病変を分類'),\n",
    "    ]\n",
    "    \n",
    "    colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral', 'lavender', 'lightpink']\n",
    "    \n",
    "    for ax, (title, method, desc), color in zip(axes.flat, applications, colors):\n",
    "        ax.set_facecolor(color)\n",
    "        ax.text(0.5, 0.7, title, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        ax.text(0.5, 0.45, f'手法: {method}', ha='center', va='center', fontsize=11)\n",
    "        ax.text(0.5, 0.25, desc, ha='center', va='center', fontsize=10, wrap=True)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('医療画像解析における空間的CNNの応用', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\nvisualize_medical_applications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_medical_requirements():\n",
    "    \"\"\"医療画像解析の特有の要件\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"医療画像解析の特有の要件\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\"\"\n",
    "【データの特徴】\n",
    "  ・データ量が少ない（アノテーションコストが高い）\n",
    "  ・高解像度・高精度が必要\n",
    "  ・3Dデータ（CT、MRI）が多い\n",
    "  ・クラス不均衡（病変領域は小さい）\n",
    "\n",
    "【なぜU-Netが適しているか】\n",
    "  ・少ないデータでも学習可能\n",
    "  ・スキップ接続で詳細な境界を保持\n",
    "  ・データ拡張との相性が良い\n",
    "\n",
    "【よく使われる損失関数】\n",
    "  ・Dice Loss: 領域の重なりを最大化\n",
    "  ・Focal Loss: 難しいサンプルに注目\n",
    "  ・Combined Loss: 複数の損失を組み合わせ\n",
    "\n",
    "【評価指標】\n",
    "  ・Dice係数: 2|A∩B| / (|A| + |B|)\n",
    "  ・IoU (Intersection over Union)\n",
    "  ・Hausdorff距離（境界の精度）\n",
    "    \"\"\")\n",
    "\nexplain_medical_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## 2. 自動運転"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_autonomous_driving():\n",
    "    \"\"\"自動運転におけるセグメンテーションの可視化\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 入力画像（模擬）\n",
    "    ax = axes[0]\n",
    "    img = np.ones((100, 150, 3)) * 0.7  # 空\n",
    "    img[60:, :, :] = [0.3, 0.3, 0.3]  # 道路\n",
    "    img[50:65, 60:90, :] = [0.8, 0.2, 0.2]  # 車\n",
    "    img[55:70, 110:125, :] = [0.2, 0.6, 0.2]  # 人\n",
    "    img[40:55, 10:25, :] = [0.4, 0.4, 0.2]  # 建物\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title('入力画像', fontsize=12)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # セグメンテーション結果\n",
    "    ax = axes[1]\n",
    "    seg = np.zeros((100, 150, 3))\n",
    "    seg[:60, :, :] = [0.5, 0.8, 1.0]  # 空\n",
    "    seg[60:, :, :] = [0.3, 0.3, 0.5]  # 道路\n",
    "    seg[50:65, 60:90, :] = [1.0, 0.2, 0.2]  # 車\n",
    "    seg[55:70, 110:125, :] = [1.0, 1.0, 0.0]  # 人\n",
    "    seg[40:55, 10:25, :] = [0.5, 0.5, 0.5]  # 建物\n",
    "    \n",
    "    ax.imshow(seg)\n",
    "    ax.set_title('セマンティックセグメンテーション', fontsize=12)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # クラス凡例\n",
    "    ax = axes[2]\n",
    "    classes = [\n",
    "        ('空', [0.5, 0.8, 1.0]),\n",
    "        ('道路', [0.3, 0.3, 0.5]),\n",
    "        ('車両', [1.0, 0.2, 0.2]),\n",
    "        ('歩行者', [1.0, 1.0, 0.0]),\n",
    "        ('建物', [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    "    \n",
    "    for i, (name, color) in enumerate(classes):\n",
    "        ax.add_patch(plt.Rectangle((0.1, 0.8 - i*0.15), 0.15, 0.1, facecolor=color))\n",
    "        ax.text(0.35, 0.85 - i*0.15, name, fontsize=12, va='center')\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('クラス凡例', fontsize=12)\n",
    "    \n",
    "    plt.suptitle('自動運転におけるセマンティックセグメンテーション', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\nvisualize_autonomous_driving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_autonomous_driving_pipeline():\n",
    "    \"\"\"自動運転のパイプライン\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"自動運転における認識パイプライン\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\"\"\n",
    "【主要タスク】\n",
    "  1. セマンティックセグメンテーション\n",
    "     → 道路、歩道、車線を識別\n",
    "  \n",
    "  2. インスタンスセグメンテーション\n",
    "     → 個々の車両・歩行者を区別\n",
    "  \n",
    "  3. パノプティックセグメンテーション\n",
    "     → セマンティック + インスタンスの統合\n",
    "  \n",
    "  4. 深度推定\n",
    "     → 単眼カメラから距離を推定\n",
    "\n",
    "【使用されるアーキテクチャ】\n",
    "  ・DeepLab系: ASPP（Atrous Spatial Pyramid Pooling）\n",
    "  ・PSPNet: Pyramid Pooling Module\n",
    "  ・EfficientPS: パノプティック用\n",
    "  ・SegFormer: Transformer + CNN\n",
    "\n",
    "【リアルタイム性の要求】\n",
    "  ・処理速度: 30fps以上\n",
    "  ・レイテンシ: 100ms以下\n",
    "  → 軽量モデル（MobileNet系バックボーン）が重要\n",
    "    \"\"\")\n",
    "\nexplain_autonomous_driving_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 3. 衛星画像解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_satellite_applications():\n",
    "    \"\"\"衛星画像解析の応用\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    applications = [\n",
    "        ('土地利用分類', '森林、農地、都市、水域などを分類\\n\\n使用手法:\\n- U-Net + ResNetバックボーン\\n- マルチスペクトル画像対応'),\n",
    "        ('建物検出', '衛星画像から建物のフットプリントを抽出\\n\\n使用手法:\\n- Mask R-CNN\\n- インスタンスセグメンテーション'),\n",
    "        ('変化検出', '2時点間の変化（森林伐採、都市化）を検出\\n\\n使用手法:\\n- Siamese Network\\n- 差分画像解析'),\n",
    "        ('災害被害評価', '洪水、火災、地震の被害範囲を特定\\n\\n使用手法:\\n- セマンティックセグメンテーション\\n- 時系列解析'),\n",
    "    ]\n",
    "    \n",
    "    colors = ['lightgreen', 'lightblue', 'lightyellow', 'lightcoral']\n",
    "    \n",
    "    for ax, (title, desc), color in zip(axes.flat, applications, colors):\n",
    "        ax.set_facecolor(color)\n",
    "        ax.text(0.5, 0.85, title, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        ax.text(0.5, 0.45, desc, ha='center', va='center', fontsize=10)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('衛星画像解析における空間的CNNの応用', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\nvisualize_satellite_applications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_satellite_challenges():\n",
    "    \"\"\"衛星画像解析の課題\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"衛星画像解析の特有の課題\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\"\"\n",
    "【データの特徴】\n",
    "  ・超高解像度（数万×数万ピクセル）\n",
    "  ・マルチスペクトル（RGB以外のバンドも利用）\n",
    "  ・時系列データ（季節変化、経年変化）\n",
    "  ・雲、影、大気の影響\n",
    "\n",
    "【技術的課題と解決策】\n",
    "\n",
    "  1. 巨大画像の処理\n",
    "     → タイル分割 + オーバーラップ\n",
    "     → 境界のスムージング\n",
    "  \n",
    "  2. スケール変化\n",
    "     → FPN（Feature Pyramid Network）\n",
    "     → マルチスケール推論\n",
    "  \n",
    "  3. クラス不均衡\n",
    "     → Focal Loss\n",
    "     → サンプリング戦略\n",
    "  \n",
    "  4. ドメインシフト\n",
    "     → 地域ごとのファインチューニング\n",
    "     → ドメイン適応\n",
    "    \"\"\")\n",
    "\nexplain_satellite_challenges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## 4. 3D再構成への応用\n",
    "\n",
    "### CNNから3DGSへの架け橋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d_reconstruction_pipeline():\n",
    "    \"\"\"3D再構成パイプラインの可視化\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(18, 8))\n",
    "    \n",
    "    # パイプラインのステップ\n",
    "    steps = [\n",
    "        (0.05, 0.5, '入力画像群', 'lightblue'),\n",
    "        (0.22, 0.5, '特徴抽出\\n(CNN)', 'lightgreen'),\n",
    "        (0.39, 0.5, '対応点検出\\n(マッチング)', 'lightyellow'),\n",
    "        (0.56, 0.5, 'カメラ推定\\n(SfM)', 'lightcoral'),\n",
    "        (0.73, 0.5, '3D表現\\n(NeRF/3DGS)', 'lavender'),\n",
    "        (0.90, 0.5, 'レンダリング', 'lightpink'),\n",
    "    ]\n",
    "    \n",
    "    for x, y, label, color in steps:\n",
    "        rect = plt.Rectangle((x, y - 0.15), 0.12, 0.3, \n",
    "                            facecolor=color, edgecolor='gray', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + 0.06, y, label, ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # 矢印\n",
    "    for i in range(len(steps) - 1):\n",
    "        ax.annotate('', xy=(steps[i+1][0], steps[i+1][1]),\n",
    "                   xytext=(steps[i][0] + 0.12, steps[i][1]),\n",
    "                   arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    # CNNの役割を強調\n",
    "    ax.annotate('', xy=(0.28, 0.25), xytext=(0.28, 0.35),\n",
    "               arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "    ax.text(0.28, 0.15, 'CNNの帰納バイアス\\n（局所性・重み共有）\\nが特徴抽出を効率化', \n",
    "           ha='center', fontsize=10, color='red')\n",
    "    \n",
    "    ax.set_xlim(0, 1.05)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('3D再構成パイプラインとCNNの役割', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\nvisualize_3d_reconstruction_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_cnn_in_3d_reconstruction():\n",
    "    \"\"\"3D再構成におけるCNNの役割\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"3D再構成におけるCNNの役割\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\"\"\n",
    "【特徴抽出での活用】\n",
    "  ・SuperPoint: キーポイント検出（CNN）\n",
    "  ・SuperGlue: 特徴マッチング（GNN + Attention）\n",
    "  ・LoFTR: Detector-freeマッチング（Transformer + CNN）\n",
    "\n",
    "【深度推定での活用】\n",
    "  ・MiDaS: 単眼深度推定\n",
    "  ・RAFT-Stereo: ステレオマッチング\n",
    "  → CNN + コスト集約で高精度な深度マップ\n",
    "\n",
    "【NeRF/3DGSでの活用】\n",
    "  ・CNN特徴を3D表現の初期化に利用\n",
    "  ・画像エンコーダとしてのCNN（PixelNeRF等）\n",
    "  ・3DGSの初期点群生成にSfM（CNN特徴ベース）\n",
    "\n",
    "【NeoVerseプロジェクトへの接続】\n",
    "  ・このノートブックシリーズで学んだCNNの知識が\n",
    "    3DGS/NeRFの理解の基盤となる\n",
    "  ・受容野の概念 → 3DGSのスプラットサイズの理解\n",
    "  ・帰納バイアス → なぜ3D表現が効率的か\n",
    "    \"\"\")\n",
    "\nexplain_cnn_in_3d_reconstruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_2d_to_3d():\n",
    "    \"\"\"2D CNNと3D表現の比較\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"2D CNN vs 3D表現（NeRF/3DGS）\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    comparison = [\n",
    "        ('', '2D CNN', '3DGS/NeRF'),\n",
    "        ('-'*15, '-'*25, '-'*25),\n",
    "        ('入力', '画像（2D）', '画像群（複数視点）'),\n",
    "        ('出力', '特徴マップ/セグメンテーション', '3Dシーン表現'),\n",
    "        ('表現', '畳み込みカーネル', 'ガウシアン/ニューラル場'),\n",
    "        ('帰納バイアス', '局所性・重み共有', 'マルチビュー一貫性'),\n",
    "        ('受容野', '固定（設計時に決定）', '適応的（学習で最適化）'),\n",
    "        ('計算コスト', '比較的低い', '高い（レンダリング必要）'),\n",
    "    ]\n",
    "    \n",
    "    for row in comparison:\n",
    "        print(f\"{row[0]:<15} {row[1]:<25} {row[2]:<25}\")\n",
    "\ncompare_2d_to_3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 5. まとめ\n",
    "\n",
    "### 空間的CNNの応用分野\n",
    "\n",
    "| 分野 | 主要タスク | キーアーキテクチャ |\n",
    "|------|----------|------------------|\n",
    "| 医療画像 | セグメンテーション | U-Net, 3D U-Net |\n",
    "| 自動運転 | 認識・セグメンテーション | DeepLab, PSPNet |\n",
    "| 衛星画像 | 土地利用分類 | U-Net + FPN |\n",
    "| 3D再構成 | 特徴抽出・深度推定 | SuperPoint, MiDaS |\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "次のノートブックでは、これまで学んだ内容を総合的に振り返り、空間的帰納バイアスの本質を深く理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_summary():\n",
    "    \"\"\"最終まとめ\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"空間的CNNの応用 - まとめ\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "【学んだこと】\n",
    "\n",
    "1. 医療画像解析\n",
    "   ・U-Netが医療分野で革命を起こした\n",
    "   ・少ないデータでも高性能を達成\n",
    "   ・Dice Loss等の専用損失関数が重要\n",
    "\n",
    "2. 自動運転\n",
    "   ・セマンティック/インスタンス/パノプティックセグメンテーション\n",
    "   ・リアルタイム性が重要な制約\n",
    "   ・軽量モデルの必要性\n",
    "\n",
    "3. 衛星画像解析\n",
    "   ・超高解像度画像の処理\n",
    "   ・マルチスペクトル・時系列データ\n",
    "   ・タイル分割とFPNの活用\n",
    "\n",
    "4. 3D再構成\n",
    "   ・CNNは特徴抽出・深度推定の基盤\n",
    "   ・NeRF/3DGSへの橋渡し\n",
    "   ・NeoVerseプロジェクトとの接続\n",
    "\n",
    "【重要な洞察】\n",
    "  CNNの空間的帰納バイアスは、\n",
    "  画像処理から3D理解まで幅広く応用される\n",
    "  普遍的な設計原理である。\n",
    "    \"\"\")\n",
    "\nfinal_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
