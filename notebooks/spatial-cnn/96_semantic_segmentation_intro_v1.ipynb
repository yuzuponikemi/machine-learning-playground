{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 96. セマンティックセグメンテーション入門\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックでは、以下を学びます：\n",
    "\n",
    "1. **セマンティックセグメンテーション**とは何か\n",
    "2. **画像分類との違い**\n",
    "3. **密なピクセル予測**の課題\n",
    "4. **基本的なアプローチ**\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. [セマンティックセグメンテーションとは](#section1)\n",
    "2. [タスクの比較](#section2)\n",
    "3. [技術的課題](#section3)\n",
    "4. [基本アプローチ](#section4)\n",
    "5. [まとめ](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import japanize_matplotlib\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1. セマンティックセグメンテーションとは\n",
    "\n",
    "### 定義\n",
    "\n",
    "**セマンティックセグメンテーション**とは：\n",
    "\n",
    "> 画像の各ピクセルにクラスラベルを割り当てるタスク\n",
    "\n",
    "「この画像には猫がいる」ではなく、「このピクセルは猫、このピクセルは背景...」と判断します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation_concept():\n",
    "    \"\"\"セグメンテーションの概念を可視化\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # サンプル画像（簡略化）\n",
    "    np.random.seed(42)\n",
    "    img = np.zeros((64, 64, 3))\n",
    "    \n",
    "    # 背景（空）\n",
    "    img[:30, :, :] = [0.5, 0.7, 1.0]  # 青空\n",
    "    \n",
    "    # 背景（地面）\n",
    "    img[30:, :, :] = [0.3, 0.6, 0.3]  # 緑の草\n",
    "    \n",
    "    # 物体（猫のシルエット）\n",
    "    img[20:50, 20:45, :] = [0.6, 0.4, 0.2]  # 茶色の猫\n",
    "    \n",
    "    # 入力画像\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('入力画像', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # セグメンテーションマスク\n",
    "    mask = np.zeros((64, 64))\n",
    "    mask[:30, :] = 0  # 空\n",
    "    mask[30:, :] = 1  # 草\n",
    "    mask[20:50, 20:45] = 2  # 猫\n",
    "    \n",
    "    cmap = plt.cm.colors.ListedColormap(['skyblue', 'lightgreen', 'orange'])\n",
    "    axes[1].imshow(mask, cmap=cmap)\n",
    "    axes[1].set_title('セグメンテーションマスク', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # 凡例\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='skyblue', label='空 (0)'),\n",
    "        Patch(facecolor='lightgreen', label='草 (1)'),\n",
    "        Patch(facecolor='orange', label='猫 (2)'),\n",
    "    ]\n",
    "    axes[1].legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # オーバーレイ\n",
    "    axes[2].imshow(img)\n",
    "    axes[2].imshow(mask, cmap=cmap, alpha=0.5)\n",
    "    axes[2].set_title('オーバーレイ表示', fontsize=14)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle('セマンティックセグメンテーション：各ピクセルにクラスを割り当て', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_segmentation_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## 2. タスクの比較\n",
    "\n",
    "コンピュータビジョンの主要タスクを比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cv_tasks():\n",
    "    \"\"\"CVタスクの比較\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # サンプル画像\n",
    "    np.random.seed(42)\n",
    "    img = np.zeros((64, 64, 3))\n",
    "    img[:, :, :] = [0.8, 0.9, 1.0]  # 背景\n",
    "    \n",
    "    # 2つの物体\n",
    "    img[10:30, 10:30, :] = [1.0, 0.5, 0.5]  # 赤い四角\n",
    "    img[35:55, 35:55, :] = [0.5, 0.5, 1.0]  # 青い四角\n",
    "    \n",
    "    tasks = [\n",
    "        ('画像分類', 'この画像には何がある？\\n→ 「四角」'),\n",
    "        ('物体検出', '物体はどこにある？\\n→ バウンディングボックス'),\n",
    "        ('セマンティックセグメンテーション', '各ピクセルは何？\\n→ ピクセル単位のラベル'),\n",
    "        ('インスタンスセグメンテーション', '各物体はどれ？\\n→ 物体ごとに区別'),\n",
    "        ('パノプティックセグメンテーション', '全てを統合\\n→ stuff + things'),\n",
    "    ]\n",
    "    \n",
    "    # 1. 画像分類\n",
    "    axes[0, 0].imshow(img)\n",
    "    axes[0, 0].set_title('画像分類\\n出力: クラスラベル', fontsize=12)\n",
    "    axes[0, 0].text(32, 70, 'クラス: \"四角\"', ha='center', fontsize=11)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # 2. 物体検出\n",
    "    axes[0, 1].imshow(img)\n",
    "    rect1 = Rectangle((10, 10), 20, 20, linewidth=2, edgecolor='red', facecolor='none')\n",
    "    rect2 = Rectangle((35, 35), 20, 20, linewidth=2, edgecolor='blue', facecolor='none')\n",
    "    axes[0, 1].add_patch(rect1)\n",
    "    axes[0, 1].add_patch(rect2)\n",
    "    axes[0, 1].set_title('物体検出\\n出力: BBox + クラス', fontsize=12)\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # 3. セマンティックセグメンテーション\n",
    "    mask = np.zeros((64, 64))\n",
    "    mask[10:30, 10:30] = 1\n",
    "    mask[35:55, 35:55] = 1  # 同じクラス\n",
    "    \n",
    "    axes[0, 2].imshow(img)\n",
    "    axes[0, 2].imshow(mask, alpha=0.5, cmap='Reds')\n",
    "    axes[0, 2].set_title('セマンティックセグメンテーション\\n出力: ピクセルごとのクラス', fontsize=12)\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # 4. インスタンスセグメンテーション\n",
    "    mask_instance = np.zeros((64, 64))\n",
    "    mask_instance[10:30, 10:30] = 1\n",
    "    mask_instance[35:55, 35:55] = 2  # 別のインスタンス\n",
    "    \n",
    "    axes[1, 0].imshow(img)\n",
    "    axes[1, 0].imshow(mask_instance, alpha=0.5, cmap='tab10')\n",
    "    axes[1, 0].set_title('インスタンスセグメンテーション\\n出力: 物体ごとに区別', fontsize=12)\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # 5. 比較表\n",
    "    axes[1, 1].axis('off')\n",
    "    table_data = [\n",
    "        ['タスク', '出力形式', '粒度'],\n",
    "        ['分類', 'クラス', '画像単位'],\n",
    "        ['検出', 'BBox+クラス', '物体単位'],\n",
    "        ['セマンティック', 'マスク', 'ピクセル単位'],\n",
    "        ['インスタンス', 'マスク+ID', 'ピクセル単位'],\n",
    "    ]\n",
    "    table = axes[1, 1].table(cellText=table_data, loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1.5, 2)\n",
    "    axes[1, 1].set_title('タスク比較', fontsize=12)\n",
    "    \n",
    "    # 空のプロット\n",
    "    axes[1, 2].axis('off')\n",
    "    axes[1, 2].text(0.5, 0.5, 'セマンティックセグメンテーションは\\n\\n・ピクセル単位の分類\\n・位置情報を完全に保持\\n・Dense Prediction', \n",
    "                   fontsize=12, ha='center', va='center', transform=axes[1, 2].transAxes,\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "    \n",
    "    plt.suptitle('コンピュータビジョンタスクの比較', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_cv_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 3. 技術的課題\n",
    "\n",
    "セマンティックセグメンテーションには、画像分類にはない課題があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_challenges():\n",
    "    \"\"\"セグメンテーションの課題を説明\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"セマンティックセグメンテーションの技術的課題\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    challenges = [\n",
    "        (\"1. 解像度の維持\",\n",
    "         \"画像分類では最終的にGlobal Poolingで空間情報を捨てる\",\n",
    "         \"セグメンテーションでは入力と同じ解像度の出力が必要\"),\n",
    "        \n",
    "        (\"2. 局所性と大域性のバランス\",\n",
    "         \"正確な境界には局所的な情報が必要\",\n",
    "         \"クラス判定には大域的なコンテキストが必要\"),\n",
    "        \n",
    "        (\"3. 計算コスト\",\n",
    "         \"高解像度を維持すると計算量が爆発\",\n",
    "         \"効率的なアーキテクチャ設計が必要\"),\n",
    "        \n",
    "        (\"4. クラス不均衡\",\n",
    "         \"背景ピクセルが圧倒的に多い\",\n",
    "         \"適切な損失関数の設計が必要\"),\n",
    "    ]\n",
    "    \n",
    "    for name, problem, requirement in challenges:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"  問題: {problem}\")\n",
    "        print(f\"  要件: {requirement}\")\n",
    "\n",
    "explain_challenges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_resolution_problem():\n",
    "    \"\"\"解像度維持の問題を可視化\"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "    \n",
    "    # 元画像\n",
    "    img = np.random.rand(64, 64)\n",
    "    axes[0].imshow(img, cmap='gray')\n",
    "    axes[0].set_title('入力\\n64×64', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # CNNによるダウンサンプリング\n",
    "    from scipy.ndimage import zoom\n",
    "    \n",
    "    img_down1 = zoom(img, 0.5)  # 32x32\n",
    "    axes[1].imshow(img_down1, cmap='gray')\n",
    "    axes[1].set_title('Conv+Pool ×2\\n32×32', fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    img_down2 = zoom(img, 0.25)  # 16x16\n",
    "    axes[2].imshow(img_down2, cmap='gray')\n",
    "    axes[2].set_title('Conv+Pool ×4\\n16×16', fontsize=12)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # 問題：これを64x64に戻す必要がある\n",
    "    img_up = zoom(img_down2, 4)  # 64x64に戻す\n",
    "    axes[3].imshow(img_up, cmap='gray')\n",
    "    axes[3].set_title('アップサンプル\\n64×64（情報損失）', fontsize=12)\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.suptitle('解像度の問題：ダウンサンプリングで詳細が失われる', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_resolution_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## 4. 基本アプローチ\n",
    "\n",
    "セグメンテーションの基本的なアプローチを紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fcn_concept():\n",
    "    \"\"\"FCN（Fully Convolutional Network）の概念\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    \n",
    "    # エンコーダ\n",
    "    encoder_layers = [(0.05, 0.5, 0.08, 0.4, '入力\\n64×64'),\n",
    "                      (0.18, 0.55, 0.06, 0.3, '32×32'),\n",
    "                      (0.28, 0.6, 0.05, 0.2, '16×16'),\n",
    "                      (0.38, 0.65, 0.04, 0.1, '8×8')]\n",
    "    \n",
    "    for x, y, w, h, label in encoder_layers:\n",
    "        rect = Rectangle((x, y - h/2), w, h, facecolor='lightblue', edgecolor='blue')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + w/2, y - h/2 - 0.05, label, ha='center', fontsize=9)\n",
    "    \n",
    "    ax.text(0.22, 0.85, 'エンコーダ\\n（特徴抽出）', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # デコーダ\n",
    "    decoder_layers = [(0.55, 0.65, 0.04, 0.1, '8×8'),\n",
    "                      (0.65, 0.6, 0.05, 0.2, '16×16'),\n",
    "                      (0.78, 0.55, 0.06, 0.3, '32×32'),\n",
    "                      (0.9, 0.5, 0.08, 0.4, '出力\\n64×64')]\n",
    "    \n",
    "    for x, y, w, h, label in decoder_layers:\n",
    "        rect = Rectangle((x, y - h/2), w, h, facecolor='lightgreen', edgecolor='green')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + w/2, y - h/2 - 0.05, label, ha='center', fontsize=9)\n",
    "    \n",
    "    ax.text(0.75, 0.85, 'デコーダ\\n（アップサンプル）', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 矢印\n",
    "    for i in range(3):\n",
    "        ax.annotate('', xy=(encoder_layers[i+1][0], encoder_layers[i+1][1]),\n",
    "                   xytext=(encoder_layers[i][0] + encoder_layers[i][2], encoder_layers[i][1]),\n",
    "                   arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "    \n",
    "    ax.annotate('', xy=(decoder_layers[0][0], decoder_layers[0][1]),\n",
    "               xytext=(encoder_layers[-1][0] + encoder_layers[-1][2], encoder_layers[-1][1]),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "    \n",
    "    for i in range(3):\n",
    "        ax.annotate('', xy=(decoder_layers[i+1][0], decoder_layers[i+1][1]),\n",
    "                   xytext=(decoder_layers[i][0] + decoder_layers[i][2], decoder_layers[i][1]),\n",
    "                   arrowprops=dict(arrowstyle='->', color='green'))\n",
    "    \n",
    "    # 説明\n",
    "    ax.text(0.5, 0.15, 'FCN: 全結合層を畳み込みに置き換え\\n→ 任意の入力サイズに対応可能', \n",
    "           ha='center', fontsize=12, transform=ax.transAxes,\n",
    "           bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Encoder-Decoder アーキテクチャ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_fcn_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 5. まとめ\n",
    "\n",
    "### 学んだこと\n",
    "\n",
    "1. **セマンティックセグメンテーション**: 各ピクセルにクラスを割り当てる\n",
    "2. **課題**: 解像度維持、局所性と大域性のバランス\n",
    "3. **基本アプローチ**: Encoder-Decoder構造\n",
    "\n",
    "### 次のノートブック\n",
    "\n",
    "次のノートブックでは、**U-Net**アーキテクチャについて詳しく学びます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
