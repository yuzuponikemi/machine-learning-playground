{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06: SVM and Kernel Methods\n",
    "\n",
    "Understand Support Vector Machines and kernel tricks.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand SVM fundamentals and margins\n",
    "- Explore different kernels (linear, RBF, polynomial)\n",
    "- Tune C and gamma parameters\n",
    "- Visualize support vectors and decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles, make_blobs\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear SVM Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linearly separable data\n",
    "X, y = make_blobs(n_samples=100, centers=2, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Linear SVM\n",
    "svm_linear = SVC(kernel='linear', C=1.0)\n",
    "svm_linear.fit(X_scaled, y)\n",
    "\n",
    "print(f\"Number of support vectors: {len(svm_linear.support_vectors_)}\")\n",
    "print(f\"Support vectors per class: {svm_linear.n_support_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SVM with margins\n",
    "def plot_svm_decision_boundary(model, X, y, ax, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Decision boundary and margins\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Plot decision function for margins\n",
    "    if hasattr(model, 'decision_function'):\n",
    "        Z_decision = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z_decision = Z_decision.reshape(xx.shape)\n",
    "        ax.contour(xx, yy, Z_decision, colors='k', levels=[-1, 0, 1], \n",
    "                   alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # Plot data points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black', s=50)\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "               s=200, facecolors='none', edgecolors='green', linewidths=2)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.gca()\n",
    "plot_svm_decision_boundary(svm_linear, X_scaled, y, ax, \n",
    "                          f'Linear SVM\\nSupport Vectors: {len(svm_linear.support_vectors_)}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Effect of C Parameter (Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with some overlap\n",
    "X, y = make_blobs(n_samples=100, centers=2, cluster_std=1.5, random_state=42)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Compare different C values\n",
    "C_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, C in enumerate(C_values):\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_scaled, y)\n",
    "    \n",
    "    plot_svm_decision_boundary(svm, X_scaled, y, axes[idx], \n",
    "                              f'C = {C}\\nSV: {len(svm.support_vectors_)}')\n",
    "\n",
    "axes[-1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nC parameter interpretation:\")\n",
    "print(\"- Small C: Wider margin, more misclassifications allowed (soft margin)\")\n",
    "print(\"- Large C: Narrow margin, fewer misclassifications (hard margin)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Non-linear Data and Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linearly separable data\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "# Scale\n",
    "X_moons_scaled = scaler.fit_transform(X_moons)\n",
    "X_circles_scaled = scaler.fit_transform(X_circles)\n",
    "\n",
    "# Visualize datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='RdYlBu', edgecolors='black')\n",
    "axes[0].set_title('Moons Dataset')\n",
    "\n",
    "axes[1].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='RdYlBu', edgecolors='black')\n",
    "axes[1].set_title('Circles Dataset')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare kernels on moons data\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, kernel in enumerate(kernels):\n",
    "    svm = SVC(kernel=kernel, C=1.0, gamma='scale')\n",
    "    svm.fit(X_moons_scaled, y_moons)\n",
    "    \n",
    "    acc = svm.score(X_moons_scaled, y_moons)\n",
    "    \n",
    "    # Simple decision boundary plot\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_moons_scaled[:, 0].min() - 0.5, X_moons_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_moons_scaled[:, 1].min() - 0.5, X_moons_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    axes[idx].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=y_moons, \n",
    "                      cmap=plt.cm.RdYlBu, edgecolors='black', s=30)\n",
    "    axes[idx].set_title(f'{kernel.upper()} Kernel\\nAccuracy: {acc:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: RBF Kernel - Gamma Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of gamma on RBF kernel\n",
    "gamma_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, gamma in enumerate(gamma_values):\n",
    "    svm = SVC(kernel='rbf', C=1.0, gamma=gamma)\n",
    "    svm.fit(X_moons_scaled, y_moons)\n",
    "    \n",
    "    acc = svm.score(X_moons_scaled, y_moons)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_moons_scaled[:, 0].min() - 0.5, X_moons_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_moons_scaled[:, 1].min() - 0.5, X_moons_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    axes[idx].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=y_moons, \n",
    "                      cmap=plt.cm.RdYlBu, edgecolors='black', s=30)\n",
    "    axes[idx].set_title(f'γ = {gamma}\\nAcc: {acc:.3f}, SV: {len(svm.support_vectors_)}')\n",
    "\n",
    "axes[-1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGamma parameter interpretation:\")\n",
    "print(\"- Small gamma: Smooth decision boundary, may underfit\")\n",
    "print(\"- Large gamma: Complex decision boundary, may overfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: C and Gamma Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_moons_scaled, y_moons, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Parameter grid\n",
    "C_range = np.logspace(-2, 3, 10)\n",
    "gamma_range = np.logspace(-3, 2, 10)\n",
    "\n",
    "# Grid search results\n",
    "results = np.zeros((len(C_range), len(gamma_range)))\n",
    "\n",
    "for i, C in enumerate(C_range):\n",
    "    for j, gamma in enumerate(gamma_range):\n",
    "        svm = SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "        svm.fit(X_train, y_train)\n",
    "        results[i, j] = svm.score(X_test, y_test)\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(results, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            xticklabels=[f'{g:.2e}' for g in gamma_range],\n",
    "            yticklabels=[f'{c:.2e}' for c in C_range])\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('C')\n",
    "plt.title('SVM RBF: Test Accuracy for C and Gamma')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best parameters\n",
    "best_idx = np.unravel_index(np.argmax(results), results.shape)\n",
    "print(f\"\\nBest C: {C_range[best_idx[0]]:.4f}\")\n",
    "print(f\"Best gamma: {gamma_range[best_idx[1]]:.4f}\")\n",
    "print(f\"Best accuracy: {results[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV for proper cross-validation\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.1, 1, 10],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"GridSearchCV Results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test score: {grid_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare polynomial degrees\n",
    "degrees = [2, 3, 4, 5]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    svm = SVC(kernel='poly', degree=degree, C=1.0, coef0=1)\n",
    "    svm.fit(X_moons_scaled, y_moons)\n",
    "    \n",
    "    acc = svm.score(X_moons_scaled, y_moons)\n",
    "    \n",
    "    # Plot\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_moons_scaled[:, 0].min() - 0.5, X_moons_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_moons_scaled[:, 1].min() - 0.5, X_moons_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    axes[idx].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=y_moons, \n",
    "                      cmap=plt.cm.RdYlBu, edgecolors='black', s=30)\n",
    "    axes[idx].set_title(f'Polynomial Degree {degree}\\nAccuracy: {acc:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multi-class data\n",
    "X_multi, y_multi = make_classification(\n",
    "    n_samples=500, n_features=20, n_informative=10,\n",
    "    n_classes=4, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM\n",
    "svm_multi = SVC(kernel='rbf', C=1.0, gamma='scale', decision_function_shape='ovr')\n",
    "svm_multi.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = svm_multi.predict(X_test_scaled)\n",
    "\n",
    "print(\"Multi-class SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: SVM for Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression data\n",
    "n_samples = 200\n",
    "X_reg = np.sort(5 * np.random.rand(n_samples, 1), axis=0)\n",
    "y_reg = np.sin(X_reg).ravel() + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Train different SVR models\n",
    "svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "svr_linear = SVR(kernel='linear', C=100)\n",
    "svr_poly = SVR(kernel='poly', C=100, degree=3)\n",
    "\n",
    "# Fit models\n",
    "svr_rbf.fit(X_reg, y_reg)\n",
    "svr_linear.fit(X_reg, y_reg)\n",
    "svr_poly.fit(X_reg, y_reg)\n",
    "\n",
    "# Predict\n",
    "X_plot = np.linspace(0, 5, 1000).reshape(-1, 1)\n",
    "y_rbf = svr_rbf.predict(X_plot)\n",
    "y_linear = svr_linear.predict(X_plot)\n",
    "y_poly = svr_poly.predict(X_plot)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_reg, y_reg, c='gray', alpha=0.5, label='Data')\n",
    "plt.plot(X_plot, y_rbf, 'r-', lw=2, label='RBF')\n",
    "plt.plot(X_plot, y_linear, 'g-', lw=2, label='Linear')\n",
    "plt.plot(X_plot, y_poly, 'b-', lw=2, label='Polynomial')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('SVR with Different Kernels')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of epsilon (tube width)\n",
    "epsilons = [0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, eps in enumerate(epsilons):\n",
    "    svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=eps)\n",
    "    svr.fit(X_reg, y_reg)\n",
    "    \n",
    "    y_pred = svr.predict(X_plot)\n",
    "    \n",
    "    axes[idx].scatter(X_reg, y_reg, c='gray', alpha=0.5)\n",
    "    axes[idx].plot(X_plot, y_pred, 'r-', lw=2)\n",
    "    \n",
    "    # Show epsilon tube\n",
    "    axes[idx].fill_between(X_plot.ravel(), y_pred - eps, y_pred + eps, alpha=0.2, color='red')\n",
    "    \n",
    "    axes[idx].set_title(f'ε = {eps}, SV: {len(svr.support_)}')\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Practical Example with Real-ish Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complex classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15,\n",
    "    n_redundant=3, n_classes=3, n_clusters_per_class=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare different SVM configurations\n",
    "configs = [\n",
    "    ('Linear', SVC(kernel='linear', C=1.0)),\n",
    "    ('RBF (C=1)', SVC(kernel='rbf', C=1.0, gamma='scale')),\n",
    "    ('RBF (C=10)', SVC(kernel='rbf', C=10.0, gamma='scale')),\n",
    "    ('Poly (d=3)', SVC(kernel='poly', degree=3, C=1.0))\n",
    "]\n",
    "\n",
    "print(\"SVM Configuration Comparison:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, model in configs:\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train_scaled, y_train)\n",
    "    test_acc = model.score(X_test_scaled, y_test)\n",
    "    n_sv = len(model.support_vectors_)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Support Vectors: {n_sv} ({n_sv/len(X_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "### SVM Fundamentals\n",
    "- Maximum margin classifier\n",
    "- Support vectors determine decision boundary\n",
    "- Works well with high-dimensional data\n",
    "\n",
    "### Key Parameters\n",
    "- **C**: Regularization (soft vs hard margin)\n",
    "  - Small C → wider margin, more errors allowed\n",
    "  - Large C → narrow margin, fewer errors\n",
    "- **gamma**: RBF kernel parameter\n",
    "  - Small γ → smooth boundary\n",
    "  - Large γ → complex boundary\n",
    "- **degree**: Polynomial kernel degree\n",
    "\n",
    "### Kernels\n",
    "- **Linear**: For linearly separable data\n",
    "- **RBF**: Most versatile, good default\n",
    "- **Polynomial**: Captures interactions\n",
    "\n",
    "### Key Takeaways\n",
    "- Always scale features before using SVM\n",
    "- RBF kernel is usually a good starting point\n",
    "- Grid search C and gamma together\n",
    "- Number of support vectors indicates complexity\n",
    "\n",
    "### Next Steps\n",
    "Continue to **Notebook 07** for MLP fundamentals - the start of neural networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
