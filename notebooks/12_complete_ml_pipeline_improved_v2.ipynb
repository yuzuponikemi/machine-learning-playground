{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. 完全な機械学習パイプライン (Complete ML Pipeline)\n",
    "\n",
    "## 概要\n",
    "このノートブックでは、データ読み込みから本番環境へのデプロイまでの完全な機械学習パイプラインを構築します。\n",
    "\n",
    "## 学習目標\n",
    "- Pipelineを使った前処理とモデルの統合\n",
    "- パイプライン内でのハイパーパラメータチューニング\n",
    "- joblibを使ったモデルの永続化\n",
    "- 本番環境向けの予測関数の実装\n",
    "- 完全なMLワークフローのベストプラクティス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 日本語フォント設定（必要に応じて）\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 乱数シード設定\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipelineの基礎\n",
    "\n",
    "### Pipelineとは\n",
    "Pipelineは、データの前処理とモデル学習を一つのワークフローにまとめる機能です。\n",
    "\n",
    "### Pipelineの利点\n",
    "1. **コードの簡潔性**: 複数のステップを一つのオブジェクトで管理\n",
    "2. **データリーク防止**: cross-validationで前処理が正しく適用される\n",
    "3. **再現性の向上**: 前処理パラメータが学習データから計算され保存される\n",
    "4. **デプロイの容易性**: 一つのオブジェクトとして保存・読み込み可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み（分類タスク）\n",
    "cancer_data = load_breast_cancer()\n",
    "X_cancer = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
    "y_cancer = cancer_data.target\n",
    "\n",
    "# データ分割\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "print(f\"訓練データサイズ: {X_train_c.shape}\")\n",
    "print(f\"テストデータサイズ: {X_test_c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的なPipelineの構築\n",
    "pipeline_basic = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42))\n",
    "])\n",
    "\n",
    "# Pipelineの学習\n",
    "pipeline_basic.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Pipelineで予測\n",
    "y_pred_basic = pipeline_basic.predict(X_test_c)\n",
    "accuracy_basic = pipeline_basic.score(X_test_c, y_test_c)\n",
    "\n",
    "print(f\"基本的なPipelineの精度: {accuracy_basic:.4f}\")\n",
    "print(\"\\nPipelineのステップ:\")\n",
    "print(pipeline_basic.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. パイプライン内でのハイパーパラメータチューニング\n",
    "\n",
    "### パイプラインパラメータの指定方法\n",
    "パイプライン内の各ステップのパラメータは `<ステップ名>__<パラメータ名>` の形式で指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCVでパイプライン全体を最適化\n",
    "pipeline_tuned = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', MLPClassifier(max_iter=500, random_state=42))\n",
    "])\n",
    "\n",
    "# パラメータグリッド（前処理とモデルの両方）\n",
    "param_grid = {\n",
    "    # Scalerの選択（StandardScaler or MinMaxScaler）\n",
    "    'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "    # MLPのパラメータ\n",
    "    'classifier__hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "    'classifier__activation': ['relu', 'tanh'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# GridSearchCV実行\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline_tuned,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"GridSearchCV実行中...\")\n",
    "grid_search.fit(X_train_c, y_train_c)\n",
    "\n",
    "print(f\"\\n最良スコア: {grid_search.best_score_:.4f}\")\n",
    "print(f\"最良パラメータ:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでの評価\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "y_pred_tuned = best_pipeline.predict(X_test_c)\n",
    "accuracy_tuned = best_pipeline.score(X_test_c, y_test_c)\n",
    "\n",
    "print(f\"チューニング後のテスト精度: {accuracy_tuned:.4f}\")\n",
    "print(\"\\n詳細な分類レポート:\")\n",
    "print(classification_report(y_test_c, y_pred_tuned, target_names=cancer_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列の可視化\n",
    "cm = confusion_matrix(y_test_c, y_pred_tuned)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=cancer_data.target_names,\n",
    "            yticklabels=cancer_data.target_names)\n",
    "plt.title('Confusion Matrix (Tuned Pipeline)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n正解数: {cm.diagonal().sum()} / {cm.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 複雑なパイプライン: 特徴量エンジニアリング\n",
    "\n",
    "### 多項式特徴量の追加\n",
    "パイプラインに特徴量生成ステップを追加できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回帰タスクのデータ準備\n",
    "housing_data = fetch_california_housing()\n",
    "X_housing = pd.DataFrame(housing_data.data[:1000], columns=housing_data.feature_names)  # サンプル数削減\n",
    "y_housing = housing_data.target[:1000]\n",
    "\n",
    "# データ分割\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"回帰データサイズ: {X_train_h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多項式特徴量を含むパイプライン\n",
    "pipeline_poly = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42))\n",
    "])\n",
    "\n",
    "# 通常のパイプライン（比較用）\n",
    "pipeline_normal = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42))\n",
    "])\n",
    "\n",
    "# 両方のパイプラインを学習\n",
    "print(\"通常のパイプライン学習中...\")\n",
    "pipeline_normal.fit(X_train_h, y_train_h)\n",
    "print(\"多項式特徴量パイプライン学習中...\")\n",
    "pipeline_poly.fit(X_train_h, y_train_h)\n",
    "\n",
    "# 予測と評価\n",
    "y_pred_normal = pipeline_normal.predict(X_test_h)\n",
    "y_pred_poly = pipeline_poly.predict(X_test_h)\n",
    "\n",
    "r2_normal = r2_score(y_test_h, y_pred_normal)\n",
    "r2_poly = r2_score(y_test_h, y_pred_poly)\n",
    "rmse_normal = np.sqrt(mean_squared_error(y_test_h, y_pred_normal))\n",
    "rmse_poly = np.sqrt(mean_squared_error(y_test_h, y_pred_poly))\n",
    "\n",
    "print(f\"\\n通常のパイプライン - R²: {r2_normal:.4f}, RMSE: {rmse_normal:.4f}\")\n",
    "print(f\"多項式特徴量パイプライン - R²: {r2_poly:.4f}, RMSE: {rmse_poly:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果の可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 通常のパイプライン\n",
    "axes[0].scatter(y_test_h, y_pred_normal, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "axes[0].plot([y_test_h.min(), y_test_h.max()], [y_test_h.min(), y_test_h.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('True Values')\n",
    "axes[0].set_ylabel('Predictions')\n",
    "axes[0].set_title(f'Normal Pipeline (R²={r2_normal:.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 多項式特徴量パイプライン\n",
    "axes[1].scatter(y_test_h, y_pred_poly, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "axes[1].plot([y_test_h.min(), y_test_h.max()], [y_test_h.min(), y_test_h.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('True Values')\n",
    "axes[1].set_ylabel('Predictions')\n",
    "axes[1].set_title(f'Polynomial Features Pipeline (R²={r2_poly:.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. モデルの永続化（保存と読み込み）\n",
    "\n",
    "### joblibを使ったモデル保存\n",
    "学習済みモデルを保存することで、後で再利用したり本番環境にデプロイしたりできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル保存用ディレクトリの作成\n",
    "model_dir = '../models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# タイムスタンプ付きのモデル名\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = os.path.join(model_dir, f'breast_cancer_classifier_{timestamp}.pkl')\n",
    "\n",
    "# モデル保存\n",
    "joblib.dump(best_pipeline, model_filename)\n",
    "print(f\"モデルを保存しました: {model_filename}\")\n",
    "\n",
    "# ファイルサイズの確認\n",
    "file_size = os.path.getsize(model_filename) / 1024  # KB\n",
    "print(f\"ファイルサイズ: {file_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "loaded_pipeline = joblib.load(model_filename)\n",
    "print(\"モデルを読み込みました\")\n",
    "\n",
    "# 読み込んだモデルで予測\n",
    "y_pred_loaded = loaded_pipeline.predict(X_test_c)\n",
    "accuracy_loaded = loaded_pipeline.score(X_test_c, y_test_c)\n",
    "\n",
    "print(f\"\\n読み込んだモデルの精度: {accuracy_loaded:.4f}\")\n",
    "print(f\"元のモデルの精度: {accuracy_tuned:.4f}\")\n",
    "print(f\"予測結果が一致: {np.array_equal(y_pred_loaded, y_pred_tuned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 本番環境向けの予測関数\n",
    "\n",
    "### 堅牢な予測関数の実装\n",
    "本番環境では、エラーハンドリングやバリデーションが重要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPredictor:\n",
    "    \"\"\"本番環境向けのモデル予測クラス\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, feature_names=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_path : str\n",
    "            学習済みモデルのパス\n",
    "        feature_names : list, optional\n",
    "            期待される特徴量名のリスト\n",
    "        \"\"\"\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.feature_names = feature_names\n",
    "        print(f\"モデルを読み込みました: {model_path}\")\n",
    "    \n",
    "    def validate_input(self, X):\n",
    "        \"\"\"入力データのバリデーション\"\"\"\n",
    "        # DataFrameの場合\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            if self.feature_names is not None:\n",
    "                # 特徴量名の確認\n",
    "                if not all(col in X.columns for col in self.feature_names):\n",
    "                    missing = set(self.feature_names) - set(X.columns)\n",
    "                    raise ValueError(f\"不足している特徴量: {missing}\")\n",
    "                # 特徴量の順序を揃える\n",
    "                X = X[self.feature_names]\n",
    "        \n",
    "        # NumPy配列の場合\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            if self.feature_names is not None:\n",
    "                if X.shape[1] != len(self.feature_names):\n",
    "                    raise ValueError(\n",
    "                        f\"特徴量数が一致しません。期待: {len(self.feature_names)}, 実際: {X.shape[1]}\"\n",
    "                    )\n",
    "        else:\n",
    "            raise TypeError(\"入力はpandas DataFrameまたはNumPy配列である必要があります\")\n",
    "        \n",
    "        # 欠損値チェック\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            if X.isnull().any().any():\n",
    "                raise ValueError(\"入力データに欠損値が含まれています\")\n",
    "        else:\n",
    "            if np.isnan(X).any():\n",
    "                raise ValueError(\"入力データに欠損値が含まれています\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"予測を実行\"\"\"\n",
    "        try:\n",
    "            X = self.validate_input(X)\n",
    "            predictions = self.model.predict(X)\n",
    "            return predictions\n",
    "        except Exception as e:\n",
    "            print(f\"予測エラー: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"確率予測を実行（分類モデルの場合）\"\"\"\n",
    "        try:\n",
    "            X = self.validate_input(X)\n",
    "            if hasattr(self.model, 'predict_proba'):\n",
    "                probabilities = self.model.predict_proba(X)\n",
    "                return probabilities\n",
    "            else:\n",
    "                raise AttributeError(\"このモデルは確率予測をサポートしていません\")\n",
    "        except Exception as e:\n",
    "            print(f\"確率予測エラー: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"モデル情報の取得\"\"\"\n",
    "        info = {\n",
    "            'model_type': type(self.model).__name__,\n",
    "            'feature_count': len(self.feature_names) if self.feature_names else 'Unknown'\n",
    "        }\n",
    "        \n",
    "        # Pipelineの場合、ステップ情報を追加\n",
    "        if hasattr(self.model, 'steps'):\n",
    "            info['pipeline_steps'] = [step[0] for step in self.model.steps]\n",
    "        \n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelPredictorの使用例\n",
    "predictor = ModelPredictor(\n",
    "    model_path=model_filename,\n",
    "    feature_names=cancer_data.feature_names.tolist()\n",
    ")\n",
    "\n",
    "# モデル情報の表示\n",
    "print(\"\\nモデル情報:\")\n",
    "for key, value in predictor.get_model_info().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単一サンプルの予測\n",
    "sample_data = X_test_c.iloc[:5]  # 最初の5サンプル\n",
    "\n",
    "predictions = predictor.predict(sample_data)\n",
    "probabilities = predictor.predict_proba(sample_data)\n",
    "\n",
    "print(\"予測結果:\")\n",
    "for i, (pred, proba) in enumerate(zip(predictions, probabilities)):\n",
    "    print(f\"サンプル {i+1}: 予測={cancer_data.target_names[pred]}, 確率={proba[pred]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エラーハンドリングのテスト\n",
    "\n",
    "# 1. 欠損値を含むデータ\n",
    "sample_with_nan = sample_data.copy()\n",
    "sample_with_nan.iloc[0, 0] = np.nan\n",
    "\n",
    "try:\n",
    "    predictor.predict(sample_with_nan)\n",
    "except ValueError as e:\n",
    "    print(f\"期待通りのエラー: {e}\")\n",
    "\n",
    "# 2. 特徴量が不足しているデータ\n",
    "sample_missing_features = sample_data.iloc[:, :10]  # 最初の10特徴量のみ\n",
    "\n",
    "try:\n",
    "    predictor.predict(sample_missing_features)\n",
    "except ValueError as e:\n",
    "    print(f\"期待通りのエラー: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 完全なMLワークフローの例\n",
    "\n",
    "### データ読み込みからデプロイまでの全ステップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_ml_workflow(X, y, test_size=0.2, model_name='ml_model'):\n",
    "    \"\"\"\n",
    "    完全な機械学習ワークフロー\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like or DataFrame\n",
    "        特徴量データ\n",
    "    y : array-like\n",
    "        ターゲット変数\n",
    "    test_size : float\n",
    "        テストデータの割合\n",
    "    model_name : str\n",
    "        保存するモデルの名前\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : ワークフローの結果\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"完全なMLワークフロー開始\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ステップ1: データ分割\n",
    "    print(\"\\n[1/6] データ分割...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    print(f\"  訓練データ: {X_train.shape[0]} サンプル\")\n",
    "    print(f\"  テストデータ: {X_test.shape[0]} サンプル\")\n",
    "    \n",
    "    # ステップ2: パイプライン構築\n",
    "    print(\"\\n[2/6] パイプライン構築...\")\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    print(\"  パイプラインステップ:\", [step[0] for step in pipeline.steps])\n",
    "    \n",
    "    # ステップ3: Cross-validation\n",
    "    print(\"\\n[3/6] Cross-validation...\")\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"  CVスコア: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # ステップ4: モデル学習\n",
    "    print(\"\\n[4/6] モデル学習...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    print(\"  学習完了\")\n",
    "    \n",
    "    # ステップ5: モデル評価\n",
    "    print(\"\\n[5/6] モデル評価...\")\n",
    "    train_score = pipeline.score(X_train, y_train)\n",
    "    test_score = pipeline.score(X_test, y_test)\n",
    "    print(f\"  訓練スコア: {train_score:.4f}\")\n",
    "    print(f\"  テストスコア: {test_score:.4f}\")\n",
    "    \n",
    "    # ステップ6: モデル保存\n",
    "    print(\"\\n[6/6] モデル保存...\")\n",
    "    model_path = os.path.join(model_dir, f'{model_name}.pkl')\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    print(f\"  保存先: {model_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ワークフロー完了\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'pipeline': pipeline,\n",
    "        'cv_scores': cv_scores,\n",
    "        'train_score': train_score,\n",
    "        'test_score': test_score,\n",
    "        'model_path': model_path,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完全ワークフローの実行\n",
    "workflow_result = complete_ml_workflow(\n",
    "    X=X_cancer,\n",
    "    y=y_cancer,\n",
    "    test_size=0.2,\n",
    "    model_name='breast_cancer_rf_pipeline'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存したモデルを使った予測\n",
    "production_predictor = ModelPredictor(\n",
    "    model_path=workflow_result['model_path'],\n",
    "    feature_names=cancer_data.feature_names.tolist()\n",
    ")\n",
    "\n",
    "# テストデータで予測\n",
    "test_predictions = production_predictor.predict(workflow_result['X_test'])\n",
    "test_probabilities = production_predictor.predict_proba(workflow_result['X_test'])\n",
    "\n",
    "# 予測の信頼度分布\n",
    "confidence_scores = np.max(test_probabilities, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(confidence_scores, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Confidence Scores')\n",
    "plt.axvline(confidence_scores.mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {confidence_scores.mean():.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"平均信頼度: {confidence_scores.mean():.4f}\")\n",
    "print(f\"最小信頼度: {confidence_scores.min():.4f}\")\n",
    "print(f\"最大信頼度: {confidence_scores.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ベストプラクティスとチェックリスト\n",
    "\n",
    "### データ準備\n",
    "- [ ] データの品質チェック（欠損値、外れ値、データ型）\n",
    "- [ ] 適切なtrain/test分割（stratify、random_state）\n",
    "- [ ] 特徴量スケーリングの必要性確認\n",
    "\n",
    "### モデル開発\n",
    "- [ ] Pipelineを使用して前処理とモデルを統合\n",
    "- [ ] Cross-validationで汎化性能を評価\n",
    "- [ ] ハイパーパラメータチューニング実施\n",
    "- [ ] 過学習の確認（train vs test スコア）\n",
    "\n",
    "### モデル評価\n",
    "- [ ] 適切な評価指標の選択\n",
    "- [ ] 混同行列や学習曲線での詳細分析\n",
    "- [ ] 誤分類サンプルの分析\n",
    "\n",
    "### デプロイ準備\n",
    "- [ ] モデルをjoblibで保存\n",
    "- [ ] 予測関数にバリデーション機能を実装\n",
    "- [ ] エラーハンドリングの実装\n",
    "- [ ] モデルのバージョン管理\n",
    "- [ ] ドキュメント作成（モデルの使用方法、入力形式など）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. まとめ\n",
    "\n",
    "### 本ノートブックで学んだこと\n",
    "\n",
    "1. **Pipelineの基礎**\n",
    "   - 前処理とモデルを一つのワークフローに統合\n",
    "   - データリーク防止と再現性の向上\n",
    "\n",
    "2. **パイプライン内でのチューニング**\n",
    "   - `<ステップ名>__<パラメータ名>` 形式でのパラメータ指定\n",
    "   - GridSearchCVでの一括最適化\n",
    "\n",
    "3. **特徴量エンジニアリング**\n",
    "   - PolynomialFeaturesなどの変換をパイプラインに組み込み\n",
    "   - 複雑な前処理フローの構築\n",
    "\n",
    "4. **モデルの永続化**\n",
    "   - joblibを使った保存と読み込み\n",
    "   - バージョン管理のためのタイムスタンプ付与\n",
    "\n",
    "5. **本番環境向け実装**\n",
    "   - 入力バリデーション機能\n",
    "   - エラーハンドリング\n",
    "   - 予測の信頼度評価\n",
    "\n",
    "6. **完全なワークフロー**\n",
    "   - データ準備から評価、保存までの全ステップ\n",
    "   - 再利用可能な関数の実装\n",
    "\n",
    "### 次のステップ\n",
    "- APIサーバーでのモデル提供（Flask, FastAPI）\n",
    "- モデルのモニタリングとA/Bテスト\n",
    "- MLOpsツールの活用（MLflow, DVC等）\n",
    "- より高度なアンサンブル手法の実装"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
