{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ノートブック07: 多層パーセプトロン (MLP) の基礎\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックでは、多層パーセプトロン (Multi-Layer Perceptron, MLP) の基礎を学びます:\n",
    "\n",
    "1. **ニューラルネットワークの基礎**\n",
    "   - パーセプトロンから多層ネットワークへ\n",
    "   - ニューロン、重み、バイアスの役割\n",
    "   - 順伝播 (Forward Propagation)\n",
    "\n",
    "2. **活性化関数**\n",
    "   - Sigmoid、Tanh、ReLU、Leaky ReLU\n",
    "   - 各関数の特性と使い分け\n",
    "   - 勾配消失問題\n",
    "\n",
    "3. **MLPの構造**\n",
    "   - 入力層、隠れ層、出力層\n",
    "   - ネットワークの深さと幅\n",
    "   - パラメータ数の計算\n",
    "\n",
    "4. **学習プロセス**\n",
    "   - 損失関数\n",
    "   - 最適化アルゴリズム (SGD、Adam)\n",
    "   - 学習曲線の解釈\n",
    "\n",
    "5. **実践例**\n",
    "   - 非線形分類問題\n",
    "   - ハイパーパラメータの影響\n",
    "   - モデルの診断\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles, load_digits\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 日本語フォント設定\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# ランダムシード固定\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 活性化関数: ニューラルネットワークの非線形性\n",
    "\n",
    "### 活性化関数の役割\n",
    "\n",
    "活性化関数は、ニューラルネットワークに**非線形性**を導入します。活性化関数がなければ、どれだけ層を重ねても線形変換の組み合わせにすぎず、複雑なパターンを学習できません。\n",
    "\n",
    "### 主要な活性化関数\n",
    "\n",
    "1. **Sigmoid**: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "   - 出力: (0, 1)\n",
    "   - 用途: 二値分類の出力層\n",
    "   - 問題: 勾配消失\n",
    "\n",
    "2. **Tanh**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "   - 出力: (-1, 1)\n",
    "   - 用途: 隠れ層 (Sigmoidより良い)\n",
    "   - 問題: 勾配消失\n",
    "\n",
    "3. **ReLU**: $\\text{ReLU}(x) = \\max(0, x)$\n",
    "   - 出力: [0, ∞)\n",
    "   - 用途: 現代のデフォルト選択\n",
    "   - 利点: 勾配消失なし、計算が高速\n",
    "   - 問題: Dead neuron (負の領域で勾配0)\n",
    "\n",
    "4. **Leaky ReLU**: $\\text{LeakyReLU}(x) = \\max(\\alpha x, x)$, $\\alpha = 0.01$\n",
    "   - ReLUの改良版\n",
    "   - Dead neuron問題を軽減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数の定義\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# 微分 (勾配の理解のため)\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数とその微分の可視化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "activations = [\n",
    "    ('Sigmoid', sigmoid(x), sigmoid_derivative(x)),\n",
    "    ('Tanh', tanh(x), tanh_derivative(x)),\n",
    "    ('ReLU', relu(x), relu_derivative(x)),\n",
    "    ('Leaky ReLU', leaky_relu(x), leaky_relu_derivative(x))\n",
    "]\n",
    "\n",
    "for idx, (name, y, dy) in enumerate(activations):\n",
    "    ax = axes.ravel()[idx]\n",
    "    \n",
    "    # 関数\n",
    "    ax.plot(x, y, 'b-', lw=2.5, label=f'{name}', alpha=0.8)\n",
    "    # 微分\n",
    "    ax.plot(x, dy, 'r--', lw=1.5, label='Derivative', alpha=0.7)\n",
    "    \n",
    "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.set_xlabel('Input (x)', fontsize=11)\n",
    "    ax.set_ylabel('Output', fontsize=11)\n",
    "    ax.set_title(f'{name} Activation Function', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Activation Function Properties:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Sigmoid:     Output (0,1)   | Vanishing gradient problem\")\n",
    "print(\"Tanh:        Output (-1,1)  | Zero-centered, still vanishes\")\n",
    "print(\"ReLU:        Output [0,∞)   | Fast, no vanishing, dead neurons\")\n",
    "print(\"Leaky ReLU:  Output (-∞,∞)  | Prevents dead neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数の比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべての活性化関数を1つのグラフに\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.plot(x, sigmoid(x), 'b-', lw=2.5, label='Sigmoid', alpha=0.8)\n",
    "plt.plot(x, tanh(x), 'r-', lw=2.5, label='Tanh', alpha=0.8)\n",
    "plt.plot(x, relu(x), 'g-', lw=2.5, label='ReLU', alpha=0.8)\n",
    "plt.plot(x, leaky_relu(x), 'm-', lw=2.5, label='Leaky ReLU', alpha=0.8)\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.xlabel('Input (x)', fontsize=12)\n",
    "plt.ylabel('Output f(x)', fontsize=12)\n",
    "plt.title('Activation Functions Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper left')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-2, 5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配消失問題\n",
    "\n",
    "SigmoidとTanhは、入力の絶対値が大きいとき、微分が0に近づきます。これにより、深いネットワークで勾配が消失し、学習が困難になります。ReLUはこの問題を解決します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配消失の可視化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "derivatives = [\n",
    "    ('Sigmoid', sigmoid_derivative(x)),\n",
    "    ('Tanh', tanh_derivative(x)),\n",
    "    ('ReLU', relu_derivative(x))\n",
    "]\n",
    "\n",
    "for idx, (name, dy) in enumerate(derivatives):\n",
    "    axes[idx].plot(x, dy, 'r-', lw=2.5)\n",
    "    axes[idx].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[idx].fill_between(x, 0, dy, alpha=0.3, color='red')\n",
    "    axes[idx].set_xlabel('Input (x)', fontsize=11)\n",
    "    axes[idx].set_ylabel('Gradient', fontsize=11)\n",
    "    axes[idx].set_title(f'{name} Gradient', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Gradient Analysis:\")\n",
    "print(\"- Sigmoid: Max gradient = 0.25 (at x=0)\")\n",
    "print(\"- Tanh:    Max gradient = 1.0 (at x=0)\")\n",
    "print(\"- ReLU:    Gradient = 1.0 (for x>0), 0 (for x≤0)\")\n",
    "print(\"\\nReLU avoids vanishing gradient problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. MLPの構造: 層、ニューロン、パラメータ\n",
    "\n",
    "### ネットワークの構成要素\n",
    "\n",
    "1. **入力層 (Input Layer)**: 特徴量を受け取る\n",
    "2. **隠れ層 (Hidden Layers)**: 特徴を変換・抽出\n",
    "3. **出力層 (Output Layer)**: 予測を出力\n",
    "\n",
    "各ニューロンは:\n",
    "- **重み (Weights)**: 入力の重要度\n",
    "- **バイアス (Bias)**: オフセット\n",
    "- **活性化関数**: 非線形変換\n",
    "\n",
    "### パラメータ数の計算\n",
    "\n",
    "層 $i$ から層 $i+1$ への接続:\n",
    "- 重み: $n_i \\times n_{i+1}$\n",
    "- バイアス: $n_{i+1}$\n",
    "- 合計: $n_i \\times n_{i+1} + n_{i+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡単な非線形データの生成\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# データの標準化 (MLPでは必須!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# データの可視化\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black', s=80, alpha=0.8)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Non-linear Classification Data (Moons)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label='Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シンプルなMLPモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1隠れ層のMLPを訓練\n",
    "mlp_simple = MLPClassifier(\n",
    "    hidden_layer_sizes=(10,),  # 10ニューロンの隠れ層1つ\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate_init=0.01,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp_simple.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"MLP Architecture:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input layer:    {X_train.shape[1]} features\")\n",
    "print(f\"Hidden layers:  {mlp_simple.hidden_layer_sizes}\")\n",
    "print(f\"Output layer:   {len(np.unique(y))} classes\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Iterations:     {mlp_simple.n_iter_}\")\n",
    "print(f\"  Final loss:     {mlp_simple.loss_:.6f}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Train accuracy: {mlp_simple.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"  Test accuracy:  {mlp_simple.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネットワークパラメータの詳細"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重みとバイアスの詳細\n",
    "print(\"Network Parameters:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_params = 0\n",
    "for i, (w, b) in enumerate(zip(mlp_simple.coefs_, mlp_simple.intercepts_)):\n",
    "    layer_params = w.size + b.size\n",
    "    total_params += layer_params\n",
    "    \n",
    "    print(f\"\\nLayer {i+1}:\")\n",
    "    print(f\"  Weights shape:      {w.shape}\")\n",
    "    print(f\"  Biases shape:       {b.shape}\")\n",
    "    print(f\"  Layer parameters:   {layer_params}\")\n",
    "    print(f\"  Calculation:        {w.shape[0]} × {w.shape[1]} + {b.shape[0]} = {layer_params}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total network parameters: {total_params}\")\n",
    "print(f\"\\nThese parameters are learned during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 決定境界の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title='Decision Boundary'):\n",
    "    \"\"\"MLPの決定境界を可視化\"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    plt.contour(xx, yy, Z, colors='k', linewidths=0.8, alpha=0.5)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', \n",
    "                edgecolors='k', s=80, alpha=0.8)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(mlp_simple, X_train_scaled, y_train, \n",
    "                      'MLP Decision Boundary (1 Hidden Layer, 10 Neurons)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ニューロン数の影響\n",
    "\n",
    "隠れ層のニューロン数は、ネットワークの**表現能力**を決定します:\n",
    "\n",
    "- **少ないニューロン**: シンプルな境界 → 未学習のリスク\n",
    "- **多いニューロン**: 複雑な境界 → 過学習のリスク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なるニューロン数で比較\n",
    "neuron_counts = [2, 5, 10, 25, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 11))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, n_neurons in enumerate(neuron_counts):\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(n_neurons,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    acc_train = mlp.score(X_train_scaled, y_train)\n",
    "    acc_test = mlp.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # 決定境界\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    axes[idx].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, \n",
    "                      cmap='RdYlBu', edgecolors='k', s=30, alpha=0.8)\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=10)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=10)\n",
    "    axes[idx].set_title(f'{n_neurons} Neurons\\nTrain: {acc_train:.3f}, Test: {acc_test:.3f}',\n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- 2-5 neurons: Simple boundary, may underfit\")\n",
    "print(\"- 10-25 neurons: Good balance\")\n",
    "print(\"- 50-100 neurons: Complex boundary, may overfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ネットワークの深さ: 層数の影響\n",
    "\n",
    "**深さ (Depth)** は隠れ層の数を指します:\n",
    "\n",
    "- **浅いネットワーク (1-2層)**: シンプルなパターン\n",
    "- **深いネットワーク (3層以上)**: 階層的な特徴抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なる深さのネットワークを比較\n",
    "architectures = [\n",
    "    ((10,), '1 Layer: (10)'),\n",
    "    ((20, 10), '2 Layers: (20, 10)'),\n",
    "    ((30, 20, 10), '3 Layers: (30, 20, 10)'),\n",
    "    ((40, 30, 20, 10), '4 Layers: (40, 30, 20, 10)')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, (arch, label) in enumerate(architectures):\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=arch,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    acc_train = mlp.score(X_train_scaled, y_train)\n",
    "    acc_test = mlp.score(X_test_scaled, y_test)\n",
    "    n_params = sum(w.size + b.size for w, b in zip(mlp.coefs_, mlp.intercepts_))\n",
    "    \n",
    "    results.append({\n",
    "        'Architecture': str(arch),\n",
    "        'Depth': len(arch),\n",
    "        'Parameters': n_params,\n",
    "        'Train Acc': acc_train,\n",
    "        'Test Acc': acc_test\n",
    "    })\n",
    "    \n",
    "    # 決定境界\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    axes[idx].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, \n",
    "                      cmap='RdYlBu', edgecolors='k', s=30, alpha=0.8)\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=10)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=10)\n",
    "    axes[idx].set_title(f'{label}\\nTrain: {acc_train:.3f}, Test: {acc_test:.3f}, Params: {n_params}',\n",
    "                        fontsize=10, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 結果をテーブルで表示\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nArchitecture Comparison:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 活性化関数の比較\n",
    "\n",
    "実際のデータで、どの活性化関数が最適かを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なる活性化関数で比較\n",
    "activations = ['identity', 'logistic', 'tanh', 'relu']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "activation_results = []\n",
    "\n",
    "for idx, activation in enumerate(activations):\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(50,),\n",
    "        activation=activation,\n",
    "        solver='adam',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    acc_train = mlp.score(X_train_scaled, y_train)\n",
    "    acc_test = mlp.score(X_test_scaled, y_test)\n",
    "    \n",
    "    activation_results.append({\n",
    "        'Activation': activation,\n",
    "        'Train Acc': acc_train,\n",
    "        'Test Acc': acc_test,\n",
    "        'Iterations': mlp.n_iter_\n",
    "    })\n",
    "    \n",
    "    # 決定境界\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    axes[idx].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, \n",
    "                      cmap='RdYlBu', edgecolors='k', s=30, alpha=0.8)\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=10)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=10)\n",
    "    axes[idx].set_title(f'{activation.upper()}\\nTest Acc: {acc_test:.3f}, Iterations: {mlp.n_iter_}',\n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_act = pd.DataFrame(activation_results)\n",
    "print(\"\\nActivation Function Comparison:\")\n",
    "print(df_act.to_string(index=False))\n",
    "print(\"\\nBest activation: ReLU (fast convergence, good performance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 学習曲線: 損失の推移\n",
    "\n",
    "学習曲線は、訓練の進行を可視化し、収束を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線を追跡\n",
    "mlp_track = MLPClassifier(\n",
    "    hidden_layer_sizes=(50, 25),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp_track.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 損失曲線\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mlp_track.loss_curve_, 'b-', linewidth=1.5)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Curve', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(mlp_track.loss_curve_, 'b-', linewidth=1.5)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Loss (log scale)', fontsize=12)\n",
    "plt.title('Training Loss Curve (Log Scale)', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {mlp_track.loss_curve_[-1]:.6f}\")\n",
    "print(f\"Total iterations: {mlp_track.n_iter_}\")\n",
    "print(f\"Training accuracy: {mlp_track.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"Test accuracy: {mlp_track.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習率の影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なる学習率で比較\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "for lr in learning_rates:\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(50,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    plt.plot(mlp.loss_curve_, label=f'lr={lr}', linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Effect of Learning Rate on Convergence', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning Rate Effects:\")\n",
    "print(\"- Too small (0.0001): Slow convergence\")\n",
    "print(\"- Optimal (0.001-0.01): Balanced convergence\")\n",
    "print(\"- Too large (0.1): Oscillating or diverging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 実践例: 手書き数字認識\n",
    "\n",
    "より実践的な問題で、MLPの能力を試します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手書き数字データセット\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(\"Digits Dataset:\")\n",
    "print(f\"Samples: {X_digits.shape[0]}\")\n",
    "print(f\"Features: {X_digits.shape[1]} (8x8 pixel images)\")\n",
    "print(f\"Classes: {len(np.unique(y_digits))} (digits 0-9)\")\n",
    "\n",
    "# サンプル画像を表示\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(digits.images[idx], cmap='gray')\n",
    "    ax.set_title(f'Label: {y_digits[idx]}', fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの分割と前処理\n",
    "X_train_dig, X_test_dig, y_train_dig, y_test_dig = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.2, random_state=42, stratify=y_digits\n",
    ")\n",
    "\n",
    "scaler_dig = StandardScaler()\n",
    "X_train_dig_scaled = scaler_dig.fit_transform(X_train_dig)\n",
    "X_test_dig_scaled = scaler_dig.transform(X_test_dig)\n",
    "\n",
    "# MLPモデルの訓練\n",
    "mlp_digits = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp_digits.fit(X_train_dig_scaled, y_train_dig)\n",
    "\n",
    "# 評価\n",
    "y_pred_dig = mlp_digits.predict(X_test_dig_scaled)\n",
    "\n",
    "print(\"\\nMLP Performance on Digits:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training accuracy: {mlp_digits.score(X_train_dig_scaled, y_train_dig):.4f}\")\n",
    "print(f\"Test accuracy: {mlp_digits.score(X_test_dig_scaled, y_test_dig):.4f}\")\n",
    "print(f\"Iterations: {mlp_digits.n_iter_}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_dig, y_pred_dig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列\n",
    "cm = confusion_matrix(y_test_dig, y_pred_dig)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(10), yticklabels=range(10),\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix: Handwritten Digits', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測の可視化\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    image = X_test_dig[idx].reshape(8, 8)\n",
    "    true_label = y_test_dig[idx]\n",
    "    pred_label = y_pred_dig[idx]\n",
    "    \n",
    "    ax.imshow(image, cmap='gray')\n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    ax.set_title(f'True: {true_label}, Pred: {pred_label}', \n",
    "                 fontsize=10, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 他のモデルとの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数のモデルを比較\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'MLP (50)': MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42),\n",
    "    'MLP (100,50)': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_dig_scaled, y_train_dig)\n",
    "    train_acc = model.score(X_train_dig_scaled, y_train_dig)\n",
    "    test_acc = model.score(X_test_dig_scaled, y_test_dig)\n",
    "    cv_scores = cross_val_score(model, X_train_dig_scaled, y_train_dig, cv=5)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Model': name,\n",
    "        'Train Acc': train_acc,\n",
    "        'Test Acc': test_acc,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std()\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_results).sort_values('Test Acc', ascending=False)\n",
    "\n",
    "print(\"\\nModel Comparison on Digits Dataset:\")\n",
    "print(\"=\"*60)\n",
    "print(df_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Train vs Test Accuracy\n",
    "x = np.arange(len(df_comparison))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, df_comparison['Train Acc'], width, \n",
    "            label='Train', alpha=0.8, color='steelblue')\n",
    "axes[0].bar(x + width/2, df_comparison['Test Acc'], width, \n",
    "            label='Test', alpha=0.8, color='coral')\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Train vs Test Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(df_comparison['Model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# CV Mean with Error Bars\n",
    "axes[1].bar(x, df_comparison['CV Mean'], yerr=df_comparison['CV Std'], \n",
    "            alpha=0.8, color='seagreen', capsize=5)\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('CV Accuracy', fontsize=12)\n",
    "axes[1].set_title('Cross-Validation Performance', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(df_comparison['Model'], rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## まとめ\n",
    "\n",
    "### MLPの主要ポイント\n",
    "\n",
    "1. **ニューラルネットワークの構造**\n",
    "   - 入力層 → 隠れ層 → 出力層\n",
    "   - 各層のニューロンは重みとバイアスを持つ\n",
    "   - 活性化関数で非線形変換\n",
    "\n",
    "2. **活性化関数**\n",
    "   - ReLU: 現代のデフォルト (高速、勾配消失なし)\n",
    "   - Sigmoid/Tanh: 勾配消失問題あり\n",
    "   - Leaky ReLU: Dead neuron問題を軽減\n",
    "\n",
    "3. **アーキテクチャの選択**\n",
    "   - ニューロン数: 50-100から開始\n",
    "   - 層数: 1-2層で開始、必要に応じて深くする\n",
    "   - パラメータ数: $(n_i \\times n_{i+1}) + n_{i+1}$\n",
    "\n",
    "4. **学習のポイント**\n",
    "   - **必ずスケーリング**: StandardScalerを使用\n",
    "   - 学習率: 0.001から開始 (Adam)\n",
    "   - Early stopping: 過学習を防止\n",
    "   - 損失曲線: 収束を確認\n",
    "\n",
    "5. **診断**\n",
    "   - 学習曲線で過学習/未学習を判定\n",
    "   - 決定境界で複雑さを確認\n",
    "   - Cross-validationで汎化性能を評価\n",
    "\n",
    "### MLPの長所と短所\n",
    "\n",
    "**長所:**\n",
    "- 非線形パターンを効果的に学習\n",
    "- 柔軟なアーキテクチャ\n",
    "- 多様なタスクに適用可能\n",
    "- 特徴量エンジニアリングが不要\n",
    "\n",
    "**短所:**\n",
    "- ブラックボックスモデル (解釈性が低い)\n",
    "- ハイパーパラメータが多い\n",
    "- スケーリングが必須\n",
    "- 訓練に時間がかかる場合がある\n",
    "\n",
    "### 実践的なガイドライン\n",
    "\n",
    "1. **データの前処理**: 必ずStandardScalerで標準化\n",
    "2. **シンプルから開始**: (50,) または (100,)\n",
    "3. **ReLUを使用**: 特別な理由がない限り\n",
    "4. **Early stoppingを有効化**: 過学習を防止\n",
    "5. **学習曲線を確認**: 収束と過学習をチェック\n",
    "6. **Cross-validationで評価**: テストセットだけに頼らない\n",
    "\n",
    "---\n",
    "\n",
    "## 練習問題\n",
    "\n",
    "1. **活性化関数の実験**: 異なるデータセット (make_circles) で活性化関数を比較してください\n",
    "2. **アーキテクチャ探索**: 最適な層数とニューロン数を見つけてください\n",
    "3. **学習率の調整**: 異なる学習率で収束速度を比較してください\n",
    "4. **他のデータセット**: IrisやワインデータセットでMLPを試してください\n",
    "5. **正則化**: alphaパラメータを変えて過学習を制御してください\n",
    "\n",
    "---\n",
    "\n",
    "**次のステップ**: ノートブック08で、MLPのハイパーパラメータ空間を体系的に探索します!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
