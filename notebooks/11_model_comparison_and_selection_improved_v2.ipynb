{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ノートブック11: モデル比較と選択\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックでは、複数の機械学習モデルを体系的に比較し、最適なモデルを選択する手法を学びます:\n",
    "\n",
    "1. **公平な比較**\n",
    "   - クロスバリデーション\n",
    "   - 同じデータ分割\n",
    "   - 標準化された評価指標\n",
    "\n",
    "2. **統計的検定**\n",
    "   - t検定による有意差の検証\n",
    "   - モデル間の性能差\n",
    "\n",
    "3. **アンサンブル手法**\n",
    "   - Voting (Hard/Soft)\n",
    "   - Stacking\n",
    "   - 性能向上の仕組み\n",
    "\n",
    "4. **選択基準**\n",
    "   - 性能 vs 解釈性\n",
    "   - 訓練時間 vs 予測時間\n",
    "   - 過学習のリスク\n",
    "\n",
    "5. **実践的な意思決定**\n",
    "   - ビジネス要件\n",
    "   - 運用制約\n",
    "   - コストベネフィット分析\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, f1_score\n",
    ")\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. データセットとベースライン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ生成\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=3,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {len(np.unique(y))}\")\n",
    "\n",
    "# ベースライン: ランダム予測\n",
    "baseline_accuracy = max(np.bincount(y_train)) / len(y_train)\n",
    "print(f\"\\nBaseline (majority class): {baseline_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. モデル定義と初期評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較するモデルを定義\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, \n",
    "                         early_stopping=True, random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"Comparing {len(models)} models\")\n",
    "print(\"\\nModel List:\")\n",
    "for i, name in enumerate(models.keys(), 1):\n",
    "    print(f\"  {i}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クロスバリデーションによる比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold クロスバリデーション\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "results = {}\n",
    "cv_scores = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"Cross-Validation Results (10-fold):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # 訓練時間の測定\n",
    "    start_time = time.time()\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    cv_scores[name] = scores\n",
    "    training_times[name] = train_time\n",
    "    \n",
    "    results[name] = {\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'min': scores.min(),\n",
    "        'max': scores.max(),\n",
    "        'time': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:22s}: {scores.mean():.4f} (+/- {scores.std():.4f}) | Time: {train_time:.2f}s\")\n",
    "\n",
    "# ランキング\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
    "\n",
    "print(\"\\nRanking by CV Accuracy:\")\n",
    "print(\"=\"*70)\n",
    "for rank, (name, res) in enumerate(sorted_results, 1):\n",
    "    print(f\"{rank:2d}. {name:22s}: {res['mean']:.4f} (+/- {res['std']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV結果の包括的な可視化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. ボックスプロット\n",
    "names = list(cv_scores.keys())\n",
    "scores_list = [cv_scores[name] for name in names]\n",
    "\n",
    "bp = axes[0, 0].boxplot(scores_list, labels=names, patch_artist=True, showmeans=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "axes[0, 0].set_ylabel('CV Accuracy', fontsize=11)\n",
    "axes[0, 0].set_title('Model Comparison: CV Score Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. 平均スコアとエラーバー\n",
    "means = [results[name]['mean'] for name in names]\n",
    "stds = [results[name]['std'] for name in names]\n",
    "x_pos = np.arange(len(names))\n",
    "\n",
    "axes[0, 1].barh(x_pos, means, xerr=stds, capsize=5, alpha=0.7)\n",
    "axes[0, 1].set_yticks(x_pos)\n",
    "axes[0, 1].set_yticklabels(names, fontsize=9)\n",
    "axes[0, 1].set_xlabel('CV Accuracy', fontsize=11)\n",
    "axes[0, 1].set_title('Mean CV Score with Std Dev', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. 訓練時間\n",
    "times = [training_times[name] for name in names]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(names)))\n",
    "axes[1, 0].bar(range(len(names)), times, color=colors, alpha=0.7)\n",
    "axes[1, 0].set_xticks(range(len(names)))\n",
    "axes[1, 0].set_xticklabels(names, rotation=45, ha='right', fontsize=9)\n",
    "axes[1, 0].set_ylabel('Training Time (seconds)', fontsize=11)\n",
    "axes[1, 0].set_title('Training Time Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. 性能 vs 訓練時間\n",
    "axes[1, 1].scatter(times, means, s=150, alpha=0.6)\n",
    "for i, name in enumerate(names):\n",
    "    axes[1, 1].annotate(name, (times[i], means[i]), \n",
    "                        fontsize=8, ha='right', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('CV Accuracy', fontsize=11)\n",
    "axes[1, 1].set_title('Performance vs Training Time', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Top-right in performance/time plot: Best tradeoff\")\n",
    "print(\"- Low variance across folds: More stable model\")\n",
    "print(\"- Fast training: Better for iterative development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 統計的有意性検定\n",
    "\n",
    "モデル間の性能差が統計的に有意かを検証します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ペアワイズt検定\n",
    "model_names = list(cv_scores.keys())\n",
    "n_models = len(model_names)\n",
    "\n",
    "p_values = np.ones((n_models, n_models))\n",
    "\n",
    "for i in range(n_models):\n",
    "    for j in range(i+1, n_models):\n",
    "        # 対応のあるt検定\n",
    "        t_stat, p_value = stats.ttest_rel(cv_scores[model_names[i]], \n",
    "                                          cv_scores[model_names[j]])\n",
    "        p_values[i, j] = p_value\n",
    "        p_values[j, i] = p_value\n",
    "\n",
    "# ヒートマップで可視化\n",
    "plt.figure(figsize=(11, 9))\n",
    "mask = np.triu(np.ones_like(p_values, dtype=bool), k=1)\n",
    "sns.heatmap(p_values, annot=True, fmt='.3f', cmap='RdYlGn_r',\n",
    "            xticklabels=model_names, yticklabels=model_names,\n",
    "            mask=mask, vmin=0, vmax=0.1, cbar_kws={'label': 'p-value'})\n",
    "plt.title('Pairwise t-test p-values\\n(Green = significant difference at α=0.05)',\n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- p < 0.05: Statistically significant difference (green)\")\n",
    "print(\"- p > 0.05: No significant difference (red)\")\n",
    "print(\"\\nSignificant pairs:\")\n",
    "for i in range(n_models):\n",
    "    for j in range(i+1, n_models):\n",
    "        if p_values[i, j] < 0.05:\n",
    "            better = model_names[i] if results[model_names[i]]['mean'] > results[model_names[j]]['mean'] else model_names[j]\n",
    "            print(f\"  {model_names[i]} vs {model_names[j]}: p={p_values[i, j]:.4f} → {better} is better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. テストセット評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべてのモデルでテストセット評価\n",
    "test_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # 訓練\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 評価\n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # 予測時間\n",
    "    start_time = time.time()\n",
    "    _ = model.predict(X_test_scaled)\n",
    "    pred_time = time.time() - start_time\n",
    "    \n",
    "    test_results.append({\n",
    "        'Model': name,\n",
    "        'Train': train_score,\n",
    "        'Test': test_score,\n",
    "        'Overfit': train_score - test_score,\n",
    "        'Pred_Time_ms': pred_time * 1000\n",
    "    })\n",
    "\n",
    "df_test = pd.DataFrame(test_results).sort_values('Test', ascending=False)\n",
    "\n",
    "print(\"Test Set Results:\")\n",
    "print(\"=\"*80)\n",
    "print(df_test.to_string(index=False))\n",
    "\n",
    "print(\"\\nOverfitting Analysis:\")\n",
    "print(\"  High overfitting (>0.10): Risk of poor generalization\")\n",
    "print(\"  Moderate (0.05-0.10): Acceptable\")\n",
    "print(\"  Low (<0.05): Good generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. アンサンブル手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier (Hard Voting)\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "        ('mlp', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, \n",
    "                              early_stopping=True, random_state=42))\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Voting Classifier (Soft Voting)\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "        ('mlp', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500,\n",
    "                              early_stopping=True, random_state=42))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "        ('svm', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# アンサンブルモデルの評価\n",
    "print(\"Ensemble Methods:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ensemble_results = []\n",
    "for name, model in [('Hard Voting', voting_hard), \n",
    "                    ('Soft Voting', voting_soft), \n",
    "                    ('Stacking', stacking)]:\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    ensemble_results.append({\n",
    "        'Method': name,\n",
    "        'CV Mean': scores.mean(),\n",
    "        'CV Std': scores.std(),\n",
    "        'Test': test_score\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  CV Score:   {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "    print(f\"  Test Score: {test_score:.4f}\")\n",
    "\n",
    "df_ensemble = pd.DataFrame(ensemble_results)\n",
    "print(\"\\nEnsemble Summary:\")\n",
    "print(df_ensemble.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 最終モデル選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべてのモデルを統合して比較\n",
    "all_models = {\n",
    "    **models,\n",
    "    'Voting (Soft)': voting_soft,\n",
    "    'Stacking': stacking\n",
    "}\n",
    "\n",
    "final_comparison = []\n",
    "\n",
    "for name, model in all_models.items():\n",
    "    cv_scores_model = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    if not hasattr(model, 'classes_'):\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    final_comparison.append({\n",
    "        'Model': name,\n",
    "        'CV Mean': cv_scores_model.mean(),\n",
    "        'CV Std': cv_scores_model.std(),\n",
    "        'Test': test_score\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(final_comparison).sort_values('Test', ascending=False)\n",
    "\n",
    "print(\"\\nFinal Model Ranking:\")\n",
    "print(\"=\"*70)\n",
    "print(df_final.to_string(index=False))\n",
    "\n",
    "# 最良モデルの選択\n",
    "best_model_name = df_final.iloc[0]['Model']\n",
    "best_model = all_models[best_model_name]\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"SELECTED MODEL: {best_model_name}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy: {df_final.iloc[0]['Test']:.4f}\")\n",
    "print(f\"CV Accuracy:   {df_final.iloc[0]['CV Mean']:.4f} (+/- {df_final.iloc[0]['CV Std']:.4f})\")\n",
    "\n",
    "# 詳細な評価\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 混同行列\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title(f'Confusion Matrix: {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## まとめ\n",
    "\n",
    "### モデル選択の要点\n",
    "\n",
    "1. **公平な比較**\n",
    "   - 同じデータ分割\n",
    "   - クロスバリデーション\n",
    "   - 標準化された前処理\n",
    "\n",
    "2. **統計的検定**\n",
    "   - 性能差の有意性を確認\n",
    "   - 小さな差は無視する\n",
    "\n",
    "3. **アンサンブル**\n",
    "   - Voting: 多数決\n",
    "   - Stacking: メタ学習\n",
    "   - 通常、単一モデルより良い\n",
    "\n",
    "4. **選択基準**\n",
    "   - **性能**: 最優先\n",
    "   - **安定性**: CVの標準偏差\n",
    "   - **過学習**: Train-Testギャップ\n",
    "   - **速度**: 訓練・予測時間\n",
    "   - **解釈性**: ビジネス要件\n",
    "\n",
    "5. **実践的考慮事項**\n",
    "   - 運用コスト\n",
    "   - メンテナンス性\n",
    "   - スケーラビリティ\n",
    "   - 説明可能性\n",
    "\n",
    "### 意思決定フレームワーク\n",
    "\n",
    "```\n",
    "IF 解釈性が重要:\n",
    "    → Logistic Regression, Decision Tree\n",
    "ELSE IF 性能最優先:\n",
    "    → Ensemble Methods, MLP\n",
    "ELSE IF 高速予測が必要:\n",
    "    → Random Forest, KNN\n",
    "ELSE IF バランス重視:\n",
    "    → Gradient Boosting, SVM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**次のステップ**: ノートブック12で、完全なMLパイプラインを構築し、本番環境へのデプロイを準備します!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
