{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 26. Tabularãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚° - Neural Networks for Tabular Data\n",
    "\n",
    "## æ¦‚è¦\n",
    "ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ç‰¹åŒ–ã—ãŸãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚PyTorchã§æ§‹ç¯‰ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨SHAPã«ã‚ˆã‚‹è§£é‡ˆå¯èƒ½æ€§ã‚’å®Ÿè£…ã—ã€GBDTã¨ã®æ¯”è¼ƒã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "## å­¦ç¿’ç›®æ¨™\n",
    "- Tabularãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç‰¹å¾´ã‚’ç†è§£ã§ãã‚‹\n",
    "- PyTorchã§Tabularç”¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè£…ã§ãã‚‹\n",
    "- SHAPã‚’ä½¿ã£ãŸè§£é‡ˆå¯èƒ½æ€§ã‚’ç†è§£ã§ãã‚‹\n",
    "- GBDTã¨ã®æ¯”è¼ƒãŒã§ãã‚‹\n",
    "- å®Ÿå‹™ã§Tabular DLã‚’é©ç”¨ã§ãã‚‹åˆ¤æ–­ãŒã§ãã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# SHAP for interpretability\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®š\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# MacBookå¯¾å¿œï¼šCPUã‚’ä½¿ç”¨\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. ãªãœTabularãƒ‡ãƒ¼ã‚¿ã«ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼Ÿ\n",
    "\n",
    "### Tabularãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´\n",
    "\n",
    "**Tabularãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰**ã¯ã€è¡Œã¨åˆ—ã§æ§‹æˆã•ã‚ŒãŸæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚\n",
    "\n",
    "### å¾“æ¥ã®å¸¸è­˜\n",
    "\n",
    "é•·å¹´ã€Tabularãƒ‡ãƒ¼ã‚¿ã§ã¯**GBDTï¼ˆGradient Boosting Decision Treesï¼‰ãŒæœ€å¼·**ã¨ã•ã‚Œã¦ãã¾ã—ãŸï¼š\n",
    "\n",
    "- âœ… XGBoostã€LightGBMã€CatBoost\n",
    "- âœ… Kaggleã‚³ãƒ³ãƒšã§åœ§å€’çš„ãªå‹ç‡\n",
    "- âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸è¦\n",
    "- âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒæ¯”è¼ƒçš„å®¹æ˜“\n",
    "\n",
    "### ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®èª²é¡Œ\n",
    "\n",
    "ä¸€æ–¹ã€å¾“æ¥ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯Tabularãƒ‡ãƒ¼ã‚¿ã§è‹¦æˆ¦ï¼š\n",
    "\n",
    "- âŒ GBDTã‚ˆã‚Šæ€§èƒ½ãŒä½ã„\n",
    "- âŒ å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦\n",
    "- âŒ ç‰¹å¾´é‡ã®å‰å‡¦ç†ãŒå¿…é ˆï¼ˆæ­£è¦åŒ–ãªã©ï¼‰\n",
    "- âŒ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒé›£ã—ã„\n",
    "\n",
    "### æ–°ä¸–ä»£ã®Tabular Deep Learning\n",
    "\n",
    "è¿‘å¹´ã€Tabularãƒ‡ãƒ¼ã‚¿å°‚ç”¨ã«è¨­è¨ˆã•ã‚ŒãŸDLãƒ¢ãƒ‡ãƒ«ãŒç™»å ´ï¼š\n",
    "\n",
    "1. **TabNet** (Google Research, 2019)\n",
    "   - è§£é‡ˆå¯èƒ½æ€§ã‚’æŒã¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "   - Sequential Attentionæ©Ÿæ§‹\n",
    "\n",
    "2. **NODE** (Neural Oblivious Decision Ensembles, 2019)\n",
    "   - æ±ºå®šæœ¨ã¨NNã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰\n",
    "\n",
    "3. **FT-Transformer** (Feature Tokenizer Transformer, 2021)\n",
    "   - å„ç‰¹å¾´é‡ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "\n",
    "æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€**PyTorchã§æ§‹ç¯‰ã—ãŸæœ€é©åŒ–ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**ã¨**SHAP**ã‚’ä½¿ç”¨ã—ã¦ã€Tabularãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¾ã™ã€‚\n",
    "\n",
    "### ã„ã¤Tabular DLã‚’ä½¿ã†ã¹ãã‹\n",
    "\n",
    "| æ¡ä»¶ | æ¨å¥¨ |\n",
    "|------|------|\n",
    "| ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º < 10k | GBDT |\n",
    "| ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º > 100k | Tabular DLæ¤œè¨ |\n",
    "| è§£é‡ˆå¯èƒ½æ€§ãŒé‡è¦ | GBDT + SHAP ã¾ãŸã¯ NN + SHAP |\n",
    "| é«˜é€Ÿæ¨è«–ãŒå¿…è¦ | GBDT |\n",
    "| GPUãŒä½¿ãˆã‚‹ | Tabular DL |\n",
    "| ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°å¤šã„ | CatBoost |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ï¼ˆä¹³ãŒã‚“è¨ºæ–­ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X.shape}\")\n",
    "print(f\"ç‰¹å¾´é‡æ•°: {X.shape[1]}\")\n",
    "print(f\"ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}\")\n",
    "print(f\"\\nã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nç‰¹å¾´é‡ã®ä¾‹:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ã•ã‚‰ã«æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}\")\n",
    "print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {X_val.shape}\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}\")\n",
    "\n",
    "# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯å¿…é ˆï¼‰\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nâš ï¸ é‡è¦:\")\n",
    "print(\"- ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå¿…é ˆ\")\n",
    "print(\"- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§fitã—ã€æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯transformã®ã¿\")\n",
    "print(\"- GBDTã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸è¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. PyTorchã§Tabularãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰\n",
    "\n",
    "### Tabularç”¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨­è¨ˆ\n",
    "\n",
    "**æœ€é©ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç‰¹å¾´ï¼š**\n",
    "\n",
    "1. **Batch Normalization**\n",
    "   - å„å±¤ã®å…¥åŠ›ã‚’æ­£è¦åŒ–\n",
    "   - å­¦ç¿’ã®å®‰å®šåŒ–ã¨é«˜é€ŸåŒ–\n",
    "\n",
    "2. **Dropout**\n",
    "   - éå­¦ç¿’ã‚’é˜²ã\n",
    "   - æ±åŒ–æ€§èƒ½ã®å‘ä¸Š\n",
    "\n",
    "3. **ReLU Activation**\n",
    "   - å‹¾é…æ¶ˆå¤±å•é¡Œã‚’å›é¿\n",
    "   - è¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„\n",
    "\n",
    "4. **é©åˆ‡ãªå±¤ã®æ·±ã•ã¨å¹…**\n",
    "   - Tabularãƒ‡ãƒ¼ã‚¿ã«ã¯æ·±ã™ããªã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒæœ‰åŠ¹\n",
    "   - é€šå¸¸2-4å±¤ã§ååˆ†\n",
    "\n",
    "### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³\n",
    "\n",
    "```\n",
    "Input (30 features)\n",
    "    â†“\n",
    "Linear(30 â†’ 128)\n",
    "    â†“\n",
    "BatchNorm â†’ ReLU â†’ Dropout(0.3)\n",
    "    â†“\n",
    "Linear(128 â†’ 64)\n",
    "    â†“\n",
    "BatchNorm â†’ ReLU â†’ Dropout(0.3)\n",
    "    â†“\n",
    "Linear(64 â†’ 32)\n",
    "    â†“\n",
    "BatchNorm â†’ ReLU â†’ Dropout(0.2)\n",
    "    â†“\n",
    "Linear(32 â†’ 2)\n",
    "    â†“\n",
    "Output (2 classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabularç”¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®šç¾©\n",
    "class TabularNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], output_dim=2, dropout_rates=[0.3, 0.3, 0.2]):\n",
    "        super(TabularNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # éš ã‚Œå±¤ã®æ§‹ç¯‰\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if i < len(dropout_rates):\n",
    "                layers.append(nn.Dropout(dropout_rates[i]))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # å‡ºåŠ›å±¤\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = TabularNN(input_dim=input_dim, hidden_dims=[128, 64, 32], output_dim=2)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:\")\n",
    "print(model)\n",
    "print(f\"\\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### 3.1 å­¦ç¿’ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™\n",
    "def create_dataloader(X, y, batch_size=32, shuffle=True):\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    y_tensor = torch.LongTensor(y.values if isinstance(y, pd.Series) else y)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = create_dataloader(X_train_scaled, y_train, batch_size=32, shuffle=True)\n",
    "val_loader = create_dataloader(X_val_scaled, y_val, batch_size=32, shuffle=False)\n",
    "test_loader = create_dataloader(X_test_scaled, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "print(\"å­¦ç¿’ã®æº–å‚™å®Œäº†\")\n",
    "print(f\"- æå¤±é–¢æ•°: CrossEntropyLoss\")\n",
    "print(f\"- ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: Adam (lr=0.001)\")\n",
    "print(f\"- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©: ReduceLROnPlateau\")\n",
    "print(f\"- ãƒãƒƒãƒã‚µã‚¤ã‚º: 32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### 3.2 å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’é–¢æ•°\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# è©•ä¾¡é–¢æ•°\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    return total_loss / len(loader), correct / total, auc\n",
    "\n",
    "print(\"å­¦ç¿’é–¢æ•°ã®æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ã®å®Ÿè¡Œ\n",
    "num_epochs = 100\n",
    "best_val_auc = 0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_auc': []\n",
    "}\n",
    "\n",
    "print(\"å­¦ç¿’é–‹å§‹...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # è¨“ç·´\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # æ¤œè¨¼\n",
    "    val_loss, val_acc, val_auc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®æ›´æ–°\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # å±¥æ­´ã®ä¿å­˜\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_epoch = epoch\n",
    "        patience_counter = 0\n",
    "        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "        print(f\"  Best Val AUC: {best_val_auc:.4f} (Epoch {best_epoch+1})\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "print(f\"\\nå­¦ç¿’å®Œäº†\")\n",
    "print(f\"æœ€è‰¯ã‚¨ãƒãƒƒã‚¯: {best_epoch+1}\")\n",
    "print(f\"æœ€è‰¯æ¤œè¨¼AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 3.3 å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Lossæ›²ç·š\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].axvline(best_epoch, color='red', linestyle='--', \n",
    "                label=f'Best Epoch ({best_epoch+1})', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracyæ›²ç·š\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].axvline(best_epoch, color='red', linestyle='--', \n",
    "                label=f'Best Epoch ({best_epoch+1})', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# AUCæ›²ç·š\n",
    "axes[2].plot(history['val_auc'], label='Validation AUC', linewidth=2, color='orange')\n",
    "axes[2].axvline(best_epoch, color='red', linestyle='--', \n",
    "                label=f'Best Epoch ({best_epoch+1})', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch', fontsize=11)\n",
    "axes[2].set_ylabel('AUC', fontsize=11)\n",
    "axes[2].set_title('Validation AUC', fontsize=12, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è¦³å¯Ÿ:\")\n",
    "print(f\"- æœ€è‰¯ã‚¨ãƒãƒƒã‚¯: {best_epoch+1}\")\n",
    "print(f\"- Early stoppingã«ã‚ˆã‚Šéå­¦ç¿’ã‚’é˜²æ­¢\")\n",
    "print(f\"- æ¤œè¨¼AUCã¯ {best_val_auc:.4f} ã«åˆ°é”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 3.4 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡\n",
    "model.eval()\n",
    "y_pred_nn = []\n",
    "y_proba_nn = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = outputs.max(1)\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "        \n",
    "        y_pred_nn.extend(predicted.cpu().numpy())\n",
    "        y_proba_nn.extend(probs.cpu().numpy())\n",
    "\n",
    "y_pred_nn = np.array(y_pred_nn)\n",
    "y_proba_nn = np.array(y_proba_nn)\n",
    "\n",
    "# è©•ä¾¡\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "auc_nn = roc_auc_score(y_test, y_proba_nn)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Neural Networkã®æ€§èƒ½ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_nn:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_nn:.4f}\")\n",
    "print(\"\\næ··åŒè¡Œåˆ—:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n",
    "print(\"\\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ:\")\n",
    "print(classification_report(y_test, y_pred_nn, \n",
    "                           target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 4. SHAP ã«ã‚ˆã‚‹è§£é‡ˆå¯èƒ½æ€§\n",
    "\n",
    "### SHAPã¨ã¯\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** ã¯ã€ã‚²ãƒ¼ãƒ ç†è«–ã«åŸºã¥ãæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ‰‹æ³•ã§ã™ã€‚\n",
    "\n",
    "### SHAPã®ç‰¹å¾´\n",
    "\n",
    "1. **ãƒ¢ãƒ‡ãƒ«éä¾å­˜**\n",
    "   - ã‚ã‚‰ã‚†ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨å¯èƒ½\n",
    "   - ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€GBDTã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ãªã©\n",
    "\n",
    "2. **ç†è«–çš„åŸºç›¤**\n",
    "   - Shapleyå€¤ï¼ˆã‚²ãƒ¼ãƒ ç†è«–ï¼‰ã«åŸºã¥ã\n",
    "   - æ•°å­¦çš„ã«æ­£å½“åŒ–ã•ã‚ŒãŸé‡è¦åº¦\n",
    "\n",
    "3. **ã‚°ãƒ­ãƒ¼ãƒãƒ«ã¨ãƒ­ãƒ¼ã‚«ãƒ«ã®è§£é‡ˆ**\n",
    "   - ã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼šå…¨ä½“çš„ãªç‰¹å¾´é‡é‡è¦åº¦\n",
    "   - ãƒ­ãƒ¼ã‚«ãƒ«ï¼šå€‹åˆ¥äºˆæ¸¬ã®èª¬æ˜\n",
    "\n",
    "4. **å¯è¦–åŒ–**\n",
    "   - Summary plotã€Force plotã€Dependence plotãªã©\n",
    "   - ç›´æ„Ÿçš„ãªç†è§£ãŒå¯èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAPã®æº–å‚™: ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ©ãƒƒãƒ—\n",
    "def model_predict(X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        outputs = model(X_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "# SHAP Explainerã®ä½œæˆï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™ã—ã¦é«˜é€ŸåŒ–ï¼‰\n",
    "background = shap.sample(X_train_scaled, 100)  # èƒŒæ™¯ãƒ‡ãƒ¼ã‚¿\n",
    "explainer = shap.KernelExplainer(model_predict, background)\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã§SHAPå€¤ã‚’è¨ˆç®—\n",
    "test_sample = X_test_scaled[:50]  # æœ€åˆã®50ã‚µãƒ³ãƒ—ãƒ«\n",
    "shap_values = explainer.shap_values(test_sample, nsamples=100)\n",
    "\n",
    "print(\"SHAPå€¤ã®è¨ˆç®—å®Œäº†\")\n",
    "print(f\"SHAPå€¤ã®å½¢çŠ¶: {np.array(shap_values).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 4.1 ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡é‡è¦åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot: ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡é‡è¦åº¦\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values[1], test_sample, feature_names=X.columns, show=False)\n",
    "plt.title('SHAP Summary Plot - Global Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ SHAP Summary Plot:\")\n",
    "print(\"- ç¸¦è»¸: ç‰¹å¾´é‡ï¼ˆé‡è¦åº¦é †ï¼‰\")\n",
    "print(\"- æ¨ªè»¸: SHAPå€¤ï¼ˆäºˆæ¸¬ã¸ã®å½±éŸ¿ï¼‰\")\n",
    "print(\"- è‰²: ç‰¹å¾´é‡ã®å€¤ï¼ˆèµ¤=é«˜ã€é’=ä½ï¼‰\")\n",
    "print(\"- å„ç‚¹: 1ã‚µãƒ³ãƒ—ãƒ«\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot: å¹³å‡çµ¶å¯¾SHAPå€¤\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values[1], test_sample, feature_names=X.columns, plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance (Mean |SHAP value|)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top 10ç‰¹å¾´é‡ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¤º\n",
    "mean_shap = np.abs(shap_values[1]).mean(axis=0)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': mean_shap\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 é‡è¦ãªç‰¹å¾´é‡:\")\n",
    "for i, row in importance_df.head(10).iterrows():\n",
    "    print(f\"{row['feature']:30s}: {row['importance']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### 4.2 å€‹åˆ¥äºˆæ¸¬ã®èª¬æ˜ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®SHAPå€¤ã‚’å¯è¦–åŒ–ï¼ˆWaterfall plotï¼‰\n",
    "sample_idx = 0\n",
    "\n",
    "# SHAPå€¤ã‚’Explanationã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›\n",
    "explanation = shap.Explanation(\n",
    "    values=shap_values[1][sample_idx],\n",
    "    base_values=explainer.expected_value[1],\n",
    "    data=test_sample[sample_idx],\n",
    "    feature_names=X.columns.tolist()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.plots.waterfall(explanation, show=False)\n",
    "plt.title(f'SHAP Waterfall Plot - Sample {sample_idx}\\n(True: {y_test.iloc[sample_idx]}, Pred: {y_pred_nn[sample_idx]})',\n",
    "         fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Waterfall Plot:\")\n",
    "print(\"- åŸºæº–å€¤ï¼ˆE[f(x)]ï¼‰ã‹ã‚‰äºˆæ¸¬å€¤ï¼ˆf(x)ï¼‰ã¸ã®å¤‰åŒ–ã‚’è¡¨ç¤º\")\n",
    "print(\"- å„ç‰¹å¾´é‡ãŒã©ã®ç¨‹åº¦äºˆæ¸¬ã«å¯„ä¸ã—ãŸã‹ã‚’å¯è¦–åŒ–\")\n",
    "print(\"- èµ¤: äºˆæ¸¬å€¤ã‚’å¢—åŠ ã•ã›ã‚‹æ–¹å‘\")\n",
    "print(\"- é’: äºˆæ¸¬å€¤ã‚’æ¸›å°‘ã•ã›ã‚‹æ–¹å‘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã®æ¯”è¼ƒ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(4):\n",
    "    sample_idx = i * 10  # 0, 10, 20, 30ç•ªç›®ã®ã‚µãƒ³ãƒ—ãƒ«\n",
    "    \n",
    "    # SHAPå€¤ã‚’å–å¾—ã—ã¦ã‚½ãƒ¼ãƒˆ\n",
    "    shap_vals = shap_values[1][sample_idx]\n",
    "    top_idx = np.argsort(np.abs(shap_vals))[-10:][::-1]\n",
    "    \n",
    "    top_features = X.columns[top_idx]\n",
    "    top_shap = shap_vals[top_idx]\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    colors = ['red' if val > 0 else 'blue' for val in top_shap]\n",
    "    axes[i].barh(range(len(top_shap)), top_shap, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_yticks(range(len(top_shap)))\n",
    "    axes[i].set_yticklabels(top_features, fontsize=9)\n",
    "    axes[i].set_xlabel('SHAP Value', fontsize=10)\n",
    "    axes[i].set_title(f'Sample {sample_idx} (True: {y_test.iloc[sample_idx]}, Pred: {y_pred_nn[sample_idx]})',\n",
    "                     fontsize=11, fontweight='bold')\n",
    "    axes[i].invert_yaxis()\n",
    "    axes[i].grid(axis='x', alpha=0.3)\n",
    "    axes[i].axvline(0, color='black', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®SHAPå€¤:\")\n",
    "print(\"- å„ã‚µãƒ³ãƒ—ãƒ«ã§ç•°ãªã‚‹ç‰¹å¾´é‡ãŒé‡è¦\")\n",
    "print(\"- ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ç‰¹å¾´ã«æ³¨ç›®ã—ã¦äºˆæ¸¬ã—ãŸã‹ã‚’ç†è§£\")\n",
    "print(\"- åŒ»ç™‚è¨ºæ–­ãªã©ã§é‡è¦ãªè§£é‡ˆå¯èƒ½æ€§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 5. GBDTã¨ã®æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "y_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "auc_lgb = roc_auc_score(y_test, y_proba_lgb)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LightGBMã®æ€§èƒ½ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_lgb:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_lgb:.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"XGBoostã®æ€§èƒ½ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½æ¯”è¼ƒ\n",
    "comparison_df = pd.DataFrame([\n",
    "    {'Model': 'Neural Network', 'Accuracy': accuracy_nn, 'ROC-AUC': auc_nn},\n",
    "    {'Model': 'LightGBM', 'Accuracy': accuracy_lgb, 'ROC-AUC': auc_lgb},\n",
    "    {'Model': 'XGBoost', 'Accuracy': accuracy_xgb, 'ROC-AUC': auc_xgb}\n",
    "]).sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "colors = ['steelblue', 'lightgreen', 'coral']\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Accuracy'], \n",
    "            alpha=0.7, edgecolor='black', color=colors)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[0].text(row.name, row['Accuracy'], f\"{row['Accuracy']:.4f}\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['ROC-AUC'], \n",
    "            alpha=0.7, edgecolor='black', color=colors)\n",
    "axes[1].set_ylabel('ROC-AUC', fontsize=11)\n",
    "axes[1].set_title('ROC-AUC Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[1].text(row.name, row['ROC-AUC'], f\"{row['ROC-AUC']:.4f}\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è¦³å¯Ÿ:\")\n",
    "print(\"- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯GBDTã¨åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®æ€§èƒ½\")\n",
    "print(\"- è§£é‡ˆå¯èƒ½æ€§ã§ã¯SHAPã«ã‚ˆã‚Šè©³ç´°ãªåˆ†æãŒå¯èƒ½\")\n",
    "print(\"- å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯GBDTã‚‚ä¾ç„¶å¼·åŠ›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 6. å®Ÿå‹™ã§ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "\n",
    "### Tabular Deep Learningã‚’ä½¿ã†ã¹ãå ´é¢\n",
    "\n",
    "| çŠ¶æ³ | æ¨å¥¨ |\n",
    "|------|------|\n",
    "| è§£é‡ˆå¯èƒ½æ€§ãŒé‡è¦ | GBDT + SHAP ã¾ãŸã¯ NN + SHAP |\n",
    "| ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º > 10k | Tabular DLæ¤œè¨ |\n",
    "| GPUãŒä½¿ãˆã‚‹ | Tabular DL âœ… |\n",
    "| é«˜é€Ÿæ¨è«–ãŒå¿…è¦ | GBDT |\n",
    "| ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º < 5k | GBDT |\n",
    "| ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°å¤šæ•° | CatBoost |\n",
    "| åŸ‹ã‚è¾¼ã¿å­¦ç¿’ãŒå¿…è¦ | NN âœ… |\n",
    "\n",
    "### ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ\n",
    "\n",
    "```python\n",
    "# âœ… Tabular NNå®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ\n",
    "\n",
    "# 1. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
    "# - ç‰¹å¾´é‡ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãŸã‹ï¼Ÿï¼ˆStandardScaleræ¨å¥¨ï¼‰\n",
    "# - æ¬ æå€¤ã‚’å‡¦ç†ã—ãŸã‹ï¼Ÿ\n",
    "# - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸã‹ï¼Ÿ\n",
    "\n",
    "# 2. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "# - è¨“ç·´ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã«åˆ†å‰²ã—ãŸã‹ï¼Ÿ\n",
    "# - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§Early stoppingã‚’ä½¿ã†ã‹?\n",
    "\n",
    "# 3. ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n",
    "# - Batch Normalizationã‚’ä½¿ç”¨ã—ãŸã‹ï¼Ÿ\n",
    "# - Dropoutã§éå­¦ç¿’ã‚’é˜²ã„ã ã‹ï¼Ÿ\n",
    "# - å±¤ã®æ·±ã•ã¨å¹…ã¯é©åˆ‡ã‹ï¼Ÿï¼ˆ2-4å±¤æ¨å¥¨ï¼‰\n",
    "\n",
    "# 4. å­¦ç¿’\n",
    "# - Early stoppingã‚’è¨­å®šã—ãŸã‹ï¼Ÿ\n",
    "# - å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½¿ç”¨ã—ãŸã‹ï¼Ÿ\n",
    "# - å­¦ç¿’æ›²ç·šã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ\n",
    "# - éå­¦ç¿’ã—ã¦ã„ãªã„ã‹ï¼Ÿ\n",
    "\n",
    "# 5. è©•ä¾¡\n",
    "# - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½ã‚’è©•ä¾¡ã—ãŸã‹ï¼Ÿ\n",
    "# - GBDTã¨æ¯”è¼ƒã—ãŸã‹ï¼Ÿ\n",
    "# - SHAPã§è§£é‡ˆå¯èƒ½æ€§ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ\n",
    "```\n",
    "\n",
    "### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "1. **å±¤ã®æ·±ã•ã¨å¹…**\n",
    "   - æ·±ã™ãã‚‹ã¨éå­¦ç¿’ã®ãƒªã‚¹ã‚¯\n",
    "   - é€šå¸¸2-4å±¤ã§ååˆ†\n",
    "   - å¹…: 64, 128, 256ãªã©\n",
    "\n",
    "2. **Dropoutç‡**\n",
    "   - 0.2 - 0.5ãŒä¸€èˆ¬çš„\n",
    "   - éå­¦ç¿’ã®å…†å€™ãŒã‚ã‚Œã°å¢—ã‚„ã™\n",
    "\n",
    "3. **å­¦ç¿’ç‡**\n",
    "   - Adam: 1e-3 ~ 1e-4\n",
    "   - ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã§å‹•çš„ã«èª¿æ•´\n",
    "\n",
    "4. **ãƒãƒƒãƒã‚µã‚¤ã‚º**\n",
    "   - å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿: 32-64\n",
    "   - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿: 128-256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 7. ã¾ã¨ã‚\n",
    "\n",
    "### æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å­¦ã‚“ã ã“ã¨\n",
    "\n",
    "1. **Tabularãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®å¿…è¦æ€§**\n",
    "   - GBDTã¨ã®æ¯”è¼ƒ\n",
    "   - ä½¿ã„åˆ†ã‘ã®åŸºæº–\n",
    "\n",
    "2. **PyTorchã§ã®Tabular NNå®Ÿè£…**\n",
    "   - Batch Normalization\n",
    "   - Dropout\n",
    "   - é©åˆ‡ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ\n",
    "\n",
    "3. **å­¦ç¿’ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯**\n",
    "   - Early stopping\n",
    "   - å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "   - å­¦ç¿’æ›²ç·šã®ç›£è¦–\n",
    "\n",
    "4. **SHAPã«ã‚ˆã‚‹è§£é‡ˆå¯èƒ½æ€§**\n",
    "   - ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡é‡è¦åº¦\n",
    "   - ãƒ­ãƒ¼ã‚«ãƒ«è§£é‡ˆï¼ˆå€‹åˆ¥äºˆæ¸¬ã®èª¬æ˜ï¼‰\n",
    "   - æ§˜ã€…ãªå¯è¦–åŒ–æ‰‹æ³•\n",
    "\n",
    "5. **GBDTã¨ã®æ¯”è¼ƒ**\n",
    "   - æ€§èƒ½ã®æ¯”è¼ƒ\n",
    "   - ãã‚Œãã‚Œã®å¼·ã¿\n",
    "   - ä½¿ã„åˆ†ã‘ã®åˆ¤æ–­åŸºæº–\n",
    "\n",
    "### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "- âœ… **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¿…é ˆ**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ç‰¹å¾´é‡ã®æ­£è¦åŒ–ãŒå¿…è¦\n",
    "- âœ… **è§£é‡ˆå¯èƒ½æ€§**: SHAPã§é€æ˜æ€§ã‚’ç¢ºä¿\n",
    "- âœ… **Early stopping**: éå­¦ç¿’ã‚’é˜²ã\n",
    "- âœ… **GBDTã¨æ¯”è¼ƒ**: ã¾ãšGBDTã‚’è©¦ã—ã€NNã§æ”¹å–„ã‚’å›³ã‚‹\n",
    "- âœ… **é©åˆ‡ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: Batch Norm + Dropout + 2-4å±¤\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "- Notebook 27ã§Kaggleå®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å­¦ã¶\n",
    "- Notebook 28ã§ç·åˆæ¼”ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«æŒ‘æˆ¦\n",
    "- å®Ÿéš›ã®Kaggleã‚³ãƒ³ãƒšã§Tabular NNã‚’è©¦ã™\n",
    "- FT-Transformerã€SAINTãªã©ä»–ã®Tabular DLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ã¶\n",
    "- ã‚ˆã‚Šè¤‡é›‘ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆResNetã€DenseNeté¢¨ï¼‰ã‚’è©¦ã™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
