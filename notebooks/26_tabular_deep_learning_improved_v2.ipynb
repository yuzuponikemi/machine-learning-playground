{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. Tabularãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚° - TabNet & NODE (Tabular Deep Learning)\n",
    "\n",
    "## æ¦‚è¦\n",
    "ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ç‰¹åŒ–ã—ãŸãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚TabNetã€NODEãªã©ã€GBDTã«å¯¾æŠ—ã§ãã‚‹æœ€æ–°ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿè£…ã—ã¾ã™ã€‚\n",
    "\n",
    "## å­¦ç¿’ç›®æ¨™\n",
    "- Tabularãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç‰¹å¾´ã‚’ç†è§£ã§ãã‚‹\n",
    "- TabNetã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ç‰¹å¾´ã‚’ç†è§£ã§ãã‚‹\n",
    "- TabNetã‚’å®Ÿè£…ãƒ»å­¦ç¿’ã§ãã‚‹\n",
    "- GBDTã¨ã®æ¯”è¼ƒãŒã§ãã‚‹\n",
    "- å®Ÿå‹™ã§Tabular DLã‚’é©ç”¨ã§ãã‚‹åˆ¤æ–­ãŒã§ãã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# TabNet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®š\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ãªãœTabularãƒ‡ãƒ¼ã‚¿ã«ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼Ÿ\n",
    "\n",
    "### Tabularãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´\n",
    "\n",
    "**Tabularãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰**ã¯ã€è¡Œã¨åˆ—ã§æ§‹æˆã•ã‚ŒãŸæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚\n",
    "\n",
    "### å¾“æ¥ã®å¸¸è­˜\n",
    "\n",
    "é•·å¹´ã€Tabularãƒ‡ãƒ¼ã‚¿ã§ã¯**GBDTï¼ˆGradient Boosting Decision Treesï¼‰ãŒæœ€å¼·**ã¨ã•ã‚Œã¦ãã¾ã—ãŸï¼š\n",
    "\n",
    "- âœ… XGBoostã€LightGBMã€CatBoost\n",
    "- âœ… Kaggleã‚³ãƒ³ãƒšã§åœ§å€’çš„ãªå‹ç‡\n",
    "- âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸è¦\n",
    "- âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒæ¯”è¼ƒçš„å®¹æ˜“\n",
    "\n",
    "### ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®èª²é¡Œ\n",
    "\n",
    "ä¸€æ–¹ã€å¾“æ¥ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯Tabularãƒ‡ãƒ¼ã‚¿ã§è‹¦æˆ¦ï¼š\n",
    "\n",
    "- âŒ GBDTã‚ˆã‚Šæ€§èƒ½ãŒä½ã„\n",
    "- âŒ å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦\n",
    "- âŒ ç‰¹å¾´é‡ã®å‰å‡¦ç†ãŒå¿…é ˆï¼ˆæ­£è¦åŒ–ãªã©ï¼‰\n",
    "- âŒ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒé›£ã—ã„\n",
    "\n",
    "### æ–°ä¸–ä»£ã®Tabular Deep Learning\n",
    "\n",
    "è¿‘å¹´ã€Tabularãƒ‡ãƒ¼ã‚¿å°‚ç”¨ã«è¨­è¨ˆã•ã‚ŒãŸDLãƒ¢ãƒ‡ãƒ«ãŒç™»å ´ï¼š\n",
    "\n",
    "1. **TabNet** (Google Research, 2019)\n",
    "   - è§£é‡ˆå¯èƒ½æ€§ã‚’æŒã¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "   - Sequential Attentionæ©Ÿæ§‹\n",
    "   - Self-supervisedäº‹å‰å­¦ç¿’\n",
    "\n",
    "2. **NODE** (Neural Oblivious Decision Ensembles, 2019)\n",
    "   - æ±ºå®šæœ¨ã¨NNã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰\n",
    "   - Oblivious Decision Trees\n",
    "\n",
    "3. **SAINT** (Self-Attention and Intersample Attention Transformer, 2021)\n",
    "   - Transformer for Tabular Data\n",
    "\n",
    "4. **FT-Transformer** (Feature Tokenizer Transformer, 2021)\n",
    "   - å„ç‰¹å¾´é‡ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "\n",
    "### ã„ã¤Tabular DLã‚’ä½¿ã†ã¹ãã‹\n",
    "\n",
    "| æ¡ä»¶ | æ¨å¥¨ |\n",
    "|------|------|\n",
    "| ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º < 10k | GBDT |\n",
    "| ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º > 100k | Tabular DLæ¤œè¨ |\n",
    "| è§£é‡ˆå¯èƒ½æ€§ãŒé‡è¦ | TabNet ã¾ãŸã¯ GBDT |\n",
    "| é«˜é€Ÿæ¨è«–ãŒå¿…è¦ | GBDT |\n",
    "| GPUãŒä½¿ãˆã‚‹ | Tabular DL |\n",
    "| äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š | TabNet |\n",
    "| ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°å¤šã„ | CatBoost ã¾ãŸã¯ TabNet |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ï¼ˆä¹³ãŒã‚“è¨ºæ–­ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X.shape}\")\n",
    "print(f\"ç‰¹å¾´é‡æ•°: {X.shape[1]}\")\n",
    "print(f\"ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}\")\n",
    "print(f\"\\nã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nç‰¹å¾´é‡ã®ä¾‹:\")\n",
    "print(X.head())\n",
    "print(f\"\\nçµ±è¨ˆé‡:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ã•ã‚‰ã«æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}\")\n",
    "print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {X_val.shape}\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}\")\n",
    "\n",
    "# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯å¿…é ˆï¼‰\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nâš ï¸ é‡è¦:\")\n",
    "print(\"- ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå¿…é ˆ\")\n",
    "print(\"- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§fitã—ã€æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯transformã®ã¿\")\n",
    "print(\"- GBDTã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸è¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TabNet - è§£é‡ˆå¯èƒ½ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "\n",
    "### TabNetã¨ã¯\n",
    "\n",
    "**TabNet**ã¯ã€Google ResearchãŒ2019å¹´ã«ç™ºè¡¨ã—ãŸTabularãƒ‡ãƒ¼ã‚¿å°‚ç”¨ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
    "\n",
    "### TabNetã®ç‰¹å¾´\n",
    "\n",
    "1. **Sequential Attentionæ©Ÿæ§‹**\n",
    "   - å„ã‚¹ãƒ†ãƒƒãƒ—ã§é‡è¦ãªç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "   - æ±ºå®šæœ¨ã®ã‚ˆã†ãªé€æ¬¡çš„ãªç‰¹å¾´é¸æŠ\n",
    "\n",
    "2. **è§£é‡ˆå¯èƒ½æ€§**\n",
    "   - Feature Importanceï¼ˆç‰¹å¾´é‡é‡è¦åº¦ï¼‰ã‚’å‡ºåŠ›\n",
    "   - Attention Maskã§é‡è¦ãªç‰¹å¾´ã‚’å¯è¦–åŒ–\n",
    "\n",
    "3. **Self-supervisedäº‹å‰å­¦ç¿’**\n",
    "   - ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’å¯èƒ½\n",
    "   - å°‘é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜æ€§èƒ½\n",
    "\n",
    "4. **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å­¦ç¿’**\n",
    "   - ç‰¹å¾´é‡é¸æŠã¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’åŒæ™‚ã«\n",
    "   - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãŒä¸è¦\n",
    "\n",
    "### TabNetã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n",
    "\n",
    "```\n",
    "Input Features\n",
    "     â†“\n",
    "Feature Transformer (Shared across steps)\n",
    "     â†“\n",
    "[Step 1] Attentive Transformer â†’ Feature Selection Mask\n",
    "     â†“\n",
    "[Step 2] Attentive Transformer â†’ Feature Selection Mask\n",
    "     â†“\n",
    "   ...\n",
    "     â†“\n",
    "[Step N] Attentive Transformer â†’ Feature Selection Mask\n",
    "     â†“\n",
    "Aggregate Decision\n",
    "     â†“\n",
    "Output (Classification/Regression)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 TabNetã®åŸºæœ¬çš„ãªä½¿ã„æ–¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNetClassifierã®åˆæœŸåŒ–\n",
    "tabnet_model = TabNetClassifier(\n",
    "    n_d=64,                    # Decisionå±¤ã®æ¬¡å…ƒæ•°\n",
    "    n_a=64,                    # Attentionå±¤ã®æ¬¡å…ƒæ•°\n",
    "    n_steps=5,                 # Sequential Attentionã®ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "    gamma=1.5,                 # Feature reusageã®ãƒšãƒŠãƒ«ãƒ†ã‚£\n",
    "    lambda_sparse=1e-4,        # ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã®æ­£å‰‡åŒ–\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\n",
    "        \"step_size\":50,\n",
    "        \"gamma\":0.9\n",
    "    },\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',        # Attention maskã®ã‚¿ã‚¤ãƒ—\n",
    "    verbose=0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"TabNetãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº†\")\n",
    "print(\"\\nãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\")\n",
    "print(f\"- n_d (Decisionå±¤): {tabnet_model.n_d}\")\n",
    "print(f\"- n_a (Attentionå±¤): {tabnet_model.n_a}\")\n",
    "print(f\"- n_steps (ã‚¹ãƒ†ãƒƒãƒ—æ•°): {tabnet_model.n_steps}\")\n",
    "print(f\"- gamma (Feature reusage penalty): {tabnet_model.gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\ntabnet_model.fit(\n    X_train=X_train_scaled,\n    y_train=y_train.values,\n    eval_set=[(X_val_scaled, y_val.values)],\n    eval_name=['valid'],\n    eval_metric=['auc', 'accuracy'],\n    max_epochs=100,\n    patience=20,               # Early stoppingã®å¿è€åº¦\n    batch_size=256,\n    virtual_batch_size=128,    # Ghost Batch Normalizationã®ã‚µã‚¤ã‚º\n    num_workers=0,\n    drop_last=False\n)\n\nprint(\"\\nTabNetã®å­¦ç¿’å®Œäº†\")\nprint(f\"æœ€è‰¯ã‚¨ãƒãƒƒã‚¯: {tabnet_model.best_epoch}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äºˆæ¸¬\n",
    "y_pred_tabnet = tabnet_model.predict(X_test_scaled)\n",
    "y_proba_tabnet = tabnet_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# è©•ä¾¡\n",
    "accuracy_tabnet = accuracy_score(y_test, y_pred_tabnet)\n",
    "auc_tabnet = roc_auc_score(y_test, y_proba_tabnet)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TabNetã®æ€§èƒ½ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_tabnet:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_tabnet:.4f}\")\n",
    "print(\"\\næ··åŒè¡Œåˆ—:\")\n",
    "print(confusion_matrix(y_test, y_pred_tabnet))\n",
    "print(\"\\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ:\")\n",
    "print(classification_report(y_test, y_pred_tabnet, \n",
    "                           target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TabNetã®å­¦ç¿’æ›²ç·š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Lossæ›²ç·š\n",
    "train_losses = [tabnet_model.history['loss'][i] for i in range(len(tabnet_model.history['loss']))]\n",
    "val_losses = [tabnet_model.history['valid_loss'][i] for i in range(len(tabnet_model.history['valid_loss']))]\n",
    "\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "axes[0].axvline(tabnet_model.best_epoch, color='red', linestyle='--', \n",
    "                label=f'Best Epoch ({tabnet_model.best_epoch})', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# AUCæ›²ç·š\n",
    "val_aucs = [tabnet_model.history['valid_auc'][i] for i in range(len(tabnet_model.history['valid_auc']))]\n",
    "\n",
    "axes[1].plot(val_aucs, label='Validation AUC', linewidth=2, color='orange')\n",
    "axes[1].axvline(tabnet_model.best_epoch, color='red', linestyle='--', \n",
    "                label=f'Best Epoch ({tabnet_model.best_epoch})', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('AUC', fontsize=11)\n",
    "axes[1].set_title('Validation AUC', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è¦³å¯Ÿ:\")\n",
    "print(f\"- æœ€è‰¯ã‚¨ãƒãƒƒã‚¯: {tabnet_model.best_epoch}\")\n",
    "print(f\"- Early stoppingã«ã‚ˆã‚Šéå­¦ç¿’ã‚’é˜²æ­¢\")\n",
    "print(f\"- æ¤œè¨¼AUCã¯ {max(val_aucs):.4f} ã«åˆ°é”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 TabNetã®ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆè§£é‡ˆå¯èƒ½æ€§ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—\n",
    "feature_importances = tabnet_model.feature_importances_\n",
    "\n",
    "# DataFrameã«æ•´å½¢\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# ä¸Šä½15ç‰¹å¾´é‡ã‚’å¯è¦–åŒ–\n",
    "top_features = importance_df.head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(top_features)), top_features['importance'].values, \n",
    "         alpha=0.7, edgecolor='black', color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "plt.xlabel('Feature Importance', fontsize=11)\n",
    "plt.title('TabNet Feature Importances (Top 15)', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 é‡è¦ãªç‰¹å¾´é‡:\")\n",
    "for i, row in importance_df.head(10).iterrows():\n",
    "    print(f\"{row['feature']:30s}: {row['importance']:.6f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ TabNetã®ç‰¹å¾´é‡é‡è¦åº¦:\")\n",
    "print(\"- Attentionæ©Ÿæ§‹ã«åŸºã¥ãé‡è¦åº¦\")\n",
    "print(\"- ã©ã®ç‰¹å¾´é‡ãŒãƒ¢ãƒ‡ãƒ«ã®æ±ºå®šã«å¯„ä¸ã—ãŸã‹ã‚’å¯è¦–åŒ–\")\n",
    "print(\"- GBDTã®ç‰¹å¾´é‡é‡è¦åº¦ã¨æ¯”è¼ƒå¯èƒ½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®Attention Maskã‚’å¯è¦–åŒ–\n",
    "# äºˆæ¸¬æ™‚ã®Attentionã‚’å–å¾—\n",
    "explain_matrix, masks = tabnet_model.explain(X_test_scaled[:10])\n",
    "\n",
    "# æœ€åˆã®5ã‚µãƒ³ãƒ—ãƒ«ã®Attention Maskã‚’å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for i in range(5):\n",
    "    # explain_matrixã¯å„ã‚µãƒ³ãƒ—ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦\n",
    "    importance_sample = explain_matrix[i]\n",
    "    \n",
    "    # ä¸Šä½10ç‰¹å¾´é‡ã®ã¿è¡¨ç¤º\n",
    "    top_idx = np.argsort(importance_sample)[-10:][::-1]\n",
    "    top_features_sample = X.columns[top_idx]\n",
    "    top_importance_sample = importance_sample[top_idx]\n",
    "    \n",
    "    axes[i].barh(range(len(top_importance_sample)), top_importance_sample, \n",
    "                alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_yticks(range(len(top_importance_sample)))\n",
    "    axes[i].set_yticklabels(top_features_sample, fontsize=8)\n",
    "    axes[i].set_xlabel('Importance', fontsize=9)\n",
    "    axes[i].set_title(f'Sample {i+1}\\n(True: {y_test.iloc[i]}, Pred: {y_pred_tabnet[i]})',\n",
    "                     fontsize=10, fontweight='bold')\n",
    "    axes[i].invert_yaxis()\n",
    "    axes[i].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®Attention:\")\n",
    "print(\"- å„ã‚µãƒ³ãƒ—ãƒ«ã§ç•°ãªã‚‹ç‰¹å¾´é‡ãŒé‡è¦\")\n",
    "print(\"- ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ç‰¹å¾´ã«æ³¨ç›®ã—ã¦äºˆæ¸¬ã—ãŸã‹ã‚’ç†è§£\")\n",
    "print(\"- åŒ»ç™‚è¨ºæ–­ãªã©ã§é‡è¦ãªè§£é‡ˆå¯èƒ½æ€§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GBDTã¨ã®æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "y_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "auc_lgb = roc_auc_score(y_test, y_proba_lgb)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LightGBMã®æ€§èƒ½ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_lgb:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_lgb:.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"XGBoostã®æ€§èƒ½ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½æ¯”è¼ƒ\n",
    "comparison_df = pd.DataFrame([\n",
    "    {'Model': 'TabNet', 'Accuracy': accuracy_tabnet, 'ROC-AUC': auc_tabnet},\n",
    "    {'Model': 'LightGBM', 'Accuracy': accuracy_lgb, 'ROC-AUC': auc_lgb},\n",
    "    {'Model': 'XGBoost', 'Accuracy': accuracy_xgb, 'ROC-AUC': auc_xgb}\n",
    "]).sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Accuracy'], \n",
    "            alpha=0.7, edgecolor='black', color=['steelblue', 'lightgreen', 'coral'])\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[0].text(row.name, row['Accuracy'], f\"{row['Accuracy']:.4f}\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['ROC-AUC'], \n",
    "            alpha=0.7, edgecolor='black', color=['steelblue', 'lightgreen', 'coral'])\n",
    "axes[1].set_ylabel('ROC-AUC', fontsize=11)\n",
    "axes[1].set_title('ROC-AUC Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[1].text(row.name, row['ROC-AUC'], f\"{row['ROC-AUC']:.4f}\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è¦³å¯Ÿ:\")\n",
    "print(\"- TabNetã¯GBDTã¨åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®æ€§èƒ½\")\n",
    "print(\"- è§£é‡ˆå¯èƒ½æ€§ã§ã¯TabNetãŒå„ªä½ï¼ˆAttentionå¯è¦–åŒ–ï¼‰\")\n",
    "print(\"- å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯GBDTã‚‚ä¾ç„¶å¼·åŠ›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TabNetã®äº‹å‰å­¦ç¿’ï¼ˆSelf-supervised Learningï¼‰\n",
    "\n",
    "TabNetã®å¼·åŠ›ãªæ©Ÿèƒ½ã®1ã¤ã¯ã€**Self-supervisedäº‹å‰å­¦ç¿’**ã§ã™ã€‚\n",
    "\n",
    "### ãªãœäº‹å‰å­¦ç¿’ãŒæœ‰åŠ¹ã‹\n",
    "\n",
    "- âœ… ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã§ãã‚‹\n",
    "- âœ… å°‘é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜æ€§èƒ½\n",
    "- âœ… ã‚ˆã‚Šè‰¯ã„ç‰¹å¾´è¡¨ç¾ã‚’å­¦ç¿’\n",
    "- âœ… éå­¦ç¿’ã‚’é˜²ãæ­£å‰‡åŒ–åŠ¹æœ\n",
    "\n",
    "### TabNetã®äº‹å‰å­¦ç¿’æ‰‹æ³•\n",
    "\n",
    "ä¸€éƒ¨ã®ç‰¹å¾´é‡ã‚’**ãƒã‚¹ã‚¯**ã—ã€æ®‹ã‚Šã®ç‰¹å¾´é‡ã‹ã‚‰å¾©å…ƒã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã—ã¾ã™ï¼ˆMasked Language Modelingã¨é¡ä¼¼ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã‚’æ¨¡æ“¬ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ™ãƒ«ã‚’é™¤å¤–ï¼‰\n",
    "X_unlabeled = np.concatenate([X_train_scaled, X_val_scaled, X_test_scaled], axis=0)\n",
    "\n",
    "print(f\"ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿: {X_unlabeled.shape}\")\n",
    "\n",
    "# äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\n",
    "pretrainer = TabNetPretrainer(\n",
    "    n_d=64,\n",
    "    n_a=64,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    lambda_sparse=1e-4,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax',\n",
    "    verbose=0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# äº‹å‰å­¦ç¿’ã®å®Ÿè¡Œ\n",
    "pretrainer.fit(\n",
    "    X_train=X_unlabeled,\n",
    "    eval_set=[X_unlabeled],\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    pretraining_ratio=0.5  # ãƒã‚¹ã‚¯ã™ã‚‹ç‰¹å¾´é‡ã®å‰²åˆ\n",
    ")\n",
    "\n",
    "print(\"\\näº‹å‰å­¦ç¿’å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§Fine-tuning\n",
    "tabnet_pretrained = TabNetClassifier(\n",
    "    n_d=64,\n",
    "    n_a=64,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    lambda_sparse=1e-4,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    verbose=0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# äº‹å‰å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "tabnet_pretrained.load_from_pretrained(pretrainer)\n",
    "\n",
    "# Fine-tuning\n",
    "tabnet_pretrained.fit(\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train.values,\n",
    "    eval_set=[(X_val_scaled, y_val.values)],\n",
    "    eval_name=['valid'],\n",
    "    eval_metric=['auc', 'accuracy'],\n",
    "    max_epochs=100,\n",
    "    patience=20,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# äºˆæ¸¬\n",
    "y_pred_pretrained = tabnet_pretrained.predict(X_test_scaled)\n",
    "y_proba_pretrained = tabnet_pretrained.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "accuracy_pretrained = accuracy_score(y_test, y_pred_pretrained)\n",
    "auc_pretrained = roc_auc_score(y_test, y_proba_pretrained)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"äº‹å‰å­¦ç¿’æ¸ˆã¿TabNetã®æ€§èƒ½\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_pretrained:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_pretrained:.4f}\")\n",
    "\n",
    "print(\"\\næ¯”è¼ƒ:\")\n",
    "print(f\"TabNet (äº‹å‰å­¦ç¿’ãªã—): AUC = {auc_tabnet:.4f}\")\n",
    "print(f\"TabNet (äº‹å‰å­¦ç¿’ã‚ã‚Š): AUC = {auc_pretrained:.4f}\")\n",
    "print(f\"æ”¹å–„: {(auc_pretrained - auc_tabnet):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TabNetã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "\n",
    "### ä¸»è¦ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "\n",
    "| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | èª¬æ˜ | æ¨å¥¨å€¤ |\n",
    "|-----------|------|--------|\n",
    "| `n_d` | Decisionå±¤ã®æ¬¡å…ƒæ•° | 8-64 |\n",
    "| `n_a` | Attentionå±¤ã®æ¬¡å…ƒæ•° | 8-64 |\n",
    "| `n_steps` | Sequential Attentionã®ã‚¹ãƒ†ãƒƒãƒ—æ•° | 3-10 |\n",
    "| `gamma` | Feature reusageã®ãƒšãƒŠãƒ«ãƒ†ã‚£ | 1.0-2.0 |\n",
    "| `lambda_sparse` | ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã®æ­£å‰‡åŒ– | 1e-6 ~ 1e-3 |\n",
    "| `learning_rate` | å­¦ç¿’ç‡ | 1e-2 ~ 5e-2 |\n",
    "| `batch_size` | ãƒãƒƒãƒã‚µã‚¤ã‚º | 256-1024 |\n",
    "| `virtual_batch_size` | Ghost Batch Norm | 128-256 |\n",
    "\n",
    "### ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "1. **n_d, n_a**: å¤§ãã„ã»ã©è¡¨ç¾åŠ›ãŒé«˜ã„ãŒã€éå­¦ç¿’ã®ãƒªã‚¹ã‚¯\n",
    "2. **n_steps**: ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒå¤šã„ã»ã©è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’\n",
    "3. **gamma**: é«˜ã„ã»ã©å„ã‚¹ãƒ†ãƒƒãƒ—ã§ç•°ãªã‚‹ç‰¹å¾´ã‚’ä½¿ã†ã‚ˆã†ã«å¼·åˆ¶\n",
    "4. **lambda_sparse**: ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’é«˜ã‚ã€è§£é‡ˆæ€§ã‚’å‘ä¸Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å®Ÿå‹™ã§ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "\n",
    "### TabNetã‚’ä½¿ã†ã¹ãå ´é¢\n",
    "\n",
    "| çŠ¶æ³ | æ¨å¥¨ |\n",
    "|------|------|\n",
    "| è§£é‡ˆå¯èƒ½æ€§ãŒé‡è¦ | TabNet âœ… |\n",
    "| ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º > 10k | TabNetæ¤œè¨ |\n",
    "| ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿å¤šæ•° | TabNetï¼ˆäº‹å‰å­¦ç¿’ï¼‰ âœ… |\n",
    "| GPUãŒä½¿ãˆã‚‹ | TabNet âœ… |\n",
    "| é«˜é€Ÿæ¨è«–ãŒå¿…è¦ | GBDT |\n",
    "| ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º < 5k | GBDT |\n",
    "| ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°å¤šæ•° | CatBoost ã¾ãŸã¯ TabNet |\n",
    "\n",
    "### ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ\n",
    "\n",
    "```python\n",
    "# âœ… TabNetå®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ\n",
    "\n",
    "# 1. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
    "# - ç‰¹å¾´é‡ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãŸã‹ï¼Ÿï¼ˆStandardScaleræ¨å¥¨ï¼‰\n",
    "# - æ¬ æå€¤ã‚’å‡¦ç†ã—ãŸã‹ï¼Ÿ\n",
    "# - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸã‹ï¼Ÿ\n",
    "\n",
    "# 2. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "# - è¨“ç·´ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã«åˆ†å‰²ã—ãŸã‹ï¼Ÿ\n",
    "# - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§Early stoppingã‚’ä½¿ã†ã‹?\n",
    "\n",
    "# 3. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "# - n_d, n_a, n_stepsã‚’èª¿æ•´ã—ãŸã‹ï¼Ÿ\n",
    "# - learning rateã‚’é©åˆ‡ã«è¨­å®šã—ãŸã‹ï¼Ÿ\n",
    "# - batch sizeã‚’èª¿æ•´ã—ãŸã‹ï¼Ÿ\n",
    "\n",
    "# 4. å­¦ç¿’\n",
    "# - Early stoppingã‚’è¨­å®šã—ãŸã‹ï¼Ÿ\n",
    "# - å­¦ç¿’æ›²ç·šã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ\n",
    "# - éå­¦ç¿’ã—ã¦ã„ãªã„ã‹ï¼Ÿ\n",
    "\n",
    "# 5. è©•ä¾¡\n",
    "# - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½ã‚’è©•ä¾¡ã—ãŸã‹ï¼Ÿ\n",
    "# - GBDTã¨æ¯”è¼ƒã—ãŸã‹ï¼Ÿ\n",
    "# - ç‰¹å¾´é‡é‡è¦åº¦ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ\n",
    "\n",
    "# 6. äº‹å‰å­¦ç¿’ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "# - ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ï¼Ÿ\n",
    "# - äº‹å‰å­¦ç¿’ã§æ€§èƒ½ãŒå‘ä¸Šã—ãŸã‹ï¼Ÿ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ã¾ã¨ã‚\n",
    "\n",
    "### æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å­¦ã‚“ã ã“ã¨\n",
    "\n",
    "1. **Tabularãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®å¿…è¦æ€§**\n",
    "   - GBDTã¨ã®æ¯”è¼ƒ\n",
    "   - æ–°ä¸–ä»£ã®Tabular DLãƒ¢ãƒ‡ãƒ«\n",
    "   - ä½¿ã„åˆ†ã‘ã®åŸºæº–\n",
    "\n",
    "2. **TabNetã®åŸºç¤**\n",
    "   - Sequential Attentionæ©Ÿæ§‹\n",
    "   - è§£é‡ˆå¯èƒ½æ€§ã®å®Ÿç¾\n",
    "   - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è§£\n",
    "\n",
    "3. **TabNetã®å®Ÿè£…**\n",
    "   - ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã¨å­¦ç¿’\n",
    "   - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n",
    "   - Early stopping\n",
    "\n",
    "4. **è§£é‡ˆå¯èƒ½æ€§**\n",
    "   - ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡é‡è¦åº¦\n",
    "   - ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®Attentionå¯è¦–åŒ–\n",
    "   - åŒ»ç™‚è¨ºæ–­ãªã©ã§ã®å¿œç”¨\n",
    "\n",
    "5. **äº‹å‰å­¦ç¿’**\n",
    "   - Self-supervised learning\n",
    "   - ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨\n",
    "   - Fine-tuningã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Š\n",
    "\n",
    "6. **GBDTã¨ã®æ¯”è¼ƒ**\n",
    "   - æ€§èƒ½ã®æ¯”è¼ƒ\n",
    "   - ãã‚Œãã‚Œã®å¼·ã¿\n",
    "   - ä½¿ã„åˆ†ã‘ã®åˆ¤æ–­åŸºæº–\n",
    "\n",
    "### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "- âœ… **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¿…é ˆ**: TabNetã¯ç‰¹å¾´é‡ã®æ­£è¦åŒ–ãŒå¿…è¦\n",
    "- âœ… **è§£é‡ˆå¯èƒ½æ€§**: Attentionå¯è¦–åŒ–ã§é€æ˜æ€§ã‚’ç¢ºä¿\n",
    "- âœ… **äº‹å‰å­¦ç¿’**: ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°æ´»ç”¨\n",
    "- âœ… **Early stopping**: éå­¦ç¿’ã‚’é˜²ã\n",
    "- âœ… **GBDTã¨æ¯”è¼ƒ**: ã¾ãšGBDTã‚’è©¦ã—ã€TabNetã§æ”¹å–„ã‚’å›³ã‚‹\n",
    "- âœ… **GPUæ¨å¥¨**: å­¦ç¿’ã®é«˜é€ŸåŒ–ã«GPUãŒæœ‰åŠ¹\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "- Notebook 27ã§Kaggleå®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å­¦ã¶\n",
    "- Notebook 28ã§ç·åˆæ¼”ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«æŒ‘æˆ¦\n",
    "- å®Ÿéš›ã®Kaggleã‚³ãƒ³ãƒšã§TabNetã‚’è©¦ã™\n",
    "- FT-Transformerã€SAINTãªã©ä»–ã®Tabular DLãƒ¢ãƒ‡ãƒ«ã‚’å­¦ã¶"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}