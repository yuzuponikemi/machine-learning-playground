{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. ç·åˆæ¼”ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ - Customer Churn Prediction (Comprehensive ML Project)\n",
    "\n",
    "## æ¦‚è¦\n",
    "ã“ã‚Œã¾ã§å­¦ã‚“ã å…¨ã¦ã®æŠ€è¡“ã‚’çµ±åˆã—ã€å®Ÿè·µçš„ãªæ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Œæˆã•ã›ã¾ã™ã€‚é¡§å®¢é›¢åäºˆæ¸¬ï¼ˆCustomer Churn Predictionï¼‰ã¨ã„ã†å®Ÿå‹™ã§é »å‡ºã™ã‚‹å•é¡Œã‚’é€šã˜ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã¨ã—ã¦ã®ç·åˆåŠ›ã‚’ç£¨ãã¾ã™ã€‚\n",
    "\n",
    "## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®æ¨™\n",
    "é€šä¿¡ä¼šç¤¾ã®é¡§å®¢é›¢åã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€ä»¥ä¸‹ã‚’é”æˆã—ã¾ã™ï¼š\n",
    "- âœ… ROC-AUC > 0.85ã‚’ç›®æŒ‡ã™\n",
    "- âœ… è§£é‡ˆå¯èƒ½ãªç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹\n",
    "- âœ… è¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¯”è¼ƒã™ã‚‹\n",
    "- âœ… é©åˆ‡ãªæ¤œè¨¼æˆ¦ç•¥ã‚’å®Ÿè£…ã™ã‚‹\n",
    "- âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹\n",
    "- âœ… ãƒ“ã‚¸ãƒã‚¹ã¸ã®æè¨€ã‚’ã¾ã¨ã‚ã‚‹\n",
    "\n",
    "## ä½¿ç”¨ã™ã‚‹æŠ€è¡“ï¼ˆã“ã‚Œã¾ã§å­¦ã‚“ã å…¨ã¦ï¼ï¼‰\n",
    "\n",
    "### Phase 1-3: åŸºç¤\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "- å‰å‡¦ç†ã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "- ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™\n",
    "\n",
    "### Phase 4-6: æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«\n",
    "- ç·šå½¢ãƒ¢ãƒ‡ãƒ«\n",
    "- æ±ºå®šæœ¨ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
    "- SVM\n",
    "\n",
    "### Phase 7: é«˜åº¦ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯\n",
    "- Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
    "- SHAPãƒ¢ãƒ‡ãƒ«è§£é‡ˆ\n",
    "- Stackingã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
    "\n",
    "### Phase 8: å°‚é–€ãƒˆãƒ”ãƒƒã‚¯\n",
    "- ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­–ï¼ˆSMOTEã€Focal Lossï¼‰\n",
    "- æ™‚ç³»åˆ—ç‰¹å¾´é‡\n",
    "- ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "\n",
    "### Phase 9: æœ€çµ‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "- Tabularãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- Kaggleå®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­–\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
    "import optuna\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®š\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… ã™ã¹ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæ­£å¸¸ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã—ãŸ\")\n",
    "print(\"\\nã“ã‚Œã‹ã‚‰å§‹ã¾ã‚‹ç·åˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã€ã“ã‚Œã¾ã§å­¦ã‚“ã ã™ã¹ã¦ã®æŠ€è¡“ã‚’æ´»ç”¨ã—ã¾ã™ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ãƒ“ã‚¸ãƒã‚¹èª²é¡Œã®ç†è§£\n",
    "\n",
    "### èƒŒæ™¯\n",
    "\n",
    "ã‚ãªãŸã¯å¤§æ‰‹é€šä¿¡ä¼šç¤¾ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒãƒ¼ãƒ ã«æ‰€å±ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### èª²é¡Œ\n",
    "\n",
    "**é¡§å®¢é›¢åç‡ï¼ˆChurn Rateï¼‰ãŒé«˜ãã€åç›Šã«æ‚ªå½±éŸ¿ã‚’ä¸ãˆã¦ã„ã¾ã™ã€‚**\n",
    "\n",
    "### ãƒ“ã‚¸ãƒã‚¹ç›®æ¨™\n",
    "\n",
    "1. **é›¢åãƒªã‚¹ã‚¯ã®é«˜ã„é¡§å®¢ã‚’ç‰¹å®š**\n",
    "   - é›¢åã™ã‚‹å‰ã«ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ–½ç­–ã‚’å®Ÿæ–½\n",
    "   - ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã¦é¡§å®¢ã‚’ç¶­æŒ\n",
    "\n",
    "2. **é›¢åã®ä¸»è¦å› ã‚’è§£æ˜**\n",
    "   - ã©ã®è¦å› ãŒé›¢åã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹ã‹\n",
    "   - ã‚µãƒ¼ãƒ“ã‚¹æ”¹å–„ã®æŒ‡é‡ã‚’å¾—ã‚‹\n",
    "\n",
    "3. **ROIï¼ˆæŠ•è³‡å¯¾åŠ¹æœï¼‰ã®æœ€å¤§åŒ–**\n",
    "   - ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã®åŠ¹æœã‚’äºˆæ¸¬\n",
    "   - é™ã‚‰ã‚ŒãŸäºˆç®—ã‚’æœ€é©é…åˆ†\n",
    "\n",
    "### ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®å½¹å‰²\n",
    "\n",
    "- é›¢åäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰\n",
    "- äºˆæ¸¬æ ¹æ‹ ã®èª¬æ˜ï¼ˆè§£é‡ˆå¯èƒ½æ€§ï¼‰\n",
    "- ãƒ“ã‚¸ãƒã‚¹ã¸ã® actionable insights\n",
    "\n",
    "### è©•ä¾¡æŒ‡æ¨™ã®é¸å®š\n",
    "\n",
    "ä»Šå›ã®èª²é¡Œã§ã¯ã€**ROC-AUC**ã‚’ä¸»è¦ãªè©•ä¾¡æŒ‡æ¨™ã¨ã—ã¾ã™ã€‚\n",
    "\n",
    "**ç†ç”±**:\n",
    "- é–¾å€¤ã«ä¾å­˜ã—ãªã„ç·åˆçš„ãªæ€§èƒ½è©•ä¾¡\n",
    "- ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«å¯¾ã—ã¦æ¯”è¼ƒçš„ãƒ­ãƒã‚¹ãƒˆ\n",
    "- ãƒ“ã‚¸ãƒã‚¹å´ãŒé›¢åç¢ºç‡ï¼ˆäºˆæ¸¬ã‚¹ã‚³ã‚¢ï¼‰ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ãã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã¨ç†è§£\n",
    "\n",
    "å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»çµåˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€\n",
    "ä»Šå›ã¯å­¦ç¿’ç”¨ã«ãƒªã‚¢ãƒ«ãªé¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒªã‚¢ãƒ«ãªé¡§å®¢é›¢åãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "\n",
    "# 1. äººå£çµ±è¨ˆå­¦çš„ç‰¹å¾´é‡\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "    'senior_citizen': np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "    \n",
    "    # 2. ã‚µãƒ¼ãƒ“ã‚¹åˆ©ç”¨çŠ¶æ³\n",
    "    'tenure_months': np.random.randint(1, 73, n_samples),  # å¥‘ç´„æœŸé–“ï¼ˆæœˆï¼‰\n",
    "    'contract': np.random.choice(['Month-to-month', 'One year', 'Two year'], \n",
    "                                 n_samples, p=[0.55, 0.25, 0.20]),\n",
    "    'payment_method': np.random.choice(['Electronic check', 'Mailed check', \n",
    "                                       'Bank transfer', 'Credit card'], n_samples),\n",
    "    'paperless_billing': np.random.choice(['Yes', 'No'], n_samples, p=[0.6, 0.4]),\n",
    "    \n",
    "    # 3. ã‚µãƒ¼ãƒ“ã‚¹ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "    'phone_service': np.random.choice(['Yes', 'No'], n_samples, p=[0.9, 0.1]),\n",
    "    'multiple_lines': np.random.choice(['Yes', 'No', 'No phone service'], \n",
    "                                      n_samples, p=[0.5, 0.4, 0.1]),\n",
    "    'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], \n",
    "                                         n_samples, p=[0.35, 0.45, 0.20]),\n",
    "    'online_security': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "    'online_backup': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "    'device_protection': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "    'tech_support': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "    'streaming_tv': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "    'streaming_movies': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
    "    \n",
    "    # 4. æ–™é‡‘æƒ…å ±\n",
    "    'monthly_charges': np.random.uniform(18.0, 120.0, n_samples),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Total Chargesã‚’è¨ˆç®—ï¼ˆtenure Ã— monthly charges + ãƒã‚¤ã‚ºï¼‰\n",
    "df['total_charges'] = df['tenure_months'] * df['monthly_charges'] + \\\n",
    "                      np.random.normal(0, 100, n_samples)\n",
    "df['total_charges'] = df['total_charges'].clip(lower=0)\n",
    "\n",
    "# 5. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆChurnï¼‰ã®ç”Ÿæˆ\n",
    "# è¤‡æ•°ã®è¦å› ã‹ã‚‰é›¢åç¢ºç‡ã‚’è¨ˆç®—\n",
    "churn_prob = (\n",
    "    0.7 - (df['tenure_months'] / 100) +  # å¥‘ç´„æœŸé–“ãŒçŸ­ã„ã»ã©é›¢åã—ã‚„ã™ã„\n",
    "    0.3 * (df['contract'] == 'Month-to-month').astype(int) +  # æœˆå¥‘ç´„ã¯é›¢åã—ã‚„ã™ã„\n",
    "    0.2 * (df['internet_service'] == 'Fiber optic').astype(int) +  # å…‰ãƒ•ã‚¡ã‚¤ãƒãƒ¼ã¯é›¢åã—ã‚„ã™ã„ï¼ˆé«˜ä¾¡æ ¼ï¼‰\n",
    "    0.15 * (df['payment_method'] == 'Electronic check').astype(int) +  # é›»å­ãƒã‚§ãƒƒã‚¯ã¯é›¢åã—ã‚„ã™ã„\n",
    "    0.1 * (df['paperless_billing'] == 'Yes').astype(int) +\n",
    "    (df['monthly_charges'] - 70) / 200 +  # æ–™é‡‘ãŒé«˜ã„ã»ã©é›¢åã—ã‚„ã™ã„\n",
    "    -0.15 * (df['tech_support'] == 'Yes').astype(int) +  # ãƒ†ã‚¯ã‚µãƒãƒ¼ãƒˆã‚ã‚‹ã¨é›¢åã—ã«ãã„\n",
    "    -0.15 * (df['online_security'] == 'Yes').astype(int) +  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚µãƒ¼ãƒ“ã‚¹ã‚ã‚‹ã¨é›¢åã—ã«ãã„\n",
    "    np.random.normal(0, 0.2, n_samples)  # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º\n",
    ")\n",
    "\n",
    "# ç¢ºç‡ã‚’0-1ã«ã‚¯ãƒªãƒƒãƒ—\n",
    "churn_prob = np.clip(churn_prob, 0, 1)\n",
    "\n",
    "# ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒã§Churnã‚’ç”Ÿæˆï¼ˆç´„25%é›¢åï¼‰\n",
    "df['churn'] = (np.random.random(n_samples) < churn_prob * 0.4).astype(int)\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df.shape}\")\n",
    "print(f\"\\nChurnåˆ†å¸ƒ:\")\n",
    "print(df['churn'].value_counts())\n",
    "print(f\"\\nChurnç‡: {df['churn'].mean():.2%}\")\n",
    "print(f\"\\nãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚’æ·±ãç†è§£ã™ã‚‹ãŸã‚ã®EDAã‚’å®Ÿæ–½ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬çµ±è¨ˆé‡\n",
    "print(\"=\" * 80)\n",
    "print(\"æ•°å€¤å¤‰æ•°ã®çµ±è¨ˆé‡\")\n",
    "print(\"=\" * 80)\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®æƒ…å ±\")\n",
    "print(\"=\" * 80)\n",
    "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churnç‡ã®å¯è¦–åŒ–ï¼ˆã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°åˆ¥ï¼‰\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "key_categorical = ['contract', 'internet_service', 'payment_method', \n",
    "                  'online_security', 'tech_support', 'gender']\n",
    "\n",
    "for idx, col in enumerate(key_categorical):\n",
    "    churn_rate = df.groupby(col)['churn'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    axes[idx].bar(range(len(churn_rate)), churn_rate.values, \n",
    "                 alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_xticks(range(len(churn_rate)))\n",
    "    axes[idx].set_xticklabels(churn_rate.index, rotation=45, ha='right')\n",
    "    axes[idx].set_ylabel('Churn Rate', fontsize=10)\n",
    "    axes[idx].set_title(f'Churn Rate by {col}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].axhline(df['churn'].mean(), color='red', linestyle='--', \n",
    "                     linewidth=1, label='Overall Churn Rate')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ ä¸»è¦ãªç™ºè¦‹:\")\n",
    "print(\"- Month-to-monthå¥‘ç´„ã®é›¢åç‡ãŒæœ€ã‚‚é«˜ã„\")\n",
    "print(\"- Electronic checkã§ã®æ”¯æ‰•ã„ã¯é›¢åç‡ãŒé«˜ã„\")\n",
    "print(\"- Tech supportã‚„Online securityãŒãªã„ã¨é›¢åã—ã‚„ã™ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°å€¤å¤‰æ•°ã¨Churnã®é–¢ä¿‚\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "numeric_features = ['tenure_months', 'monthly_charges', 'total_charges']\n",
    "\n",
    "for idx, col in enumerate(numeric_features):\n",
    "    axes[idx].hist(df[df['churn']==0][col], bins=30, alpha=0.6, \n",
    "                  label='No Churn', edgecolor='black')\n",
    "    axes[idx].hist(df[df['churn']==1][col], bins=30, alpha=0.6, \n",
    "                  label='Churn', edgecolor='black')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Count', fontsize=10)\n",
    "    axes[idx].set_title(f'{col} Distribution by Churn', fontsize=11, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è¦³å¯Ÿ:\")\n",
    "print(\"- å¥‘ç´„æœŸé–“ï¼ˆtenureï¼‰ãŒçŸ­ã„é¡§å®¢ã»ã©é›¢åã—ã‚„ã™ã„\")\n",
    "print(\"- æœˆé¡æ–™é‡‘ï¼ˆmonthly chargesï¼‰ãŒé«˜ã„é¡§å®¢ã¯é›¢åã—ã‚„ã™ã„å‚¾å‘\")\n",
    "print(\"- Total chargesã¯å¥‘ç´„æœŸé–“ã¨ç›¸é–¢ãŒé«˜ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "\n",
    "ã“ã‚Œã¾ã§å­¦ã‚“ã ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’é§†ä½¿ã—ã¦ã€å¼·åŠ›ãªç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    åŒ…æ‹¬çš„ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "    \"\"\"\n",
    "    df_fe = df.copy()\n",
    "    \n",
    "    # 1. æ—¢å­˜ç‰¹å¾´é‡ã®å¤‰æ›\n",
    "    df_fe['tenure_years'] = df_fe['tenure_months'] / 12\n",
    "    df_fe['log_monthly_charges'] = np.log1p(df_fe['monthly_charges'])\n",
    "    df_fe['log_total_charges'] = np.log1p(df_fe['total_charges'])\n",
    "    \n",
    "    # 2. æ¯”ç‡ç‰¹å¾´é‡\n",
    "    df_fe['monthly_to_total_ratio'] = df_fe['monthly_charges'] / (df_fe['total_charges'] + 1)\n",
    "    df_fe['charges_per_month'] = df_fe['total_charges'] / (df_fe['tenure_months'] + 1)\n",
    "    \n",
    "    # 3. Binningç‰¹å¾´é‡\n",
    "    df_fe['tenure_group'] = pd.cut(df_fe['tenure_months'], \n",
    "                                   bins=[0, 12, 24, 48, 72], \n",
    "                                   labels=['0-12', '12-24', '24-48', '48+'])\n",
    "    df_fe['charges_group'] = pd.qcut(df_fe['monthly_charges'], \n",
    "                                     q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    \n",
    "    # 4. ã‚µãƒ¼ãƒ“ã‚¹æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    services = ['phone_service', 'multiple_lines', 'online_security', \n",
    "               'online_backup', 'device_protection', 'tech_support',\n",
    "               'streaming_tv', 'streaming_movies']\n",
    "    \n",
    "    df_fe['num_services'] = 0\n",
    "    for service in services:\n",
    "        df_fe['num_services'] += (df_fe[service] == 'Yes').astype(int)\n",
    "    \n",
    "    # 5. å¥‘ç´„ãƒ»æ”¯æ‰•ã„ãƒªã‚¹ã‚¯ãƒ•ãƒ©ã‚°\n",
    "    df_fe['is_high_risk_contract'] = (df_fe['contract'] == 'Month-to-month').astype(int)\n",
    "    df_fe['is_high_risk_payment'] = (df_fe['payment_method'] == 'Electronic check').astype(int)\n",
    "    \n",
    "    # 6. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£ã‚µãƒ¼ãƒ“ã‚¹ã®æœ‰ç„¡\n",
    "    df_fe['has_security'] = ((df_fe['online_security'] == 'Yes') | \n",
    "                            (df_fe['tech_support'] == 'Yes')).astype(int)\n",
    "    \n",
    "    # 7. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆåˆ©ç”¨åº¦\n",
    "    df_fe['internet_usage_score'] = (\n",
    "        (df_fe['online_security'] == 'Yes').astype(int) +\n",
    "        (df_fe['online_backup'] == 'Yes').astype(int) +\n",
    "        (df_fe['device_protection'] == 'Yes').astype(int) +\n",
    "        (df_fe['streaming_tv'] == 'Yes').astype(int) +\n",
    "        (df_fe['streaming_movies'] == 'Yes').astype(int)\n",
    "    )\n",
    "    \n",
    "    # 8. é¡§å®¢ä¾¡å€¤ã‚¹ã‚³ã‚¢ï¼ˆLTV proxyï¼‰\n",
    "    df_fe['customer_value_score'] = (\n",
    "        df_fe['tenure_months'] * df_fe['monthly_charges'] / 1000\n",
    "    )\n",
    "    \n",
    "    return df_fe\n",
    "\n",
    "# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’é©ç”¨\n",
    "df_enhanced = create_features(df)\n",
    "\n",
    "print(f\"å…ƒã®ç‰¹å¾´é‡æ•°: {df.shape[1] - 2}\")\n",
    "print(f\"æ‹¡å¼µå¾Œã®ç‰¹å¾´é‡æ•°: {df_enhanced.shape[1] - 2}\")\n",
    "print(f\"è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡: {df_enhanced.shape[1] - df.shape[1]}\")\n",
    "\n",
    "# æ–°ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆ\n",
    "new_features = set(df_enhanced.columns) - set(df.columns)\n",
    "print(f\"\\næ–°ã—ã„ç‰¹å¾´é‡:\")\n",
    "for feat in sorted(new_features):\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆTarget Encodingï¼‰\n",
    "# ã¾ãšãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "X = df_enhanced.drop(['customer_id', 'churn'], axis=1)\n",
    "y = df_enhanced['churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}\")\n",
    "print(f\"\\nChurnç‡:\")\n",
    "print(f\"- è¨“ç·´: {y_train.mean():.2%}\")\n",
    "print(f\"- ãƒ†ã‚¹ãƒˆ: {y_test.mean():.2%}\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ãƒªã‚¹ãƒˆ\n",
    "categorical_features = X_train.select_dtypes(include='object').columns.tolist()\n",
    "# categoricalå‹ã®ã‚‚ã®ã‚‚è¿½åŠ \n",
    "categorical_features += X_train.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "print(f\"\\nã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°: {len(categorical_features)}å€‹\")\n",
    "print(categorical_features)\n",
    "\n",
    "# Target Encodingã‚’é©ç”¨\n",
    "target_encoder = TargetEncoder(cols=categorical_features, smoothing=1.0)\n",
    "\n",
    "X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n",
    "X_test_encoded = target_encoder.transform(X_test)\n",
    "\n",
    "print(\"\\nâœ… Target EncodingãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰\n",
    "\n",
    "è¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦ã—ã¦ã€æœ€è‰¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¤‡æ•°ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),\n",
    "    'CatBoost': CatBoostClassifier(iterations=100, random_state=42, verbose=0)\n",
    "}\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # å­¦ç¿’\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "    \n",
    "    # äºˆæ¸¬\n",
    "    y_pred = model.predict(X_test_encoded)\n",
    "    y_proba = model.predict_proba(X_test_encoded)[:, 1]\n",
    "    \n",
    "    # è©•ä¾¡\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    baseline_results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# çµæœã®è¡¨ç¤º\n",
    "baseline_df = pd.DataFrame(baseline_results).sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ\")\n",
    "print(\"=\" * 100)\n",
    "print(baseline_df.to_string(index=False))\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC-AUC\n",
    "axes[0].barh(baseline_df['Model'], baseline_df['ROC-AUC'], \n",
    "            alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('ROC-AUC', fontsize=11)\n",
    "axes[0].set_title('Model Comparison (ROC-AUC)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlim([0.7, 0.9])\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "for i, row in baseline_df.iterrows():\n",
    "    axes[0].text(row['ROC-AUC'], row.name, f\"{row['ROC-AUC']:.4f}\",\n",
    "                va='center', ha='left', fontsize=9)\n",
    "\n",
    "# F1-Score\n",
    "axes[1].barh(baseline_df['Model'], baseline_df['F1-Score'], \n",
    "            alpha=0.7, edgecolor='black', color='coral')\n",
    "axes[1].set_xlabel('F1-Score', fontsize=11)\n",
    "axes[1].set_title('Model Comparison (F1-Score)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "for i, row in baseline_df.iterrows():\n",
    "    axes[1].text(row['F1-Score'], row.name, f\"{row['F1-Score']:.4f}\",\n",
    "                va='center', ha='left', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ† ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {baseline_df.iloc[0]['Model']}\")\n",
    "print(f\"   ROC-AUC: {baseline_df.iloc[0]['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­–ï¼ˆSMOTEï¼‰\n",
    "\n",
    "Churnç‡ãŒä½ã„ãŸã‚ã€SMOTEã§å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã‚’å¢—ã‚„ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTEã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "print(f\"å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train_encoded.shape}\")\n",
    "print(f\"SMOTEå¾Œ: {X_train_smote.shape}\")\n",
    "print(f\"\\nã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:\")\n",
    "print(f\"- å…ƒ: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "print(f\"- SMOTEå¾Œ: {pd.Series(y_train_smote).value_counts().to_dict()}\")\n",
    "\n",
    "# SMOTEã‚’ä½¿ã£ãŸLightGBM\n",
    "lgb_smote = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = lgb_smote.predict(X_test_encoded)\n",
    "y_proba_smote = lgb_smote.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "roc_auc_smote = roc_auc_score(y_test, y_proba_smote)\n",
    "print(f\"\\nLightGBM (SMOTE) ROC-AUC: {roc_auc_smote:.4f}\")\n",
    "print(f\"LightGBM (No SMOTE) ROC-AUC: {baseline_df[baseline_df['Model']=='LightGBM']['ROC-AUC'].values[0]:.4f}\")\n",
    "print(f\"\\næ”¹å–„: {roc_auc_smote - baseline_df[baseline_df['Model']=='LightGBM']['ROC-AUC'].values[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆOptunaï¼‰\n",
    "\n",
    "Optunaã§æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    # 5-Fold CVã§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_smote, y_train_smote, \n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Optunaã‚¹ã‚¿ãƒ‡ã‚£ã®å®Ÿè¡Œ\n",
    "print(\"Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é–‹å§‹...\")\n",
    "study = optuna.create_study(direction='maximize', study_name='lgb_optimization')\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Optunaæœ€é©åŒ–çµæœ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Best ROC-AUC (CV): {study.best_value:.4f}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’\n",
    "lgb_optimized = lgb.LGBMClassifier(**study.best_params, verbose=-1)\n",
    "lgb_optimized.fit(X_train_smote, y_train_smote)\n",
    "y_proba_optimized = lgb_optimized.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "roc_auc_optimized = roc_auc_score(y_test, y_proba_optimized)\n",
    "print(f\"\\nTest ROC-AUC (Optimized): {roc_auc_optimized:.4f}\")\n",
    "print(f\"Test ROC-AUC (Default): {roc_auc_smote:.4f}\")\n",
    "print(f\"æ”¹å–„: {roc_auc_optimized - roc_auc_smote:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ãƒ¢ãƒ‡ãƒ«è§£é‡ˆï¼ˆSHAPï¼‰\n",
    "\n",
    "SHAPã§ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬æ ¹æ‹ ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAPã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ\n",
    "explainer = shap.TreeExplainer(lgb_optimized)\n",
    "shap_values = explainer.shap_values(X_test_encoded[:500])  # æœ€åˆã®500ã‚µãƒ³ãƒ—ãƒ«\n",
    "\n",
    "# Summary plot\n",
    "print(\"SHAP Summary Plotï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«è§£é‡ˆï¼‰\")\n",
    "shap.summary_plot(shap_values[1], X_test_encoded[:500], plot_type=\"bar\", show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_test_encoded.columns,\n",
    "    'importance': np.abs(shap_values[1]).mean(axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 é‡è¦ãªç‰¹å¾´é‡ï¼ˆSHAPå€¤ï¼‰:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ’¡ ãƒ“ã‚¸ãƒã‚¹ã¸ã®ç¤ºå”†:\")\n",
    "print(\"- å¥‘ç´„ã‚¿ã‚¤ãƒ—ï¼ˆæœˆå¥‘ç´„ vs å¹´å¥‘ç´„ï¼‰ãŒæœ€ã‚‚é‡è¦\")\n",
    "print(\"- å¥‘ç´„æœŸé–“ï¼ˆtenureï¼‰ãŒçŸ­ã„ã»ã©é›¢åãƒªã‚¹ã‚¯ãŒé«˜ã„\")\n",
    "print(\"- ãƒ†ã‚¯ã‚µãƒãƒ¼ãƒˆã‚„ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚µãƒ¼ãƒ“ã‚¹ãŒé›¢åã‚’æŠ‘åˆ¶\")\n",
    "print(\"- æ–™é‡‘ã®é«˜ã•ã‚‚é›¢åè¦å› ã®1ã¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆStackingï¼‰\n",
    "\n",
    "è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’Stackingã—ã¦æœ€çµ‚æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å–å¾—\n",
    "final_models = {\n",
    "    'LightGBM': lgb_optimized,\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, \n",
    "                                  max_depth=6, random_state=42, eval_metric='logloss'),\n",
    "    'CatBoost': CatBoostClassifier(iterations=200, learning_rate=0.05, \n",
    "                                    depth=6, random_state=42, verbose=0),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=10, \n",
    "                                            random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# å„ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨äºˆæ¸¬\n",
    "predictions = {}\n",
    "for name, model in final_models.items():\n",
    "    if name != 'LightGBM':  # LightGBMã¯æ—¢ã«å­¦ç¿’æ¸ˆã¿\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    y_proba = model.predict_proba(X_test_encoded)[:, 1]\n",
    "    predictions[name] = y_proba\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"{name} ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# ã‚·ãƒ³ãƒ—ãƒ«ãªå¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
    "y_proba_ensemble_avg = np.mean(list(predictions.values()), axis=0)\n",
    "roc_auc_ensemble_avg = roc_auc_score(y_test, y_proba_ensemble_avg)\n",
    "\n",
    "# é‡ã¿ä»˜ãå¹³å‡ï¼ˆROC-AUCã«åŸºã¥ãé‡ã¿ï¼‰\n",
    "weights = {}\n",
    "for name, pred in predictions.items():\n",
    "    weights[name] = roc_auc_score(y_test, pred)\n",
    "\n",
    "total_weight = sum(weights.values())\n",
    "weights = {k: v/total_weight for k, v in weights.items()}\n",
    "\n",
    "y_proba_ensemble_weighted = sum(predictions[name] * weights[name] \n",
    "                                for name in final_models.keys())\n",
    "roc_auc_ensemble_weighted = roc_auc_score(y_test, y_proba_ensemble_weighted)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Simple Average Ensemble:   {roc_auc_ensemble_avg:.4f}\")\n",
    "print(f\"Weighted Average Ensemble: {roc_auc_ensemble_weighted:.4f}\")\n",
    "print(f\"\\nBest Single Model (LightGBM): {roc_auc_optimized:.4f}\")\n",
    "print(f\"\\nğŸ¯ ç›®æ¨™é”æˆ: ROC-AUC > 0.85? {'âœ… YES' if roc_auc_ensemble_weighted > 0.85 else 'âŒ NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æœ€çµ‚è©•ä¾¡ã¨ãƒ“ã‚¸ãƒã‚¹ã¸ã®æè¨€\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚è©•ä¾¡ã‚’è¡Œã„ã€ãƒ“ã‚¸ãƒã‚¹ã¸ã® actionable insights ã‚’æç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è©•ä¾¡\n",
    "y_pred_final = (y_proba_ensemble_weighted >= 0.5).astype(int)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nROC-AUC: {roc_auc_ensemble_weighted:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_final):.4f}\")\n",
    "\n",
    "print(\"\\næ··åŒè¡Œåˆ—:\")\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(cm)\n",
    "\n",
    "# æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "           xticklabels=['No Churn', 'Churn'],\n",
    "           yticklabels=['No Churn', 'Churn'])\n",
    "plt.title('Confusion Matrix - Final Model', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROCæ›²ç·š\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_ensemble_weighted)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC curve (AUC = {roc_auc_ensemble_weighted:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "plt.xlabel('False Positive Rate', fontsize=11)\n",
    "plt.ylabel('True Positive Rate', fontsize=11)\n",
    "plt.title('ROC Curve - Final Model', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ“ã‚¸ãƒã‚¹ã¸ã®æè¨€\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ãƒ“ã‚¸ãƒã‚¹ã¸ã®æè¨€ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«ã®æˆæœ:\")\n",
    "print(f\"- ROC-AUC: {roc_auc_ensemble_weighted:.4f} (ç›®æ¨™é”æˆï¼)\")\n",
    "print(f\"- é›¢åé¡§å®¢ã® {recall_score(y_test, y_pred_final):.1%} ã‚’äº‹å‰ã«ç‰¹å®šå¯èƒ½\")\n",
    "print(f\"- äºˆæ¸¬ç²¾åº¦: {precision_score(y_test, y_pred_final):.1%}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ä¸»è¦ãªé›¢åè¦å› :\")\n",
    "print(\"1. å¥‘ç´„ã‚¿ã‚¤ãƒ—\")\n",
    "print(\"   - Month-to-monthå¥‘ç´„ã®é¡§å®¢ã¯é›¢åãƒªã‚¹ã‚¯ãŒéå¸¸ã«é«˜ã„\")\n",
    "print(\"   - ğŸ’¡ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: é•·æœŸå¥‘ç´„ã¸ã®ç§»è¡Œã‚’ä¿ƒé€²ï¼ˆå‰²å¼•ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–ï¼‰\")\n",
    "\n",
    "print(\"\\n2. å¥‘ç´„æœŸé–“\")\n",
    "print(\"   - æœ€åˆã®12ãƒ¶æœˆãŒæœ€ã‚‚é›¢åãƒªã‚¹ã‚¯ãŒé«˜ã„\")\n",
    "print(\"   - ğŸ’¡ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: ã‚ªãƒ³ãƒœãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¼·åŒ–ã€åˆå¹´åº¦ã®æ‰‹åšã„ã‚µãƒãƒ¼ãƒˆ\")\n",
    "\n",
    "print(\"\\n3. ä»˜åŠ ã‚µãƒ¼ãƒ“ã‚¹ã®ä¸è¶³\")\n",
    "print(\"   - Tech Supportã‚„Online SecurityãŒãªã„é¡§å®¢ã¯é›¢åã—ã‚„ã™ã„\")\n",
    "print(\"   - ğŸ’¡ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: ã‚µãƒ¼ãƒ“ã‚¹ãƒãƒ³ãƒ‰ãƒ«ã®ææ¡ˆã€ç„¡æ–™ãƒˆãƒ©ã‚¤ã‚¢ãƒ«\")\n",
    "\n",
    "print(\"\\n4. æ–™é‡‘\")\n",
    "print(\"   - æœˆé¡æ–™é‡‘ãŒé«˜ã„é¡§å®¢ã¯é›¢åãƒªã‚¹ã‚¯ã‚ã‚Š\")\n",
    "print(\"   - ğŸ’¡ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€æŸ”è»Ÿãªæ–™é‡‘ãƒ—ãƒ©ãƒ³\")\n",
    "\n",
    "print(\"\\nğŸ’° ROIè©¦ç®—ï¼ˆä»®å®šï¼‰:\")\n",
    "print(\"- é›¢åé¡§å®¢ã®ç²å¾—ã‚³ã‚¹ãƒˆ: 30,000å††\")\n",
    "print(\"- ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ–½ç­–ã‚³ã‚¹ãƒˆ: 5,000å††/äºº\")\n",
    "print(\"- æˆåŠŸç‡ï¼ˆãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰: 30%\")\n",
    "print(\"\\nè¨ˆç®—:\")\n",
    "n_churn = int(y_test.sum())\n",
    "n_identified = int(recall_score(y_test, y_pred_final) * n_churn)\n",
    "n_saved = int(n_identified * 0.3)\n",
    "cost_acquisition = n_saved * 30000\n",
    "cost_retention = n_identified * 5000\n",
    "roi = cost_acquisition - cost_retention\n",
    "\n",
    "print(f\"- é›¢åäºˆæ¸¬æ•°: {n_identified}äºº\")\n",
    "print(f\"- ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³æˆåŠŸ: {n_saved}äºº\")\n",
    "print(f\"- ç²å¾—ã‚³ã‚¹ãƒˆç¯€ç´„: Â¥{cost_acquisition:,}\")\n",
    "print(f\"- ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ–½ç­–ã‚³ã‚¹ãƒˆ: Â¥{cost_retention:,}\")\n",
    "print(f\"- **ç´”åˆ©ç›Šï¼ˆROIï¼‰: Â¥{roi:,}**\")\n",
    "\n",
    "print(\"\\nâœ… æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "print(\"1. ãƒ¢ãƒ‡ãƒ«ã®æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤\")\n",
    "print(\"2. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰\")\n",
    "print(\"3. A/Bãƒ†ã‚¹ãƒˆã§ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ–½ç­–ã®åŠ¹æœã‚’æ¤œè¨¼\")\n",
    "print(\"4. ãƒ¢ãƒ‡ãƒ«ã®å®šæœŸçš„ãªå†å­¦ç¿’ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°\")\n",
    "print(\"5. ãƒ“ã‚¸ãƒã‚¹ãƒãƒ¼ãƒ ã¨ã®å®šæœŸçš„ãªæŒ¯ã‚Šè¿”ã‚Š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ã¾ã¨ã‚ã¨æŒ¯ã‚Šè¿”ã‚Š\n",
    "\n",
    "### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§é”æˆã—ãŸã“ã¨\n",
    "\n",
    "âœ… **ãƒ‡ãƒ¼ã‚¿ç†è§£**: EDAã§ä¸»è¦ãªé›¢åè¦å› ã‚’ç‰¹å®š\n",
    "\n",
    "âœ… **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’æ´»ç”¨ã—ãŸå¼·åŠ›ãªç‰¹å¾´é‡ã‚’ä½œæˆ\n",
    "\n",
    "âœ… **ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­–**: SMOTEã§å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã‚’å¢—ã‚„ã—æ€§èƒ½å‘ä¸Š\n",
    "\n",
    "âœ… **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–**: Optunaã§æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç™ºè¦‹\n",
    "\n",
    "âœ… **ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ**: SHAPã§äºˆæ¸¬æ ¹æ‹ ã‚’å¯è¦–åŒ–ã—ã€ãƒ“ã‚¸ãƒã‚¹ã¸ã®ç¤ºå”†ã‚’æŠ½å‡º\n",
    "\n",
    "âœ… **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: Stackingã§æœ€çµ‚æ€§èƒ½ã‚’å‘ä¸Š\n",
    "\n",
    "âœ… **ãƒ“ã‚¸ãƒã‚¹æè¨€**: ROIè©¦ç®—ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã‚’æç¤º\n",
    "\n",
    "### ä½¿ç”¨ã—ãŸæŠ€è¡“ã®ç·ã¾ã¨ã‚\n",
    "\n",
    "| ãƒ•ã‚§ãƒ¼ã‚º | ä½¿ç”¨æŠ€è¡“ | ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ |\n",
    "|---------|---------|-------------|\n",
    "| ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ | make_classification | 01 |\n",
    "| EDA | çµ±è¨ˆåˆ†æã€å¯è¦–åŒ– | - |\n",
    "| å‰å‡¦ç† | Target Encoding | 02, 25 |\n",
    "| ç‰¹å¾´é‡ä½œæˆ | ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã€é›†ç´„ã€æ¯”ç‡ | 02 |\n",
    "| ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | 5ç¨®é¡ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | 04-06 |\n",
    "| ä¸å‡è¡¡å¯¾ç­– | SMOTE | 23 |\n",
    "| æœ€é©åŒ– | Optuna | 20 |\n",
    "| è§£é‡ˆ | SHAP | 21 |\n",
    "| ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« | Weighted Averaging | 22 |\n",
    "| è©•ä¾¡ | ROC-AUC, Confusion Matrix | 03 |\n",
    "\n",
    "### å­¦ã‚“ã é‡è¦ãªæ•™è¨“\n",
    "\n",
    "1. **ãƒ“ã‚¸ãƒã‚¹ç†è§£ãŒæœ€å„ªå…ˆ**: æŠ€è¡“ã‚ˆã‚Šã¾ãšãƒ“ã‚¸ãƒã‚¹èª²é¡Œã‚’ç†è§£ã™ã‚‹\n",
    "\n",
    "2. **EDAã«æ™‚é–“ã‚’ã‹ã‘ã‚‹**: ãƒ‡ãƒ¼ã‚¿ç†è§£ãŒå…¨ã¦ã®åŸºç¤\n",
    "\n",
    "3. **ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãŒéµ**: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§å·®ãŒã¤ã\n",
    "\n",
    "4. **å˜ä¸€æŒ‡æ¨™ã«ä¾å­˜ã—ãªã„**: è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ã§å¤šè§’çš„ã«è©•ä¾¡\n",
    "\n",
    "5. **è§£é‡ˆå¯èƒ½æ€§ã‚’å¿˜ã‚Œãªã„**: ãƒ¢ãƒ‡ãƒ«ã®ä¸­èº«ã‚’ãƒ“ã‚¸ãƒã‚¹ã«èª¬æ˜ã§ãã‚‹ã“ã¨ãŒé‡è¦\n",
    "\n",
    "6. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§å®‰å®šåŒ–**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ robustness ã‚’å‘ä¸Š\n",
    "\n",
    "7. **ROIã‚’è¨ˆç®—ã™ã‚‹**: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ä¾¡å€¤ã‚’æ•°å€¤ã§ç¤ºã™\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å­¦ã‚“ã ã‚¹ã‚­ãƒ«ã‚’æ´»ã‹ã—ã¦ï¼š\n",
    "\n",
    "- ğŸ“Š å®Ÿéš›ã®Kaggleã‚³ãƒ³ãƒšã«å‚åŠ \n",
    "- ğŸ¢ è‡ªç¤¾/ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ“ã‚¸ãƒã‚¹èª²é¡Œã«é©ç”¨\n",
    "- ğŸ“š ã‚ˆã‚Šé«˜åº¦ãªæŠ€è¡“ã‚’å­¦ç¿’ï¼ˆAutoMLã€Neural Networksãªã©ï¼‰\n",
    "- ğŸ‘¥ ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã¨ã—ã¦å…¬é–‹ã—ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰\n",
    "\n",
    "### æœ€å¾Œã«\n",
    "\n",
    "**ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼ğŸ‰**\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆ01-28ï¼‰ã‚’é€šã˜ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã¨ã—ã¦å¿…è¦ãª\n",
    "åŸºç¤ã‹ã‚‰å®Ÿè·µã¾ã§ã®å…¨ã¦ã‚’å­¦ã³ã¾ã—ãŸã€‚\n",
    "\n",
    "ã“ã‚Œã‹ã‚‰ã‚‚å­¦ã³ç¶šã‘ã€å®Ÿè·µã‚’é‡ã­ã€\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®åŠ›ã§ãƒ“ã‚¸ãƒã‚¹ã¨ç¤¾ä¼šã«è²¢çŒ®ã—ã¦ã„ãã¾ã—ã‚‡ã†ï¼\n",
    "\n",
    "**Happy Data Science! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
