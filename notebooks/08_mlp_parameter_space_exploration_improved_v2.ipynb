{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ノートブック08: MLPパラメータ空間の探索\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックでは、MLPの体系的なハイパーパラメータチューニングを学びます:\n",
    "\n",
    "1. **GridSearchCV**\n",
    "   - 網羅的なパラメータ探索\n",
    "   - 最適な組み合わせの発見\n",
    "\n",
    "2. **RandomizedSearchCV**\n",
    "   - 大規模パラメータ空間の探索\n",
    "   - 効率的なサンプリング\n",
    "\n",
    "3. **パラメータの可視化**\n",
    "   - ヒートマップで相互作用を理解\n",
    "   - パラメータの感度分析\n",
    "\n",
    "4. **学習曲線**\n",
    "   - データ量と性能の関係\n",
    "   - 過学習/未学習の診断\n",
    "\n",
    "5. **最適化戦略**\n",
    "   - 段階的なチューニング\n",
    "   - 実践的なベストプラクティス\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, RandomizedSearchCV,\n",
    "    cross_val_score, learning_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import uniform, loguniform\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. データセットの準備\n",
    "\n",
    "複雑な多クラス分類問題を生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多クラス分類データの生成\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=3,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# データ分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# スケーリング\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {len(np.unique(y))}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. アーキテクチャの探索\n",
    "\n",
    "異なるネットワーク構造を体系的に評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 様々なアーキテクチャをテスト\n",
    "architectures = [\n",
    "    # 1層\n",
    "    (25,), (50,), (100,), (200,),\n",
    "    # 2層\n",
    "    (50, 25), (100, 50), (200, 100),\n",
    "    # 3層\n",
    "    (100, 50, 25), (200, 100, 50),\n",
    "    # 4層\n",
    "    (150, 100, 50, 25)\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=arch,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # クロスバリデーション\n",
    "    cv_scores = cross_val_score(mlp, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # 訓練とテスト\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    test_acc = mlp.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # パラメータ数\n",
    "    n_params = sum(w.size + b.size for w, b in zip(mlp.coefs_, mlp.intercepts_))\n",
    "    \n",
    "    results.append({\n",
    "        'Architecture': str(arch),\n",
    "        'Depth': len(arch),\n",
    "        'Parameters': n_params,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std(),\n",
    "        'Test Acc': test_acc,\n",
    "        'Iterations': mlp.n_iter_\n",
    "    })\n",
    "\n",
    "df_arch = pd.DataFrame(results).sort_values('CV Mean', ascending=False)\n",
    "\n",
    "print(\"\\nArchitecture Comparison (Top 5):\")\n",
    "print(df_arch.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アーキテクチャ結果の可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# CVスコア\n",
    "df_sorted = df_arch.sort_values('CV Mean')\n",
    "colors = ['C0' if d==1 else 'C1' if d==2 else 'C2' if d==3 else 'C3' for d in df_sorted['Depth']]\n",
    "\n",
    "axes[0].barh(range(len(df_sorted)), df_sorted['CV Mean'], \n",
    "             xerr=df_sorted['CV Std'], color=colors, alpha=0.7)\n",
    "axes[0].set_yticks(range(len(df_sorted)))\n",
    "axes[0].set_yticklabels(df_sorted['Architecture'], fontsize=9)\n",
    "axes[0].set_xlabel('CV Accuracy', fontsize=11)\n",
    "axes[0].set_title('Architecture Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# パラメータ数 vs 性能\n",
    "for depth in [1, 2, 3, 4]:\n",
    "    subset = df_arch[df_arch['Depth'] == depth]\n",
    "    axes[1].scatter(subset['Parameters'], subset['CV Mean'], \n",
    "                    s=100, label=f'{depth} layer(s)', alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('Number of Parameters', fontsize=11)\n",
    "axes[1].set_ylabel('CV Accuracy', fontsize=11)\n",
    "axes[1].set_title('Model Complexity vs Performance', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 学習率とAlphaの探索\n",
    "\n",
    "ヒートマップで相互作用を可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率 vs Alpha のグリッド探索\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "alphas = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]\n",
    "\n",
    "results_grid = np.zeros((len(learning_rates), len(alphas)))\n",
    "\n",
    "print(\"Running grid search...\")\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, alpha in enumerate(alphas):\n",
    "        mlp = MLPClassifier(\n",
    "            hidden_layer_sizes=(100, 50),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=alpha,\n",
    "            learning_rate_init=lr,\n",
    "            max_iter=500,\n",
    "            early_stopping=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        cv_scores = cross_val_score(mlp, X_train_scaled, y_train, cv=3, scoring='accuracy')\n",
    "        results_grid[i, j] = cv_scores.mean()\n",
    "\n",
    "# ヒートマップ\n",
    "plt.figure(figsize=(11, 8))\n",
    "sns.heatmap(results_grid, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=[f'{a:.4f}' for a in alphas],\n",
    "            yticklabels=[f'{lr:.4f}' for lr in learning_rates],\n",
    "            cbar_kws={'label': 'CV Accuracy'})\n",
    "plt.xlabel('Alpha (L2 Regularization)', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Learning Rate vs Alpha Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_idx = np.unravel_index(np.argmax(results_grid), results_grid.shape)\n",
    "print(f\"\\nBest combination:\")\n",
    "print(f\"  Learning rate: {learning_rates[best_idx[0]]}\")\n",
    "print(f\"  Alpha: {alphas[best_idx[1]]}\")\n",
    "print(f\"  CV Accuracy: {results_grid[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GridSearchCVによる包括的探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータグリッドの定義\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "mlp_base = MLPClassifier(\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    mlp_base,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV...\")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Score: {grid_search.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10の組み合わせを表示\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "cols = ['param_hidden_layer_sizes', 'param_activation', 'param_alpha',\n",
    "        'param_learning_rate_init', 'mean_test_score', 'std_test_score',\n",
    "        'mean_train_score', 'rank_test_score']\n",
    "results_summary = results_df[cols].sort_values('rank_test_score')\n",
    "\n",
    "print(\"\\nTop 10 Parameter Combinations:\")\n",
    "print(results_summary.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch結果の可視化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# アーキテクチャの影響\n",
    "arch_results = results_df.groupby('param_hidden_layer_sizes')['mean_test_score'].agg(['mean', 'std'])\n",
    "arch_labels = [str(a) for a in arch_results.index]\n",
    "axes[0, 0].bar(arch_labels, arch_results['mean'], yerr=arch_results['std'], alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Architecture', fontsize=10)\n",
    "axes[0, 0].set_ylabel('Mean CV Accuracy', fontsize=10)\n",
    "axes[0, 0].set_title('Effect of Architecture', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 活性化関数の影響\n",
    "act_results = results_df.groupby('param_activation')['mean_test_score'].agg(['mean', 'std'])\n",
    "axes[0, 1].bar(act_results.index, act_results['mean'], yerr=act_results['std'], alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Activation', fontsize=10)\n",
    "axes[0, 1].set_ylabel('Mean CV Accuracy', fontsize=10)\n",
    "axes[0, 1].set_title('Effect of Activation Function', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Alphaの影響\n",
    "alpha_results = results_df.groupby('param_alpha')['mean_test_score'].agg(['mean', 'std'])\n",
    "axes[1, 0].errorbar(range(len(alpha_results)), alpha_results['mean'],\n",
    "                    yerr=alpha_results['std'], marker='o', capsize=5)\n",
    "axes[1, 0].set_xticks(range(len(alpha_results)))\n",
    "axes[1, 0].set_xticklabels([f'{a:.4f}' for a in alpha_results.index])\n",
    "axes[1, 0].set_xlabel('Alpha', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Mean CV Accuracy', fontsize=10)\n",
    "axes[1, 0].set_title('Effect of Regularization', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Train vs Test\n",
    "axes[1, 1].scatter(results_df['mean_train_score'], results_df['mean_test_score'], alpha=0.5)\n",
    "axes[1, 1].plot([0.8, 1], [0.8, 1], 'r--', lw=1)\n",
    "axes[1, 1].set_xlabel('Mean Train Score', fontsize=10)\n",
    "axes[1, 1].set_ylabel('Mean Test Score', fontsize=10)\n",
    "axes[1, 1].set_title('Train vs Test Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. RandomizedSearchCVで大規模探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ分布の定義\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [\n",
    "        (50,), (100,), (200,),\n",
    "        (50, 25), (100, 50), (200, 100),\n",
    "        (100, 50, 25), (200, 100, 50)\n",
    "    ],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': loguniform(1e-5, 1e-1),\n",
    "    'learning_rate_init': loguniform(1e-4, 1e-1),\n",
    "    'batch_size': [32, 64, 128, 256]\n",
    "}\n",
    "\n",
    "mlp_random = MLPClassifier(\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    mlp_random,\n",
    "    param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting RandomizedSearchCV...\")\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOMIZED SEARCH RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV Score: {random_search.best_score_:.4f}\")\n",
    "print(f\"Test Score: {random_search.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 学習曲線の分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最良モデルで学習曲線を生成\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_model, X_train_scaled, y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "test_mean = test_scores.mean(axis=1)\n",
    "test_std = test_scores.std(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score', linewidth=2)\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,\n",
    "                 alpha=0.1, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, 'o-', color='green', label='Cross-validation score', linewidth=2)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std,\n",
    "                 alpha=0.1, color='green')\n",
    "\n",
    "plt.xlabel('Training Set Size', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Learning Curve for Best MLP Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning Curve Analysis:\")\n",
    "print(\"- Training and validation curves converge: Good fit\")\n",
    "print(\"- Large gap between curves: Overfitting\")\n",
    "print(\"- Both curves plateau at low score: Underfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## まとめ\n",
    "\n",
    "### パラメータチューニングの戦略\n",
    "\n",
    "1. **段階的アプローチ**\n",
    "   - まずアーキテクチャを決定\n",
    "   - 次に学習率とalphaを調整\n",
    "   - 最後に細かいパラメータを微調整\n",
    "\n",
    "2. **GridSearchCV vs RandomizedSearchCV**\n",
    "   - GridSearchCV: 小規模パラメータ空間\n",
    "   - RandomizedSearchCV: 大規模パラメータ空間\n",
    "\n",
    "3. **重要なパラメータ**\n",
    "   - `hidden_layer_sizes`: アーキテクチャ\n",
    "   - `alpha`: 正則化強度\n",
    "   - `learning_rate_init`: 学習率\n",
    "   - `activation`: 活性化関数\n",
    "\n",
    "4. **診断ツール**\n",
    "   - ヒートマップ: パラメータ相互作用\n",
    "   - 学習曲線: 過学習/未学習\n",
    "   - クロスバリデーション: 汎化性能\n",
    "\n",
    "### ベストプラクティス\n",
    "\n",
    "1. 常にクロスバリデーションを使用\n",
    "2. Early stoppingを有効化\n",
    "3. 学習曲線で診断\n",
    "4. ヒートマップで可視化\n",
    "5. テストセットは最後に評価\n",
    "\n",
    "---\n",
    "\n",
    "**次のステップ**: ノートブック09で、MLPを回帰問題と時系列予測に適用します!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
