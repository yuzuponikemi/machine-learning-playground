{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®é«˜åº¦ãªå‡¦ç† (Advanced Categorical Variable Encoding)\n",
    "\n",
    "## æ¦‚è¦\n",
    "ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’åŠ¹æœçš„ã«æ•°å€¤åŒ–ã™ã‚‹ãŸã‚ã®é«˜åº¦ãªæ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚Target Encodingã€Frequency Encodingã€Embeddingãªã©ã€å®Ÿå‹™ã§å·®ãŒã¤ããƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ç¶²ç¾…ã—ã¾ã™ã€‚\n",
    "\n",
    "## å­¦ç¿’ç›®æ¨™\n",
    "- ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ç¨®é¡ã‚’ç†è§£ã§ãã‚‹\n",
    "- åŸºæœ¬çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•ã‚’ä½¿ãˆã‚‹\n",
    "- Target Encodingã‚’æ­£ã—ãå®Ÿè£…ã§ãã‚‹\n",
    "- é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å¤‰æ•°ã‚’æ‰±ãˆã‚‹\n",
    "- Embeddingã®æ¦‚å¿µã‚’ç†è§£ã§ãã‚‹\n",
    "- å®Ÿå‹™ã§ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’é©ç”¨ã§ãã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from category_encoders import TargetEncoder, LeaveOneOutEncoder, WOEEncoder, CatBoostEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¨­å®š\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã¨ã¯\n",
    "\n",
    "### ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®å®šç¾©\n",
    "\n",
    "**ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ï¼ˆCategorical Variableï¼‰**ã¯ã€æœ‰é™å€‹ã®å€¤ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰ã‚’å–ã‚‹å¤‰æ•°ã§ã™ã€‚\n",
    "\n",
    "### ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ç¨®é¡\n",
    "\n",
    "1. **åç¾©å¤‰æ•°ï¼ˆNominalï¼‰**\n",
    "   - é †åºé–¢ä¿‚ãŒãªã„\n",
    "   - ä¾‹: è‰²ï¼ˆèµ¤ã€é’ã€ç·‘ï¼‰ã€éƒ½å¸‚åã€å•†å“ã‚«ãƒ†ã‚´ãƒª\n",
    "\n",
    "2. **é †åºå¤‰æ•°ï¼ˆOrdinalï¼‰**\n",
    "   - é †åºé–¢ä¿‚ãŒã‚ã‚‹\n",
    "   - ä¾‹: æ•™è‚²ãƒ¬ãƒ™ãƒ«ï¼ˆå°å­¦æ ¡ < ä¸­å­¦æ ¡ < é«˜æ ¡ < å¤§å­¦ï¼‰ã€ã‚µã‚¤ã‚ºï¼ˆS < M < L < XLï¼‰\n",
    "\n",
    "3. **äºŒå€¤å¤‰æ•°ï¼ˆBinaryï¼‰**\n",
    "   - 2ã¤ã®å€¤ã®ã¿\n",
    "   - ä¾‹: æ€§åˆ¥ï¼ˆç”·/å¥³ï¼‰ã€Yes/No\n",
    "\n",
    "### ãªãœã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå¿…è¦ã‹\n",
    "\n",
    "å¤šãã®æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯**æ•°å€¤ãƒ‡ãƒ¼ã‚¿**ã—ã‹æ‰±ãˆã¾ã›ã‚“ã€‚\n",
    "\n",
    "- âœ… ãƒ„ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆRandom Forestã€XGBoostãªã©ï¼‰: ã‚«ãƒ†ã‚´ãƒªã‚’ç›´æ¥æ‰±ãˆã‚‹å ´åˆã‚‚ã‚ã‚‹\n",
    "- âŒ ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆLogistic Regressionã€SVMãªã©ï¼‰: æ•°å€¤åŒ–ãŒå¿…é ˆ\n",
    "\n",
    "### å®Ÿä¸–ç•Œã§ã®ä¾‹\n",
    "\n",
    "- ğŸ  **ä¸å‹•ç”£**: åœ°åŸŸã€å»ºç‰©ã‚¿ã‚¤ãƒ—\n",
    "- ğŸ›’ **Eã‚³ãƒãƒ¼ã‚¹**: å•†å“ã‚«ãƒ†ã‚´ãƒªã€ãƒ–ãƒ©ãƒ³ãƒ‰\n",
    "- ğŸ’³ **é‡‘è**: è·æ¥­ã€éƒ½é“åºœçœŒ\n",
    "- ğŸ¥ **åŒ»ç™‚**: è¨ºæ–­åã€æ²»ç™‚æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆä½å®…ãƒ­ãƒ¼ãƒ³æ‰¿èªäºˆæ¸¬ï¼‰\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°\n",
    "cities = ['Tokyo', 'Osaka', 'Nagoya', 'Fukuoka', 'Sapporo', 'Sendai', 'Hiroshima', 'Kyoto']\n",
    "occupations = ['Engineer', 'Doctor', 'Teacher', 'Sales', 'Manager', 'Student', 'Self-employed', \n",
    "               'Artist', 'Unemployed', 'Retired']\n",
    "education = ['High School', 'Bachelor', 'Master', 'PhD']\n",
    "property_type = ['Apartment', 'House', 'Condo', 'Townhouse']\n",
    "marital_status = ['Single', 'Married', 'Divorced', 'Widowed']\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "data = {\n",
    "    'city': np.random.choice(cities, n_samples),\n",
    "    'occupation': np.random.choice(occupations, n_samples),\n",
    "    'education': np.random.choice(education, n_samples),\n",
    "    'property_type': np.random.choice(property_type, n_samples),\n",
    "    'marital_status': np.random.choice(marital_status, n_samples),\n",
    "    'age': np.random.randint(22, 70, n_samples),\n",
    "    'income': np.random.lognormal(13, 0.5, n_samples),  # å¹´åï¼ˆä¸‡å††ï¼‰\n",
    "    'loan_amount': np.random.lognormal(15, 0.4, n_samples)  # ãƒ­ãƒ¼ãƒ³é¡ï¼ˆä¸‡å††ï¼‰\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆæ‰¿èª/å´ä¸‹ï¼‰ã®ç”Ÿæˆ\n",
    "# åå…¥ãŒé«˜ã„ã€æ•™è‚²ãƒ¬ãƒ™ãƒ«ãŒé«˜ã„ã€ç‰¹å®šã®è·æ¥­ãªã©ã§æ‰¿èªã•ã‚Œã‚„ã™ã„\n",
    "approval_score = (\n",
    "    (df['income'] / df['income'].mean()) * 0.3 +\n",
    "    (df['education'].map({'High School': 0.5, 'Bachelor': 0.7, 'Master': 0.85, 'PhD': 1.0})) * 0.25 +\n",
    "    (df['occupation'].map({\n",
    "        'Doctor': 1.0, 'Engineer': 0.9, 'Manager': 0.85, 'Teacher': 0.75,\n",
    "        'Sales': 0.6, 'Self-employed': 0.55, 'Artist': 0.5, \n",
    "        'Student': 0.3, 'Unemployed': 0.1, 'Retired': 0.4\n",
    "    })) * 0.25 +\n",
    "    (1 - df['loan_amount'] / df['income']) * 0.2 +\n",
    "    np.random.normal(0, 0.1, n_samples)\n",
    ")\n",
    "\n",
    "df['approved'] = (approval_score > approval_score.median()).astype(int)\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df.shape}\")\n",
    "print(f\"\\nã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®æ•°: {df.select_dtypes(include='object').shape[1]}\")\n",
    "print(f\"æ•°å€¤å¤‰æ•°ã®æ•°: {df.select_dtypes(include=['int64', 'float64']).shape[1] - 1}\")\n",
    "print(f\"\\nã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ:\")\n",
    "print(df['approved'].value_counts())\n",
    "print(f\"\\nãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®è©³ç´°\n",
    "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "print(\"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®è©³ç´°:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°: {n_unique}\")\n",
    "    print(f\"  å€¤: {df[col].unique()[:10].tolist()}\")\n",
    "    \n",
    "    # ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°ï¼‰ã«åŸºã¥ãåˆ†é¡\n",
    "    if n_unique == 2:\n",
    "        cardinality = \"ä½ï¼ˆBinaryï¼‰\"\n",
    "    elif n_unique <= 5:\n",
    "        cardinality = \"ä½\"\n",
    "    elif n_unique <= 10:\n",
    "        cardinality = \"ä¸­\"\n",
    "    else:\n",
    "        cardinality = \"é«˜\"\n",
    "    \n",
    "    print(f\"  ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£: {cardinality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åŸºæœ¬çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•\n",
    "\n",
    "### 3.1 Label Encoding\n",
    "\n",
    "å„ã‚«ãƒ†ã‚´ãƒªã«æ•´æ•°ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚\n",
    "\n",
    "- âœ… **é©ç”¨å ´é¢**: é †åºå¤‰æ•°\n",
    "- âŒ **æ³¨æ„**: åç¾©å¤‰æ•°ã«ä½¿ã†ã¨èª¤ã£ãŸé †åºé–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹å¯èƒ½æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_label = df.copy()\n",
    "\n",
    "# æ•™è‚²ãƒ¬ãƒ™ãƒ«ï¼ˆé †åºå¤‰æ•°ï¼‰\n",
    "education_order = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_label['education_label'] = df_label['education'].map(education_order)\n",
    "\n",
    "print(\"Label Encodingï¼ˆæ•™è‚²ãƒ¬ãƒ™ãƒ«ï¼‰:\")\n",
    "print(df_label[['education', 'education_label']].drop_duplicates().sort_values('education_label'))\n",
    "\n",
    "print(\"\\nğŸ’¡ ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "print(\"- é †åºå¤‰æ•°ã«ã¯é©åˆ‡ï¼ˆHigh School < Bachelor < Master < PhDï¼‰\")\n",
    "print(\"- åç¾©å¤‰æ•°ã«ã¯ä¸é©åˆ‡ï¼ˆéƒ½å¸‚åãªã©ã«é †åºé–¢ä¿‚ã¯ãªã„ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 One-Hot Encoding\n",
    "\n",
    "å„ã‚«ãƒ†ã‚´ãƒªã‚’äºŒå€¤å¤‰æ•°ï¼ˆãƒ€ãƒŸãƒ¼å¤‰æ•°ï¼‰ã«å¤‰æ›ã—ã¾ã™ã€‚\n",
    "\n",
    "- âœ… **é©ç”¨å ´é¢**: ä½ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®åç¾©å¤‰æ•°\n",
    "- âŒ **æ³¨æ„**: é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã ã¨æ¬¡å…ƒçˆ†ç™º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "df_onehot = df.copy()\n",
    "\n",
    "# property_typeï¼ˆ4ã‚«ãƒ†ã‚´ãƒªï¼‰ã‚’One-Hot Encoding\n",
    "onehot_encoded = pd.get_dummies(df_onehot['property_type'], prefix='property')\n",
    "df_onehot = pd.concat([df_onehot, onehot_encoded], axis=1)\n",
    "\n",
    "print(\"One-Hot Encodingï¼ˆç‰©ä»¶ã‚¿ã‚¤ãƒ—ï¼‰:\")\n",
    "print(df_onehot[['property_type'] + onehot_encoded.columns.tolist()].head(10))\n",
    "\n",
    "print(f\"\\nå…ƒã®åˆ—æ•°: 1\")\n",
    "print(f\"One-Hotå¾Œã®åˆ—æ•°: {len(onehot_encoded.columns)}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "print(\"- å„ã‚«ãƒ†ã‚´ãƒªãŒç‹¬ç«‹ã—ãŸåˆ—ã«ãªã‚‹\")\n",
    "print(\"- ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã«æœ€é©\")\n",
    "print(\"- ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãŒé«˜ã„ã¨åˆ—æ•°ãŒçˆ†ç™ºçš„ã«å¢—ãˆã‚‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å¤‰æ•°ã§ã®One-Hot Encodingã®å•é¡Œ\n",
    "print(\"é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å¤‰æ•°ã®ä¾‹ï¼ˆoccupation: 10ã‚«ãƒ†ã‚´ãƒªï¼‰:\")\n",
    "occupation_onehot = pd.get_dummies(df['occupation'], prefix='occ')\n",
    "print(f\"åˆ—æ•°: {len(occupation_onehot.columns)}\")\n",
    "print(f\"\\nã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãŒ100ãªã‚‰ â†’ 100åˆ—\")\n",
    "print(f\"ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãŒ1000ãªã‚‰ â†’ 1000åˆ—ï¼ˆæ¬¡å…ƒã®å‘ªã„ï¼‰\")\n",
    "\n",
    "print(\"\\nâš ï¸ å•é¡Œç‚¹:\")\n",
    "print(\"- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¢—å¤§\")\n",
    "print(\"- å­¦ç¿’æ™‚é–“ã®å¢—åŠ \")\n",
    "print(\"- éå­¦ç¿’ã®ãƒªã‚¹ã‚¯\")\n",
    "print(\"- ã‚¹ãƒ‘ãƒ¼ã‚¹ï¼ˆç–ï¼‰ãªç‰¹å¾´é‡è¡Œåˆ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. é«˜åº¦ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•\n",
    "\n",
    "### 4.1 Frequency Encodingï¼ˆé »åº¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰\n",
    "\n",
    "å„ã‚«ãƒ†ã‚´ãƒªã®å‡ºç¾é »åº¦ã‚’å€¤ã¨ã—ã¦ä½¿ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Encoding\n",
    "def frequency_encoding(df, column):\n",
    "    \"\"\"\n",
    "    é »åº¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "    column : str\n",
    "        å¯¾è±¡ã®ã‚«ãƒ©ãƒ å\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    encoded : Series\n",
    "        ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ã®ç³»åˆ—\n",
    "    \"\"\"\n",
    "    freq = df[column].value_counts(normalize=True)\n",
    "    return df[column].map(freq)\n",
    "\n",
    "df_freq = df.copy()\n",
    "df_freq['city_freq'] = frequency_encoding(df_freq, 'city')\n",
    "df_freq['occupation_freq'] = frequency_encoding(df_freq, 'occupation')\n",
    "\n",
    "print(\"Frequency Encoding:\")\n",
    "print(df_freq[['city', 'city_freq', 'occupation', 'occupation_freq']].head(10))\n",
    "\n",
    "# é »åº¦ã®åˆ†å¸ƒ\n",
    "city_freq_summary = df_freq.groupby('city')['city_freq'].first().sort_values(ascending=False)\n",
    "print(\"\\néƒ½å¸‚åˆ¥ã®é »åº¦:\")\n",
    "print(city_freq_summary)\n",
    "\n",
    "print(\"\\nğŸ’¡ ãƒ¡ãƒªãƒƒãƒˆ:\")\n",
    "print(\"- å˜ä¸€ã®åˆ—ã§è¡¨ç¾ï¼ˆæ¬¡å…ƒå¢—åŠ ãªã—ï¼‰\")\n",
    "print(\"- é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã§ã‚‚ä½¿ãˆã‚‹\")\n",
    "print(\"- å®Ÿè£…ãŒç°¡å˜\")\n",
    "\n",
    "print(\"\\nâš ï¸ ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ:\")\n",
    "print(\"- ç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªãŒåŒã˜å€¤ã«ãªã‚‹å¯èƒ½æ€§\")\n",
    "print(\"- ã‚«ãƒ†ã‚´ãƒªã®æ„å‘³ã‚’å¤±ã†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Target Encodingï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰\n",
    "\n",
    "å„ã‚«ãƒ†ã‚´ãƒªã«ãŠã‘ã‚‹ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®å¹³å‡å€¤ã‚’ä½¿ã„ã¾ã™ã€‚\n",
    "\n",
    "**âš ï¸ é‡è¦**: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚ã€**Out-of-Fold (OOF)** ã§è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒŠã‚¤ãƒ¼ãƒ–ãªTarget Encodingï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚ã‚Š - æ‚ªã„ä¾‹ï¼‰\n",
    "def naive_target_encoding(df, column, target):\n",
    "    \"\"\"\n",
    "    ãƒŠã‚¤ãƒ¼ãƒ–ãªTarget Encodingï¼ˆæ•™è‚²ç›®çš„ã®ã¿ï¼å®Ÿå‹™ã§ã¯ä½¿ç”¨ç¦æ­¢ï¼‰\n",
    "    \"\"\"\n",
    "    target_mean = df.groupby(column)[target].mean()\n",
    "    return df[column].map(target_mean)\n",
    "\n",
    "df_target_naive = df.copy()\n",
    "df_target_naive['occupation_target_naive'] = naive_target_encoding(\n",
    "    df_target_naive, 'occupation', 'approved'\n",
    ")\n",
    "\n",
    "print(\"ãƒŠã‚¤ãƒ¼ãƒ–ãªTarget Encodingï¼ˆè·æ¥­ï¼‰:\")\n",
    "occupation_target = df_target_naive.groupby('occupation').agg({\n",
    "    'approved': 'mean',\n",
    "    'occupation_target_naive': 'first'\n",
    "}).sort_values('approved', ascending=False)\n",
    "print(occupation_target)\n",
    "\n",
    "print(\"\\nâš ï¸ è­¦å‘Š:\")\n",
    "print(\"- ã“ã®ã‚³ãƒ¼ãƒ‰ã¯æ•™è‚²ç›®çš„ã®æ‚ªã„ä¾‹ã§ã™\")\n",
    "print(\"- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«æ¼ã‚Œã‚‹ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ï¼‰\")\n",
    "print(\"- éå­¦ç¿’ã‚’å¼•ãèµ·ã“ã™\")\n",
    "print(\"- å®Ÿå‹™ã§ã¯çµ¶å¯¾ã«ä½¿ç”¨ç¦æ­¢ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­£ã—ã„Target Encodingï¼ˆOut-of-Foldæ–¹å¼ï¼‰\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "X = df[categorical_cols + ['age', 'income', 'loan_amount']]\n",
    "y = df['approved']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Target Encoderï¼ˆcategory_encodersãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰\n",
    "target_encoder = TargetEncoder(cols=categorical_cols, smoothing=1.0)\n",
    "\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§fitã—ã¦å¤‰æ›\n",
    "X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡ã§å¤‰æ›\n",
    "X_test_encoded = target_encoder.transform(X_test)\n",
    "\n",
    "print(\"æ­£ã—ã„Target Encoding:\")\n",
    "print(\"\\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¾Œï¼‰:\")\n",
    "print(X_train_encoded.head(10))\n",
    "\n",
    "print(\"\\nğŸ’¡ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "print(\"- fit()ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§è¡Œã†\")\n",
    "print(\"- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯transform()ã®ã¿é©ç”¨\")\n",
    "print(\"- ã“ã‚Œã«ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ã\")\n",
    "print(\"- ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã‚‚å„Foldã§å€‹åˆ¥ã«fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothingãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŠ¹æœ\n",
    "print(\"Target Encodingã®ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è·æ¥­åˆ¥ã®çµ±è¨ˆ\n",
    "occupation_stats = df.groupby('occupation').agg({\n",
    "    'approved': ['mean', 'count']\n",
    "}).round(4)\n",
    "occupation_stats.columns = ['approval_rate', 'count']\n",
    "occupation_stats = occupation_stats.sort_values('count')\n",
    "\n",
    "print(occupation_stats)\n",
    "\n",
    "print(\"\\nğŸ’¡ ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã®å¿…è¦æ€§:\")\n",
    "print(\"- ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ã‚«ãƒ†ã‚´ãƒªã¯ä¿¡é ¼æ€§ãŒä½ã„\")\n",
    "print(\"- ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã§å…¨ä½“å¹³å‡ã«è¿‘ã¥ã‘ã‚‹\")\n",
    "print(\"- éå­¦ç¿’ã‚’é˜²ãåŠ¹æœ\")\n",
    "print(\"\\nå¼: (count * mean + smoothing * global_mean) / (count + smoothing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 CatBoost Encoding\n",
    "\n",
    "CatBoostã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ä½¿ã‚ã‚Œã‚‹ã€ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸTarget Encodingã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Encoding\n",
    "from category_encoders import CatBoostEncoder\n",
    "\n",
    "catboost_encoder = CatBoostEncoder(cols=categorical_cols)\n",
    "\n",
    "X_train_catboost = catboost_encoder.fit_transform(X_train, y_train)\n",
    "X_test_catboost = catboost_encoder.transform(X_test)\n",
    "\n",
    "print(\"CatBoost Encoding:\")\n",
    "print(\"\\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¾Œï¼‰:\")\n",
    "print(X_train_catboost.head(10))\n",
    "\n",
    "print(\"\\nğŸ’¡ CatBoost Encodingã®ç‰¹å¾´:\")\n",
    "print(\"- Target Encodingã®æ”¹è‰¯ç‰ˆ\")\n",
    "print(\"- ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸé †åºä¾å­˜ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\")\n",
    "print(\"- CatBoostãƒ¢ãƒ‡ãƒ«ã¨ç›¸æ€§ãŒè‰¯ã„\")\n",
    "print(\"- ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’æœ€å°åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Weight of Evidence (WoE) Encoding\n",
    "\n",
    "**WoE Encoding**ã¯ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒ­ã‚°ã‚ªãƒƒã‚ºæ¯”ã«åŸºã¥ãã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WoE Encoding\n",
    "from category_encoders import WOEEncoder\n",
    "\n",
    "woe_encoder = WOEEncoder(cols=categorical_cols)\n",
    "\n",
    "X_train_woe = woe_encoder.fit_transform(X_train, y_train)\n",
    "X_test_woe = woe_encoder.transform(X_test)\n",
    "\n",
    "print(\"WoE Encoding:\")\n",
    "print(\"\\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¾Œï¼‰:\")\n",
    "print(X_train_woe.head(10))\n",
    "\n",
    "# è·æ¥­åˆ¥ã®WoEå€¤\n",
    "occupation_woe = X_train_woe.groupby(X_train['occupation'])['occupation'].first().sort_values()\n",
    "print(\"\\nè·æ¥­åˆ¥ã®WoEå€¤:\")\n",
    "print(occupation_woe)\n",
    "\n",
    "print(\"\\nğŸ’¡ WoEã®è§£é‡ˆ:\")\n",
    "print(\"- æ­£ã®å€¤: æ‰¿èªã•ã‚Œã‚„ã™ã„ã‚«ãƒ†ã‚´ãƒª\")\n",
    "print(\"- è² ã®å€¤: å´ä¸‹ã•ã‚Œã‚„ã™ã„ã‚«ãƒ†ã‚´ãƒª\")\n",
    "print(\"- 0ã«è¿‘ã„: å…¨ä½“å¹³å‡ã¨åŒç¨‹åº¦\")\n",
    "print(\"\\nå¼: ln(P(approved|category) / P(rejected|category))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å¤‰æ•°ã®æ‰±ã„\n",
    "\n",
    "### ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®å‰Šæ¸›\n",
    "\n",
    "é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å¤‰æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ãŒå¤šã„ï¼‰ã¯ã€ä»¥ä¸‹ã®æ–¹æ³•ã§æ‰±ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å¤‰æ•°ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆéƒ½é“åºœçœŒï¼‰\n",
    "np.random.seed(42)\n",
    "prefectures = [f'Prefecture_{i:02d}' for i in range(1, 48)]  # 47éƒ½é“åºœçœŒ\n",
    "\n",
    "# Zipfåˆ†å¸ƒï¼ˆä¸€éƒ¨ã®éƒ½é“åºœçœŒã«äººå£é›†ä¸­ï¼‰\n",
    "zipf_probs = 1.0 / np.arange(1, 48)\n",
    "zipf_probs = zipf_probs / zipf_probs.sum()\n",
    "\n",
    "df_high_card = df.copy()\n",
    "df_high_card['prefecture'] = np.random.choice(prefectures, n_samples, p=zipf_probs)\n",
    "\n",
    "print(f\"éƒ½é“åºœçœŒã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {df_high_card['prefecture'].nunique()}\")\n",
    "print(\"\\nä¸Šä½10éƒ½é“åºœçœŒã®é »åº¦:\")\n",
    "print(df_high_card['prefecture'].value_counts().head(10))\n",
    "print(\"\\nä¸‹ä½10éƒ½é“åºœçœŒã®é »åº¦:\")\n",
    "print(df_high_card['prefecture'].value_counts().tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³•1: é »åº¦ã®ä½ã„ã‚«ãƒ†ã‚´ãƒªã‚’ã¾ã¨ã‚ã‚‹\n",
    "def group_rare_categories(df, column, threshold=0.01, rare_label='Other'):\n",
    "    \"\"\"\n",
    "    é »åº¦ã®ä½ã„ã‚«ãƒ†ã‚´ãƒªã‚’'Other'ã«ã¾ã¨ã‚ã‚‹\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    threshold : float\n",
    "        ã“ã®å‰²åˆä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã‚’ã¾ã¨ã‚ã‚‹\n",
    "    \"\"\"\n",
    "    freq = df[column].value_counts(normalize=True)\n",
    "    rare_categories = freq[freq < threshold].index\n",
    "    \n",
    "    df_new = df.copy()\n",
    "    df_new[column] = df_new[column].apply(\n",
    "        lambda x: rare_label if x in rare_categories else x\n",
    "    )\n",
    "    \n",
    "    return df_new, rare_categories\n",
    "\n",
    "df_grouped, rare_prefs = group_rare_categories(df_high_card, 'prefecture', threshold=0.01)\n",
    "\n",
    "print(f\"å…ƒã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {df_high_card['prefecture'].nunique()}\")\n",
    "print(f\"ã‚°ãƒ«ãƒ¼ãƒ—åŒ–å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {df_grouped['prefecture'].nunique()}\")\n",
    "print(f\"'Other'ã«ã¾ã¨ã‚ã‚‰ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªæ•°: {len(rare_prefs)}\")\n",
    "print(f\"\\nã‚°ãƒ«ãƒ¼ãƒ—åŒ–å¾Œã®åˆ†å¸ƒ:\")\n",
    "print(df_grouped['prefecture'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³•2: Target Encodingï¼ˆé«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã«å¼·ã„ï¼‰\n",
    "# ã™ã§ã«å®Ÿè£…æ¸ˆã¿ - Target Encodingã¯é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã§ã‚‚æœ‰åŠ¹\n",
    "\n",
    "# æ–¹æ³•3: Frequency Encoding\n",
    "df_high_card['prefecture_freq'] = frequency_encoding(df_high_card, 'prefecture')\n",
    "\n",
    "print(\"é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å¤‰æ•°ã®å¯¾ç­–æ‰‹æ³•:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. ä½é »åº¦ã‚«ãƒ†ã‚´ãƒªã‚’ã¾ã¨ã‚ã‚‹\")\n",
    "print(\"   - ãƒ¡ãƒªãƒƒãƒˆ: ã‚«ãƒ†ã‚´ãƒªæ•°å‰Šæ¸›\")\n",
    "print(\"   - ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: æƒ…å ±ã®æå¤±\")\n",
    "\n",
    "print(\"\\n2. Target Encoding\")\n",
    "print(\"   - ãƒ¡ãƒªãƒƒãƒˆ: é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã§ã‚‚å˜ä¸€åˆ—\")\n",
    "print(\"   - ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®ãƒªã‚¹ã‚¯\")\n",
    "\n",
    "print(\"\\n3. Frequency Encoding\")\n",
    "print(\"   - ãƒ¡ãƒªãƒƒãƒˆ: ã‚·ãƒ³ãƒ—ãƒ«ã€é«˜é€Ÿ\")\n",
    "print(\"   - ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: ã‚«ãƒ†ã‚´ãƒªã®æ„å‘³ã‚’å¤±ã†\")\n",
    "\n",
    "print(\"\\n4. Embeddingï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰\")\n",
    "print(\"   - ãƒ¡ãƒªãƒƒãƒˆ: è¤‡é›‘ãªé–¢ä¿‚æ€§ã‚’å­¦ç¿’\")\n",
    "print(\"   - ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: å­¦ç¿’ã‚³ã‚¹ãƒˆé«˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•ã®æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•ã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’æ¯”è¼ƒ\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# æ•°å€¤å¤‰æ•°ã®ã¿æŠ½å‡º\n",
    "numeric_cols = ['age', 'income', 'loan_amount']\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Label Encodingï¼ˆæ•™è‚²ãƒ¬ãƒ™ãƒ«ã®ã¿ï¼‰\n",
    "X_label = X.copy()\n",
    "X_label['education'] = X_label['education'].map(education_order)\n",
    "for col in categorical_cols:\n",
    "    if col != 'education':\n",
    "        le = LabelEncoder()\n",
    "        X_label[col] = le.fit_transform(X_label[col])\n",
    "\n",
    "X_train_label, X_test_label, y_train_label, y_test_label = train_test_split(\n",
    "    X_label, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "rf_label = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_label.fit(X_train_label, y_train_label)\n",
    "pred_label = rf_label.predict_proba(X_test_label)[:, 1]\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'Label Encoding',\n",
    "    'Accuracy': accuracy_score(y_test_label, rf_label.predict(X_test_label)),\n",
    "    'ROC-AUC': roc_auc_score(y_test_label, pred_label)\n",
    "})\n",
    "\n",
    "# 2. One-Hot Encoding\n",
    "X_onehot = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "X_train_onehot, X_test_onehot, y_train_onehot, y_test_onehot = train_test_split(\n",
    "    X_onehot, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "rf_onehot = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_onehot.fit(X_train_onehot, y_train_onehot)\n",
    "pred_onehot = rf_onehot.predict_proba(X_test_onehot)[:, 1]\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'One-Hot Encoding',\n",
    "    'Accuracy': accuracy_score(y_test_onehot, rf_onehot.predict(X_test_onehot)),\n",
    "    'ROC-AUC': roc_auc_score(y_test_onehot, pred_onehot)\n",
    "})\n",
    "\n",
    "# 3. Target Encoding\n",
    "rf_target = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_target.fit(X_train_encoded, y_train)\n",
    "pred_target = rf_target.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'Target Encoding',\n",
    "    'Accuracy': accuracy_score(y_test, rf_target.predict(X_test_encoded)),\n",
    "    'ROC-AUC': roc_auc_score(y_test, pred_target)\n",
    "})\n",
    "\n",
    "# 4. CatBoost Encoding\n",
    "rf_catboost = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_catboost.fit(X_train_catboost, y_train)\n",
    "pred_catboost = rf_catboost.predict_proba(X_test_catboost)[:, 1]\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'CatBoost Encoding',\n",
    "    'Accuracy': accuracy_score(y_test, rf_catboost.predict(X_test_catboost)),\n",
    "    'ROC-AUC': roc_auc_score(y_test, pred_catboost)\n",
    "})\n",
    "\n",
    "# 5. WoE Encoding\n",
    "rf_woe = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_woe.fit(X_train_woe, y_train)\n",
    "pred_woe = rf_woe.predict_proba(X_test_woe)[:, 1]\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'WoE Encoding',\n",
    "    'Accuracy': accuracy_score(y_test, rf_woe.predict(X_test_woe)),\n",
    "    'ROC-AUC': roc_auc_score(y_test, pred_woe)\n",
    "})\n",
    "\n",
    "# çµæœã®è¡¨ç¤º\n",
    "results_df = pd.DataFrame(results).sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•ã®æ€§èƒ½æ¯”è¼ƒï¼ˆRandom Forestï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# ãƒ™ã‚¹ãƒˆæ‰‹æ³•\n",
    "best_idx = results_df['ROC-AUC'].idxmax()\n",
    "print(f\"\\nğŸ† ãƒ™ã‚¹ãƒˆæ‰‹æ³•: {results_df.loc[best_idx, 'Encoding']}\")\n",
    "print(f\"   ROC-AUC: {results_df.loc[best_idx, 'ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœã®å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].barh(results_df['Encoding'], results_df['Accuracy'], \n",
    "             alpha=0.7, edgecolor='black', color='skyblue')\n",
    "axes[0].set_xlabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "for i, (idx, row) in enumerate(results_df.iterrows()):\n",
    "    axes[0].text(row['Accuracy'], i, f\"{row['Accuracy']:.4f}\", \n",
    "                va='center', ha='left', fontsize=9)\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1].barh(results_df['Encoding'], results_df['ROC-AUC'], \n",
    "             alpha=0.7, edgecolor='black', color='salmon')\n",
    "axes[1].set_xlabel('ROC-AUC', fontsize=11)\n",
    "axes[1].set_title('ROC-AUC Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "for i, (idx, row) in enumerate(results_df.iterrows()):\n",
    "    axes[1].text(row['ROC-AUC'], i, f\"{row['ROC-AUC']:.4f}\", \n",
    "                va='center', ha='left', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å®Ÿå‹™ã§ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "\n",
    "### ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•ã®é¸æŠã‚¬ã‚¤ãƒ‰\n",
    "\n",
    "| çŠ¶æ³ | æ¨å¥¨æ‰‹æ³• | ç†ç”± |\n",
    "|------|---------|------|\n",
    "| ä½ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ï¼ˆ<10ï¼‰ | One-Hot Encoding | ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹æœçš„ |\n",
    "| ä¸­ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ï¼ˆ10-50ï¼‰ | Target Encoding / WoE | æ¬¡å…ƒã‚’æŠ‘ãˆã¤ã¤åŠ¹æœçš„ |\n",
    "| é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ï¼ˆ>50ï¼‰ | Target/CatBoost Encoding | æ¬¡å…ƒçˆ†ç™ºã‚’å›é¿ |\n",
    "| é †åºå¤‰æ•° | Ordinal Encoding | é †åºæƒ…å ±ã‚’ä¿æŒ |\n",
    "| ãƒ„ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« | Label/Target Encoding | é«˜é€Ÿã€åŠ¹æœçš„ |\n",
    "| ç·šå½¢ãƒ¢ãƒ‡ãƒ« | One-Hot / WoE | ç·šå½¢åˆ†é›¢å¯èƒ½ã« |\n",
    "| ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚° | Embedding | è¤‡é›‘ãªé–¢ä¿‚æ€§ã‚’å­¦ç¿’ |\n",
    "\n",
    "### ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ\n",
    "\n",
    "```python\n",
    "# âœ… ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ\n",
    "\n",
    "# 1. ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª\n",
    "# - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’ç‰¹å®šã—ãŸã‹ï¼Ÿ\n",
    "# - ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ\n",
    "# - æ¬ æå€¤ã¯ã‚ã‚‹ã‹ï¼Ÿ\n",
    "# - é †åºé–¢ä¿‚ã¯ã‚ã‚‹ã‹ï¼Ÿ\n",
    "\n",
    "# 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•ã®é¸æŠ\n",
    "# - ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã«å¿œã˜ãŸæ‰‹æ³•ã‚’é¸ã‚“ã ã‹ï¼Ÿ\n",
    "# - ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ãŸæ‰‹æ³•ã‚’é¸ã‚“ã ã‹ï¼Ÿ\n",
    "# - è¤‡æ•°ã®æ‰‹æ³•ã‚’è©¦ã—ãŸã‹ï¼Ÿ\n",
    "\n",
    "# 3. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®é˜²æ­¢\n",
    "# - Target Encodingã§fit/transformã‚’åˆ†ã‘ãŸã‹ï¼Ÿ\n",
    "# - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’ä½¿ã£ã¦ã„ã‚‹ã‹ï¼Ÿ\n",
    "# - ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§å„Foldã”ã¨ã«fitã—ãŸã‹ï¼Ÿ\n",
    "\n",
    "# 4. é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å¯¾ç­–\n",
    "# - ä½é »åº¦ã‚«ãƒ†ã‚´ãƒªã‚’ã¾ã¨ã‚ãŸã‹ï¼Ÿ\n",
    "# - Target Encodingã‚’æ¤œè¨ã—ãŸã‹ï¼Ÿ\n",
    "# - Embeddingã‚’æ¤œè¨ã—ãŸã‹ï¼Ÿ\n",
    "\n",
    "# 5. æ¤œè¨¼\n",
    "# - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‰å¾Œã§æ€§èƒ½ã‚’æ¯”è¼ƒã—ãŸã‹ï¼Ÿ\n",
    "# - éå­¦ç¿’ã—ã¦ã„ãªã„ã‹ç¢ºèªã—ãŸã‹ï¼Ÿ\n",
    "# - ç‰¹å¾´é‡é‡è¦åº¦ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ\n",
    "```\n",
    "\n",
    "### ã‚ˆãã‚ã‚‹è½ã¨ã—ç©´\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯**: Target Encodingã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’ä½¿ã†\n",
    "2. **æ¬¡å…ƒã®å‘ªã„**: é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã§One-Hot Encoding\n",
    "3. **é †åºã®ç„¡è¦–**: é †åºå¤‰æ•°ã«Label Encodingã‚’ä½¿ã‚ãªã„\n",
    "4. **éå­¦ç¿’**: ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ãªã—ã®Target Encoding\n",
    "5. **æ¬ æå€¤ã®æ”¾ç½®**: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‰ã«å‡¦ç†ã™ã¹ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ã¾ã¨ã‚\n",
    "\n",
    "### æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å­¦ã‚“ã ã“ã¨\n",
    "\n",
    "1. **ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®åŸºç¤**\n",
    "   - åç¾©å¤‰æ•°ã€é †åºå¤‰æ•°ã€äºŒå€¤å¤‰æ•°\n",
    "   - ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®æ¦‚å¿µ\n",
    "   - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å¿…è¦æ€§\n",
    "\n",
    "2. **åŸºæœ¬çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**\n",
    "   - Label Encoding: é †åºå¤‰æ•°å‘ã‘\n",
    "   - One-Hot Encoding: ä½ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å‘ã‘\n",
    "   - Ordinal Encoding: é †åºã‚’ä¿æŒ\n",
    "\n",
    "3. **é«˜åº¦ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**\n",
    "   - Frequency Encoding: ã‚·ãƒ³ãƒ—ãƒ«ã§é«˜é€Ÿ\n",
    "   - Target Encoding: é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã«å¼·ã„\n",
    "   - CatBoost Encoding: Target Encodingã®æ”¹è‰¯ç‰ˆ\n",
    "   - WoE Encoding: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°å‘ã‘\n",
    "\n",
    "4. **é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£å¯¾ç­–**\n",
    "   - ä½é »åº¦ã‚«ãƒ†ã‚´ãƒªã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "   - Target Encoding\n",
    "   - Frequency Encoding\n",
    "   - Embeddingï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰\n",
    "\n",
    "5. **ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®é˜²æ­¢**\n",
    "   - Out-of-Fold Target Encoding\n",
    "   - fit/transformã®åˆ†é›¢\n",
    "   - ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®æ³¨æ„\n",
    "\n",
    "6. **å®Ÿå‹™ã§ã®é©ç”¨**\n",
    "   - çŠ¶æ³åˆ¥ã®æ‰‹æ³•é¸æŠ\n",
    "   - ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ¯”è¼ƒ\n",
    "   - ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "\n",
    "### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "- âœ… **ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã§é¸ã¶**: ä½ãªã‚‰One-Hotã€é«˜ãªã‚‰Target Encoding\n",
    "- âœ… **ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯å³ç¦**: Target Encodingã¯fit/transformã‚’åˆ†é›¢\n",
    "- âœ… **é †åºã‚’è€ƒæ…®**: é †åºå¤‰æ•°ã¯é©åˆ‡ã«æ‰±ã†\n",
    "- âœ… **è¤‡æ•°è©¦ã™**: 1ã¤ã®æ‰‹æ³•ã«å›ºåŸ·ã—ãªã„\n",
    "- âœ… **éå­¦ç¿’æ³¨æ„**: ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã‚„ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ´»ç”¨\n",
    "- âœ… **ãƒ¢ãƒ‡ãƒ«ã¨ã®ç›¸æ€§**: ãƒ„ãƒªãƒ¼ã¨ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§æœ€é©æ‰‹æ³•ãŒç•°ãªã‚‹\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "- Phase 8å®Œäº†ï¼Phase 9ï¼ˆKaggleå®Ÿè·µã‚³ãƒ¼ã‚¹ï¼‰ã¸é€²ã‚€\n",
    "- å®Ÿéš›ã®Kaggleã‚³ãƒ³ãƒšã§ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’æ´»ç”¨\n",
    "- Embeddingã‚’ä½¿ã£ãŸãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ã¶\n",
    "- ã‚ˆã‚Šé«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’æ¢æ±‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
