{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Your First MLP Experiment\n",
    "\n",
    "This notebook gets you started immediately with:\n",
    "- Generating synthetic data\n",
    "- Training an MLP classifier\n",
    "- Exploring parameter space\n",
    "- Visualizing decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "We'll use the `make_moons` dataset - a classic non-linearly separable dataset that's perfect for demonstrating neural network capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate moon-shaped data\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Class 0', alpha=0.6)\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Class 1', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Make Moons Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (important for neural networks!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a simple MLP\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,),  # One hidden layer with 50 neurons\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_score = mlp.score(X_train, y_train)\n",
    "test_score = mlp.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_score:.4f}\")\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")\n",
    "print(f\"\\nNumber of iterations: {mlp.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"Plot the decision boundary of a classifier.\"\"\"\n",
    "    h = 0.02  # Step size in mesh\n",
    "    \n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black', s=50)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(mlp, X_scaled, y, \"MLP Decision Boundary (Single Hidden Layer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore Parameter Space with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,), (20, 10), (50, 25)],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Create MLP for grid search\n",
    "mlp_gs = MLPClassifier(\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    mlp_gs, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"Best Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Score: {grid_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Extract relevant columns\n",
    "results_df = results_df[[\n",
    "    'param_hidden_layer_sizes', \n",
    "    'param_alpha', \n",
    "    'param_learning_rate_init',\n",
    "    'mean_test_score', \n",
    "    'std_test_score',\n",
    "    'mean_train_score',\n",
    "    'rank_test_score'\n",
    "]].sort_values('rank_test_score')\n",
    "\n",
    "print(\"Top 10 Parameter Combinations:\")\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Effect of alpha on accuracy for different architectures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Group by hidden layer sizes\n",
    "for hl_size in param_grid['hidden_layer_sizes']:\n",
    "    mask = results_df['param_hidden_layer_sizes'] == hl_size\n",
    "    subset = results_df[mask].groupby('param_alpha')['mean_test_score'].mean()\n",
    "    axes[0].plot(subset.index, subset.values, marker='o', label=str(hl_size))\n",
    "\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Alpha (L2 Regularization)')\n",
    "axes[0].set_ylabel('Mean CV Accuracy')\n",
    "axes[0].set_title('Effect of Alpha on Accuracy')\n",
    "axes[0].legend(title='Hidden Layers')\n",
    "\n",
    "# Group by alpha\n",
    "for alpha in param_grid['alpha']:\n",
    "    mask = results_df['param_alpha'] == alpha\n",
    "    subset = results_df[mask].groupby('param_hidden_layer_sizes').agg({'mean_test_score': 'mean'}).reset_index()\n",
    "    x_labels = [str(hl) for hl in subset['param_hidden_layer_sizes']]\n",
    "    axes[1].plot(x_labels, subset['mean_test_score'].values, marker='s', label=f'Î±={alpha}')\n",
    "\n",
    "axes[1].set_xlabel('Hidden Layer Sizes')\n",
    "axes[1].set_ylabel('Mean CV Accuracy')\n",
    "axes[1].set_title('Effect of Architecture on Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Model Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary of best model\n",
    "best_model = grid_search.best_estimator_\n",
    "plot_decision_boundary(\n",
    "    best_model, \n",
    "    X_scaled, \n",
    "    y, \n",
    "    f\"Best MLP Decision Boundary\\n{grid_search.best_params_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Architectures Visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different architectures and visualize\n",
    "architectures = [\n",
    "    (10,),\n",
    "    (50,),\n",
    "    (100,),\n",
    "    (50, 25),\n",
    "    (100, 50, 25)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, arch in enumerate(architectures):\n",
    "    # Train model\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=arch,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    axes[idx].scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap=plt.cm.RdYlBu, \n",
    "                      edgecolors='black', s=20)\n",
    "    axes[idx].set_title(f'Architecture: {arch}\\nAccuracy: {model.score(X_test, y_test):.3f}')\n",
    "\n",
    "# Hide empty subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Loss Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and plot loss curve\n",
    "mlp_loss = MLPClassifier(\n",
    "    hidden_layer_sizes=(50, 25),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp_loss.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mlp_loss.loss_curve_, linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MLP Training Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Loss: {mlp_loss.loss_curve_[-1]:.6f}\")\n",
    "print(f\"Test Accuracy: {mlp_loss.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this quick start, you learned:\n",
    "\n",
    "1. **Data Generation**: Using `make_moons` to create non-linear classification data\n",
    "2. **Preprocessing**: Scaling features with `StandardScaler`\n",
    "3. **MLP Training**: Using `MLPClassifier` with various parameters\n",
    "4. **Parameter Exploration**: Using `GridSearchCV` for hyperparameter tuning\n",
    "5. **Visualization**: Decision boundaries and loss curves\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Proceed to **Notebook 01** for deeper data simulation\n",
    "- Try different datasets: `make_circles`, `make_classification`\n",
    "- Experiment with more parameters: `activation`, `solver`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
