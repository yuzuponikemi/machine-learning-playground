{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 63. NeRF入門 - Neural Radiance Fields\n",
    "\n",
    "**難易度**: ★★★★★（上級）\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックを完了すると、以下ができるようになります：\n",
    "\n",
    "- [ ] NeRFの基本アーキテクチャを理解する\n",
    "- [ ] 位置エンコーディングの役割を説明できる\n",
    "- [ ] 階層的サンプリング（Coarse-to-Fine）の仕組みを理解する\n",
    "- [ ] シンプルなNeRFを実装できる\n",
    "- [ ] NeRFの発展形（Instant-NGP, 3DGS等）への橋渡しを理解する\n",
    "\n",
    "## 前提知識\n",
    "\n",
    "- Ray Castingと座標系（Notebook 61）\n",
    "- ボリュームレンダリング（Notebook 62）\n",
    "- ニューラルネットワークの基礎（MLP）\n",
    "- PyTorchの基本操作\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. [NeRFとは](#1-nerfとは)\n",
    "2. [NeRFのアーキテクチャ](#2-nerfのアーキテクチャ)\n",
    "3. [位置エンコーディング](#3-位置エンコーディング)\n",
    "4. [階層的サンプリング](#4-階層的サンプリング)\n",
    "5. [実装（NumPyベース）](#5-実装numpyベース)\n",
    "6. [発展的なトピック](#6-発展的なトピック)\n",
    "7. [まとめとセルフチェック](#7-まとめとセルフチェック)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from typing import Tuple, List, Optional, Callable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(\"ライブラリの読み込み完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. NeRFとは\n",
    "\n",
    "### 1.1 Neural Radiance Fields の概要\n",
    "\n",
    "**NeRF (Neural Radiance Fields)** は、2020年にMildenhallらによって提案された、ニューラルネットワークを使って3Dシーンを表現・レンダリングする手法です。\n",
    "\n",
    "**キーアイデア**: シーン全体を1つのニューラルネットワーク（MLP）で表現し、任意の位置・方向からの色と密度を出力する。\n",
    "\n",
    "$$\n",
    "F_\\theta: (\\mathbf{x}, \\mathbf{d}) \\rightarrow (\\mathbf{c}, \\sigma)\n",
    "$$\n",
    "\n",
    "- 入力: 3D位置 $\\mathbf{x} = (x, y, z)$ と視線方向 $\\mathbf{d} = (\\theta, \\phi)$\n",
    "- 出力: RGB色 $\\mathbf{c} = (r, g, b)$ と体積密度 $\\sigma$\n",
    "\n",
    "### 1.2 NeRFの革新性\n",
    "\n",
    "| 従来手法 | NeRF |\n",
    "|---------|------|\n",
    "| 明示的な3D表現（メッシュ、点群） | 暗黙的な連続表現 |\n",
    "| 離散的 | 任意の解像度 |\n",
    "| 表面のみ | 半透明・霧も表現可能 |\n",
    "| ビュー依存効果が困難 | 視線依存の色を自然に学習 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeRFの概念図\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# 左: 従来の3D表現\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.set_title('従来: 明示的3D表現\\n（メッシュ/点群）', fontsize=11)\n",
    "\n",
    "# 立方体のワイヤーフレーム\n",
    "vertices = np.array([\n",
    "    [0,0,0], [1,0,0], [1,1,0], [0,1,0],\n",
    "    [0,0,1], [1,0,1], [1,1,1], [0,1,1]\n",
    "])\n",
    "edges = [[0,1],[1,2],[2,3],[3,0],[4,5],[5,6],[6,7],[7,4],\n",
    "         [0,4],[1,5],[2,6],[3,7]]\n",
    "for e in edges:\n",
    "    ax1.plot3D(*zip(vertices[e[0]], vertices[e[1]]), 'b-', linewidth=2)\n",
    "ax1.scatter(*vertices.T, c='blue', s=50)\n",
    "ax1.set_xlabel('X'); ax1.set_ylabel('Y'); ax1.set_zlabel('Z')\n",
    "\n",
    "# 中央: NeRFの入出力\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.set_title('NeRF: ニューラル表現\\nF(x,d) → (c, σ)', fontsize=11)\n",
    "\n",
    "# MLP図\n",
    "layers = [6, 8, 8, 8, 4]\n",
    "x_pos = np.linspace(0.1, 0.9, len(layers))\n",
    "colors_layer = ['green', 'purple', 'purple', 'purple', 'orange']\n",
    "for i, (x, n, col) in enumerate(zip(x_pos, layers, colors_layer)):\n",
    "    y_positions = np.linspace(0.2, 0.8, n)\n",
    "    ax2.scatter([x]*n, y_positions, s=100, c=col, alpha=0.8)\n",
    "    if i < len(layers) - 1:\n",
    "        for y1 in y_positions:\n",
    "            y_next = np.linspace(0.2, 0.8, layers[i+1])\n",
    "            for y2 in y_next[:3]:  # 一部だけ描画\n",
    "                ax2.plot([x, x_pos[i+1]], [y1, y2], 'gray', alpha=0.1)\n",
    "\n",
    "ax2.text(0.05, 0.5, '(x,y,z)\\n(θ,φ)', ha='right', va='center', fontsize=10, color='green')\n",
    "ax2.text(0.95, 0.6, '(r,g,b)', ha='left', va='center', fontsize=10, color='orange')\n",
    "ax2.text(0.95, 0.4, 'σ', ha='left', va='center', fontsize=12, color='orange')\n",
    "ax2.set_xlim(0, 1); ax2.set_ylim(0, 1)\n",
    "ax2.axis('off')\n",
    "\n",
    "# 右: レンダリング結果\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.set_title('出力: フォトリアルな画像', fontsize=11)\n",
    "\n",
    "# グラデーション画像（NeRF出力のイメージ）\n",
    "img = np.zeros((50, 50, 3))\n",
    "for i in range(50):\n",
    "    for j in range(50):\n",
    "        r = np.sqrt((i-25)**2 + (j-25)**2)\n",
    "        if r < 20:\n",
    "            # 球のような見た目\n",
    "            depth = np.sqrt(max(0, 20**2 - r**2))\n",
    "            normal_z = depth / 20\n",
    "            img[i, j] = [0.3 + 0.5*normal_z, 0.2 + 0.3*normal_z, 0.6 + 0.3*normal_z]\n",
    "        else:\n",
    "            img[i, j] = [0.9, 0.9, 0.95]  # 背景\n",
    "ax3.imshow(img)\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 NeRFのパイプライン\n",
    "\n",
    "```\n",
    "1. レイ生成: 各ピクセルに対してカメラからレイを生成\n",
    "2. サンプリング: レイに沿って点をサンプル\n",
    "3. 位置エンコーディング: 座標を高次元に変換\n",
    "4. MLPクエリ: 各点で(色, 密度)を取得\n",
    "5. ボリュームレンダリング: 色を統合してピクセル値を計算\n",
    "6. 損失計算: Ground Truthとの誤差を計算\n",
    "7. 逆伝播: ネットワークを更新\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeRFパイプラインの詳細図\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# 1. 入力画像\n",
    "ax = axes[0, 0]\n",
    "ax.set_title('1. 入力: 多視点画像', fontsize=11)\n",
    "for i, angle in enumerate([0, 30, 60]):\n",
    "    rect = plt.Rectangle((0.1 + i*0.28, 0.3), 0.22, 0.4,\n",
    "                         facecolor=f'C{i}', alpha=0.7, edgecolor='black')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(0.21 + i*0.28, 0.25, f'視点{i+1}', ha='center', fontsize=9)\n",
    "ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "# 2. レイ生成\n",
    "ax = axes[0, 1]\n",
    "ax.set_title('2. レイ生成', fontsize=11)\n",
    "ax.scatter([0.15], [0.5], s=150, c='blue', marker='o', label='カメラ')\n",
    "for angle in np.linspace(-0.25, 0.25, 5):\n",
    "    ax.arrow(0.15, 0.5, 0.7, angle, head_width=0.03, head_length=0.04,\n",
    "             fc='red', ec='red', alpha=0.7)\n",
    "ax.text(0.5, 0.1, 'r(t) = o + td', ha='center', fontsize=10)\n",
    "ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "# 3. サンプリング\n",
    "ax = axes[0, 2]\n",
    "ax.set_title('3. 点サンプリング', fontsize=11)\n",
    "x_samples = np.linspace(0.15, 0.85, 10)\n",
    "ax.plot([0.1, 0.9], [0.5, 0.5], 'r-', linewidth=2)\n",
    "ax.scatter(x_samples, [0.5]*10, c='green', s=60, zorder=5)\n",
    "ax.text(0.5, 0.3, 't₁, t₂, ..., tₙ', ha='center', fontsize=10)\n",
    "ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "# 4. 位置エンコーディング + MLP\n",
    "ax = axes[1, 0]\n",
    "ax.set_title('4. 位置エンコーディング + MLP', fontsize=11)\n",
    "ax.text(0.1, 0.7, 'x', fontsize=12, ha='center')\n",
    "ax.arrow(0.15, 0.7, 0.15, 0, head_width=0.05, head_length=0.03, fc='black')\n",
    "ax.text(0.42, 0.7, 'γ(x)', fontsize=10, ha='center', \n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "ax.arrow(0.55, 0.7, 0.15, 0, head_width=0.05, head_length=0.03, fc='black')\n",
    "ax.text(0.82, 0.7, 'MLP', fontsize=10, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='purple', alpha=0.3))\n",
    "ax.text(0.5, 0.35, 'γ(x) = [sin(2⁰πx), cos(2⁰πx), ...]', ha='center', fontsize=9)\n",
    "ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "# 5. ボリュームレンダリング\n",
    "ax = axes[1, 1]\n",
    "ax.set_title('5. ボリュームレンダリング', fontsize=11)\n",
    "colors_vol = plt.cm.viridis(np.linspace(0.2, 0.8, 8))\n",
    "x_pos = np.linspace(0.1, 0.7, 8)\n",
    "for x, c in zip(x_pos, colors_vol):\n",
    "    ax.add_patch(plt.Rectangle((x, 0.4), 0.07, 0.2, facecolor=c, alpha=0.7))\n",
    "ax.arrow(0.75, 0.5, 0.1, 0, head_width=0.05, head_length=0.03, fc='black')\n",
    "ax.add_patch(plt.Rectangle((0.87, 0.42), 0.08, 0.16, facecolor='orange', edgecolor='black'))\n",
    "ax.text(0.5, 0.25, 'C = Σ Tᵢαᵢcᵢ', ha='center', fontsize=10)\n",
    "ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "# 6. 損失と学習\n",
    "ax = axes[1, 2]\n",
    "ax.set_title('6. 損失計算と学習', fontsize=11)\n",
    "ax.text(0.25, 0.7, 'Ĉ', fontsize=14, ha='center')\n",
    "ax.text(0.5, 0.7, '-', fontsize=20, ha='center')\n",
    "ax.text(0.75, 0.7, 'C_gt', fontsize=12, ha='center')\n",
    "ax.text(0.5, 0.5, '↓', fontsize=20, ha='center')\n",
    "ax.text(0.5, 0.35, 'L = ||Ĉ - C_gt||²', ha='center', fontsize=11,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. NeRFのアーキテクチャ\n",
    "\n",
    "### 2.1 ネットワーク構造\n",
    "\n",
    "NeRFのMLPは以下の特徴を持ちます：\n",
    "\n",
    "1. **密度 σ は位置のみに依存**: 物体の形状は視点に依存しない\n",
    "2. **色 c は位置と方向の両方に依存**: 反射・光沢などのビュー依存効果\n",
    "\n",
    "```\n",
    "入力: γ(x) (位置エンコーディング)\n",
    "  ↓\n",
    "8層 MLP (256 units, ReLU)\n",
    "  ↓ (5層目でスキップ接続)\n",
    "出力1: σ (密度) + 特徴ベクトル\n",
    "  ↓ 特徴ベクトル + γ(d) (方向)\n",
    "1層 MLP (128 units)\n",
    "  ↓\n",
    "出力2: c (RGB色)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeRFアーキテクチャの詳細図\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# レイヤーの描画\n",
    "def draw_layer(ax, x, y, width, height, label, color='lightblue'):\n",
    "    rect = plt.Rectangle((x-width/2, y-height/2), width, height,\n",
    "                         facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, label, ha='center', va='center', fontsize=9)\n",
    "\n",
    "# 入力\n",
    "ax.text(0.02, 0.7, 'x\\n(3D位置)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "ax.text(0.02, 0.3, 'd\\n(方向)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 位置エンコーディング\n",
    "draw_layer(ax, 0.1, 0.7, 0.06, 0.15, 'γ(x)\\n60D', 'yellow')\n",
    "draw_layer(ax, 0.1, 0.3, 0.06, 0.1, 'γ(d)\\n24D', 'yellow')\n",
    "\n",
    "# MLP層\n",
    "layers_x = np.linspace(0.2, 0.65, 8)\n",
    "for i, x in enumerate(layers_x):\n",
    "    draw_layer(ax, x, 0.7, 0.05, 0.2, f'256\\nReLU', 'lightblue')\n",
    "    if i == 4:  # スキップ接続\n",
    "        ax.annotate('', xy=(x-0.025, 0.55), xytext=(0.1, 0.62),\n",
    "                   arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
    "        ax.text(0.22, 0.57, 'skip', fontsize=8, color='green')\n",
    "\n",
    "# 密度出力\n",
    "draw_layer(ax, 0.72, 0.8, 0.05, 0.08, 'σ', 'orange')\n",
    "draw_layer(ax, 0.72, 0.65, 0.05, 0.12, '256D\\n特徴', 'lightgreen')\n",
    "\n",
    "# 方向との結合\n",
    "ax.annotate('', xy=(0.78, 0.45), xytext=(0.72, 0.59),\n",
    "           arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "ax.annotate('', xy=(0.78, 0.45), xytext=(0.13, 0.3),\n",
    "           arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "\n",
    "# 色出力用MLP\n",
    "draw_layer(ax, 0.82, 0.45, 0.05, 0.12, '128\\nReLU', 'lightblue')\n",
    "draw_layer(ax, 0.9, 0.45, 0.05, 0.1, 'RGB', 'orange')\n",
    "\n",
    "# 矢印\n",
    "for i in range(len(layers_x)-1):\n",
    "    ax.annotate('', xy=(layers_x[i+1]-0.025, 0.7), xytext=(layers_x[i]+0.025, 0.7),\n",
    "               arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "\n",
    "ax.annotate('', xy=(0.69, 0.7), xytext=(0.67, 0.7),\n",
    "           arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "ax.annotate('', xy=(0.87, 0.45), xytext=(0.85, 0.45),\n",
    "           arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "\n",
    "# タイトル\n",
    "ax.text(0.5, 0.95, 'NeRF ネットワークアーキテクチャ', ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 注釈\n",
    "ax.text(0.5, 0.05, '密度σは位置xのみに依存 / 色cは位置xと方向dの両方に依存', \n",
    "        ha='center', fontsize=10, style='italic')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 なぜこの構造が効果的か\n",
    "\n",
    "1. **密度と色の分離**: \n",
    "   - 密度 σ: 物体の「存在」を表す（視点に依存しない）\n",
    "   - 色 c: 見え方を表す（視点で変化する反射など）\n",
    "\n",
    "2. **スキップ接続**: 勾配消失を防ぎ、高周波の詳細を保持\n",
    "\n",
    "3. **深いネットワーク**: 複雑なシーンを表現するのに十分な容量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 位置エンコーディング\n",
    "\n",
    "### 3.1 なぜ位置エンコーディングが必要か\n",
    "\n",
    "MLPは本質的に **低周波バイアス（spectral bias）** を持ち、滑らかな関数しか学習できません。\n",
    "\n",
    "位置エンコーディングにより、高周波成分を明示的に入力に含めることで、細かいディテールを学習可能にします。\n",
    "\n",
    "### 3.2 数学的定義\n",
    "\n",
    "位置 $p$ に対する位置エンコーディング $\\gamma(p)$:\n",
    "\n",
    "$$\n",
    "\\gamma(p) = \\left( \\sin(2^0\\pi p), \\cos(2^0\\pi p), \\sin(2^1\\pi p), \\cos(2^1\\pi p), ..., \\sin(2^{L-1}\\pi p), \\cos(2^{L-1}\\pi p) \\right)\n",
    "$$\n",
    "\n",
    "- **NeRFでの設定**: 位置 L=10 (60次元)、方向 L=4 (24次元)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(x: np.ndarray, L: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    NeRFスタイルの位置エンコーディング\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : np.ndarray, shape (..., D)\n",
    "        入力座標\n",
    "    L : int\n",
    "        周波数の数（出力次元 = 入力次元 × 2 × L）\n",
    "    \"\"\"\n",
    "    encodings = []\n",
    "    for i in range(L):\n",
    "        freq = 2.0 ** i * np.pi\n",
    "        encodings.append(np.sin(freq * x))\n",
    "        encodings.append(np.cos(freq * x))\n",
    "    return np.concatenate(encodings, axis=-1)\n",
    "\n",
    "# テスト\n",
    "x_test = np.array([0.5, 0.3, 0.7])  # 3D座標\n",
    "encoded = positional_encoding(x_test, L=10)\n",
    "\n",
    "print(f\"入力次元: {len(x_test)}\")\n",
    "print(f\"L = 10 での出力次元: {len(encoded)}\")\n",
    "print(f\"計算: 3 × 2 × 10 = {3 * 2 * 10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置エンコーディングの効果を可視化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1D関数: 高周波のステップ関数\n",
    "x = np.linspace(0, 1, 500)\n",
    "y_true = np.where((x > 0.3) & (x < 0.7), 1.0, 0.0) + \\\n",
    "         0.3 * np.sin(20 * np.pi * x)  # 高周波成分\n",
    "\n",
    "# 左上: 目標関数\n",
    "ax = axes[0, 0]\n",
    "ax.plot(x, y_true, 'b-', linewidth=2)\n",
    "ax.set_title('目標関数（高周波成分を含む）')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 右上: 位置エンコーディングなしのMLP出力（概念的）\n",
    "ax = axes[0, 1]\n",
    "# 低周波フィルタをかけた近似\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "y_low_freq = gaussian_filter1d(y_true, sigma=30)\n",
    "ax.plot(x, y_true, 'b-', linewidth=1, alpha=0.5, label='目標')\n",
    "ax.plot(x, y_low_freq, 'r-', linewidth=2, label='PE無しMLP出力')\n",
    "ax.set_title('位置エンコーディングなし\\n（低周波バイアス）')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 左下: 位置エンコーディングの各周波数成分\n",
    "ax = axes[1, 0]\n",
    "L = 6\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, L))\n",
    "for i in range(L):\n",
    "    freq = 2**i * np.pi\n",
    "    ax.plot(x, np.sin(freq * x), color=colors[i], \n",
    "            label=f'sin(2^{i}πx)', linewidth=1.5, alpha=0.8)\n",
    "ax.set_title(f'位置エンコーディングの成分 (L={L})')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 右下: 位置エンコーディングありのMLP出力（概念的）\n",
    "ax = axes[1, 1]\n",
    "# 高周波も再現できる近似\n",
    "y_with_pe = y_true + np.random.normal(0, 0.02, len(y_true))  # 少しノイズを加えて表現\n",
    "ax.plot(x, y_true, 'b-', linewidth=1, alpha=0.5, label='目標')\n",
    "ax.plot(x, y_with_pe, 'g-', linewidth=1.5, label='PE有りMLP出力')\n",
    "ax.set_title('位置エンコーディングあり\\n（高周波も学習可能）')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 階層的サンプリング\n",
    "\n",
    "### 4.1 Coarse-to-Fine戦略\n",
    "\n",
    "NeRFは2段階のサンプリングを使用します：\n",
    "\n",
    "1. **Coarse（粗い）ネットワーク**: \n",
    "   - $N_c$ 個の均等なサンプル点\n",
    "   - シーン全体の大まかな構造を把握\n",
    "\n",
    "2. **Fine（細かい）ネットワーク**: \n",
    "   - Coarseの出力（重み）に基づいて重要な領域を特定\n",
    "   - 追加の $N_f$ 個のサンプルを重要な領域に集中\n",
    "\n",
    "### 4.2 重要度サンプリング\n",
    "\n",
    "Coarseネットワークの出力から計算された重み $w_i = T_i \\alpha_i$ を確率分布として使用：\n",
    "\n",
    "$$\n",
    "\\hat{w}_i = \\frac{w_i}{\\sum_j w_j}\n",
    "$$\n",
    "\n",
    "この分布から追加のサンプル点を生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pdf(bins: np.ndarray, weights: np.ndarray, \n",
    "               n_samples: int, det: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    重み付き分布から逆CDF法でサンプリング\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    bins : np.ndarray, shape (N+1,)\n",
    "        ビンの境界\n",
    "    weights : np.ndarray, shape (N,)\n",
    "        各ビンの重み\n",
    "    n_samples : int\n",
    "        生成するサンプル数\n",
    "    det : bool\n",
    "        決定的サンプリングを使用するか\n",
    "    \"\"\"\n",
    "    weights = weights + 1e-5  # ゼロ除算防止\n",
    "    pdf = weights / np.sum(weights)\n",
    "    cdf = np.cumsum(pdf)\n",
    "    cdf = np.concatenate([[0], cdf])\n",
    "    \n",
    "    if det:\n",
    "        u = np.linspace(0, 1, n_samples)\n",
    "    else:\n",
    "        u = np.random.uniform(size=n_samples)\n",
    "    \n",
    "    # 逆CDF\n",
    "    inds = np.searchsorted(cdf, u, side='right') - 1\n",
    "    inds = np.clip(inds, 0, len(weights) - 1)\n",
    "    \n",
    "    below = bins[inds]\n",
    "    above = bins[inds + 1]\n",
    "    t = (u - cdf[inds]) / (cdf[inds + 1] - cdf[inds] + 1e-5)\n",
    "    \n",
    "    return below + t * (above - below)\n",
    "\n",
    "# デモ\n",
    "near, far = 2.0, 6.0\n",
    "n_coarse = 64\n",
    "n_fine = 128\n",
    "\n",
    "# Coarseサンプル（均等）\n",
    "t_coarse = np.linspace(near, far, n_coarse)\n",
    "\n",
    "# 仮の重み（物体が t=3.5 付近にあると仮定）\n",
    "weights_coarse = np.exp(-((t_coarse - 3.5) / 0.5)**2)\n",
    "\n",
    "# Fineサンプル（重要度に基づく）\n",
    "bins = np.linspace(near, far, n_coarse + 1)\n",
    "t_fine = sample_pdf(bins, weights_coarse, n_fine)\n",
    "\n",
    "# 結合してソート\n",
    "t_all = np.sort(np.concatenate([t_coarse, t_fine]))\n",
    "\n",
    "print(f\"Coarseサンプル数: {len(t_coarse)}\")\n",
    "print(f\"Fineサンプル数: {len(t_fine)}\")\n",
    "print(f\"合計: {len(t_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 階層的サンプリングの可視化\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# 1. Coarseサンプルと重み\n",
    "ax = axes[0]\n",
    "ax.bar(t_coarse, weights_coarse, width=0.05, alpha=0.7, color='blue', label='Coarse重み')\n",
    "ax.scatter(t_coarse, np.zeros_like(t_coarse) - 0.05, c='blue', s=30, marker='^', label='Coarseサンプル')\n",
    "ax.axvline(3.5, color='gray', linestyle='--', alpha=0.5, label='物体位置')\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('重み')\n",
    "ax.set_title('Step 1: Coarseネットワーク → 重み計算')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlim(near - 0.2, far + 0.2)\n",
    "\n",
    "# 2. 重みに基づくFineサンプリング\n",
    "ax = axes[1]\n",
    "pdf = weights_coarse / np.sum(weights_coarse)\n",
    "ax.bar(t_coarse, pdf, width=0.05, alpha=0.7, color='orange', label='PDF')\n",
    "ax.scatter(t_fine, np.zeros_like(t_fine) - 0.01, c='red', s=20, marker='v', \n",
    "           alpha=0.5, label='Fineサンプル')\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('確率密度')\n",
    "ax.set_title('Step 2: 重みからPDFを計算 → Fineサンプリング')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlim(near - 0.2, far + 0.2)\n",
    "\n",
    "# 3. サンプル密度の比較\n",
    "ax = axes[2]\n",
    "# ヒストグラムでサンプル密度を可視化\n",
    "hist_bins = np.linspace(near, far, 30)\n",
    "ax.hist(t_coarse, bins=hist_bins, alpha=0.5, color='blue', label='Coarse', density=True)\n",
    "ax.hist(t_fine, bins=hist_bins, alpha=0.5, color='red', label='Fine', density=True)\n",
    "ax.hist(t_all, bins=hist_bins, alpha=0.3, color='green', label='結合', density=True)\n",
    "ax.axvline(3.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('サンプル密度')\n",
    "ax.set_title('Step 3: サンプル密度の比較')\n",
    "ax.legend()\n",
    "ax.set_xlim(near - 0.2, far + 0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n観察:\")\n",
    "print(\"- Coarseサンプル: 全範囲を均等にカバー\")\n",
    "print(\"- Fineサンプル: 物体周辺（重みが高い領域）に集中\")\n",
    "print(\"- 結合: 重要な領域を高密度でサンプリング\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 実装（NumPyベース）\n",
    "\n",
    "### 5.1 シンプルなNeRF実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeRF:\n",
    "    \"\"\"\n",
    "    NumPyベースのシンプルなNeRF実装（学習は省略、推論のみ）\n",
    "    実際のNeRFはPyTorch/JAXで実装されます。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pos_L: int = 10, dir_L: int = 4):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pos_L : int\n",
    "            位置の位置エンコーディング周波数数\n",
    "        dir_L : int  \n",
    "            方向の位置エンコーディング周波数数\n",
    "        \"\"\"\n",
    "        self.pos_L = pos_L\n",
    "        self.dir_L = dir_L\n",
    "        \n",
    "        # 入力次元\n",
    "        self.pos_dim = 3 * 2 * pos_L  # 60\n",
    "        self.dir_dim = 3 * 2 * dir_L  # 24\n",
    "        \n",
    "        # ランダムな重み（デモ用、実際は学習で獲得）\n",
    "        self._init_random_weights()\n",
    "    \n",
    "    def _init_random_weights(self):\n",
    "        \"\"\"ランダムな重みを初期化（デモ用）\"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(self.pos_dim, 64) * 0.1\n",
    "        self.W2 = np.random.randn(64, 64) * 0.1\n",
    "        self.W_sigma = np.random.randn(64, 1) * 0.1\n",
    "        self.W_feat = np.random.randn(64, 32) * 0.1\n",
    "        self.W_color = np.random.randn(32 + self.dir_dim, 3) * 0.1\n",
    "    \n",
    "    def positional_encoding(self, x: np.ndarray, L: int) -> np.ndarray:\n",
    "        \"\"\"位置エンコーディング\"\"\"\n",
    "        encodings = []\n",
    "        for i in range(L):\n",
    "            freq = 2.0 ** i * np.pi\n",
    "            encodings.append(np.sin(freq * x))\n",
    "            encodings.append(np.cos(freq * x))\n",
    "        return np.concatenate(encodings, axis=-1)\n",
    "    \n",
    "    def forward(self, positions: np.ndarray, directions: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        positions : np.ndarray, shape (N, 3)\n",
    "        directions : np.ndarray, shape (N, 3)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        sigmas : np.ndarray, shape (N,)\n",
    "        colors : np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        # 位置エンコーディング\n",
    "        pos_enc = self.positional_encoding(positions, self.pos_L)\n",
    "        dir_enc = self.positional_encoding(directions, self.dir_L)\n",
    "        \n",
    "        # MLP層（簡略化）\n",
    "        h = np.maximum(0, pos_enc @ self.W1)  # ReLU\n",
    "        h = np.maximum(0, h @ self.W2)\n",
    "        \n",
    "        # 密度出力\n",
    "        sigma_raw = h @ self.W_sigma\n",
    "        sigmas = np.maximum(0, sigma_raw.flatten())  # ReLU (正の値のみ)\n",
    "        \n",
    "        # 特徴ベクトル\n",
    "        features = h @ self.W_feat\n",
    "        \n",
    "        # 方向と結合して色を出力\n",
    "        combined = np.concatenate([features, dir_enc], axis=-1)\n",
    "        colors_raw = combined @ self.W_color\n",
    "        colors = 1 / (1 + np.exp(-colors_raw))  # Sigmoid\n",
    "        \n",
    "        return sigmas, colors\n",
    "\n",
    "# インスタンス作成\n",
    "nerf = SimpleNeRF(pos_L=6, dir_L=4)\n",
    "print(f\"位置エンコーディング次元: {nerf.pos_dim}\")\n",
    "print(f\"方向エンコーディング次元: {nerf.dir_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_render(sigmas: np.ndarray, colors: np.ndarray, \n",
    "                  deltas: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    ボリュームレンダリング\n",
    "    \"\"\"\n",
    "    alphas = 1.0 - np.exp(-sigmas * deltas)\n",
    "    \n",
    "    # 累積透過率\n",
    "    T = np.ones(len(sigmas))\n",
    "    for i in range(1, len(sigmas)):\n",
    "        T[i] = T[i-1] * (1 - alphas[i-1])\n",
    "    \n",
    "    weights = T * alphas\n",
    "    color = np.sum(weights[:, None] * colors, axis=0)\n",
    "    \n",
    "    return color, weights, T\n",
    "\n",
    "def render_rays(nerf: SimpleNeRF, origins: np.ndarray, directions: np.ndarray,\n",
    "                near: float, far: float, n_samples: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    レイをレンダリング\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    origins : np.ndarray, shape (N_rays, 3)\n",
    "    directions : np.ndarray, shape (N_rays, 3)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    colors : np.ndarray, shape (N_rays, 3)\n",
    "    \"\"\"\n",
    "    N_rays = len(origins)\n",
    "    colors = np.zeros((N_rays, 3))\n",
    "    \n",
    "    # サンプリング\n",
    "    t_vals = np.linspace(near, far, n_samples)\n",
    "    deltas = np.ones(n_samples) * (far - near) / n_samples\n",
    "    \n",
    "    for i in range(N_rays):\n",
    "        # レイ上のサンプル点\n",
    "        points = origins[i] + t_vals[:, None] * directions[i]\n",
    "        dirs = np.broadcast_to(directions[i], points.shape)\n",
    "        \n",
    "        # NeRFクエリ\n",
    "        sigmas, rgbs = nerf.forward(points, dirs)\n",
    "        \n",
    "        # レンダリング\n",
    "        color, _, _ = volume_render(sigmas, rgbs, deltas)\n",
    "        colors[i] = color\n",
    "    \n",
    "    return colors\n",
    "\n",
    "# テスト: 少数のレイをレンダリング\n",
    "test_origins = np.array([[0, 0, 0], [0, 0, 0]])\n",
    "test_directions = np.array([[0, 0, 1], [0.1, 0, 1]])\n",
    "test_directions = test_directions / np.linalg.norm(test_directions, axis=1, keepdims=True)\n",
    "\n",
    "colors = render_rays(nerf, test_origins, test_directions, near=2.0, far=6.0, n_samples=64)\n",
    "print(f\"レンダリング結果: {colors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小さい画像をレンダリング\n",
    "def render_image_nerf(nerf: SimpleNeRF, H: int, W: int, focal: float,\n",
    "                      near: float, far: float, n_samples: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    NeRFで画像をレンダリング\n",
    "    \"\"\"\n",
    "    image = np.zeros((H, W, 3))\n",
    "    \n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            # ピクセル座標からレイ方向\n",
    "            x = (j - W/2) / focal\n",
    "            y = (i - H/2) / focal\n",
    "            direction = np.array([x, y, 1.0])\n",
    "            direction = direction / np.linalg.norm(direction)\n",
    "            \n",
    "            origin = np.array([0, 0, 0])\n",
    "            \n",
    "            # レンダリング\n",
    "            color = render_rays(nerf, origin[None], direction[None],\n",
    "                               near, far, n_samples)[0]\n",
    "            image[i, j] = np.clip(color, 0, 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "print(\"NeRFでの画像レンダリング中...\")\n",
    "# 非常に小さい画像でデモ（計算負荷のため）\n",
    "image_nerf = render_image_nerf(nerf, H=32, W=32, focal=30,\n",
    "                                near=2.0, far=6.0, n_samples=32)\n",
    "print(\"完了!\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.imshow(image_nerf)\n",
    "ax.set_title('NeRFレンダリング結果 (ランダム重み)\\n32×32, 32サンプル/レイ')\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n注意: これはランダムな重みを使用しているため、意味のある画像にはなりません。\")\n",
    "print(\"実際のNeRFでは、多視点画像から学習した重みを使用します。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 NeRF学習の擬似コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "==================== NeRF学習の擬似コード (PyTorch) ====================\n",
    "\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self, pos_L=10, dir_L=4, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoder(L=pos_L)\n",
    "        self.dir_encoder = PositionalEncoder(L=dir_L)\n",
    "        \n",
    "        # 8層MLP for density\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(pos_L * 6, hidden_dim),\n",
    "            *[nn.Linear(hidden_dim, hidden_dim) for _ in range(7)]\n",
    "        ])\n",
    "        \n",
    "        # 密度と特徴出力\n",
    "        self.sigma_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.feature_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # 色出力用\n",
    "        self.color_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + dir_L * 6, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, pos, dir):\n",
    "        pos_enc = self.pos_encoder(pos)\n",
    "        dir_enc = self.dir_encoder(dir)\n",
    "        \n",
    "        h = pos_enc\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = F.relu(layer(h))\n",
    "            if i == 4:  # Skip connection\n",
    "                h = torch.cat([h, pos_enc], dim=-1)\n",
    "        \n",
    "        sigma = F.relu(self.sigma_layer(h))\n",
    "        feature = self.feature_layer(h)\n",
    "        \n",
    "        color = self.color_layer(torch.cat([feature, dir_enc], dim=-1))\n",
    "        \n",
    "        return sigma, color\n",
    "\n",
    "# 学習ループ\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        rays_o, rays_d, target_rgb = batch\n",
    "        \n",
    "        # Coarseネットワーク\n",
    "        t_coarse = stratified_sample(near, far, n_coarse)\n",
    "        points_coarse = rays_o + t_coarse * rays_d\n",
    "        sigma_c, color_c = model_coarse(points_coarse, rays_d)\n",
    "        rgb_coarse, weights_c = volume_render(sigma_c, color_c, t_coarse)\n",
    "        \n",
    "        # Fineネットワーク（階層的サンプリング）\n",
    "        t_fine = sample_pdf(t_coarse, weights_c, n_fine)\n",
    "        t_all = torch.sort(torch.cat([t_coarse, t_fine]))\n",
    "        points_fine = rays_o + t_all * rays_d\n",
    "        sigma_f, color_f = model_fine(points_fine, rays_d)\n",
    "        rgb_fine, _ = volume_render(sigma_f, color_f, t_all)\n",
    "        \n",
    "        # 損失\n",
    "        loss = mse_loss(rgb_coarse, target_rgb) + mse_loss(rgb_fine, target_rgb)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "========================================================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 発展的なトピック\n",
    "\n",
    "### 6.1 NeRFの課題と後続研究\n",
    "\n",
    "| 課題 | 解決策 |\n",
    "|-----|-------|\n",
    "| 学習が遅い（数日） | Instant-NGP: ハッシュエンコーディング |\n",
    "| 推論が遅い | Plenoxels, TensoRF: 明示的な格子表現 |\n",
    "| 動的シーン非対応 | D-NeRF, Nerfies: 変形フィールド |\n",
    "| 制限された品質 | Mip-NeRF: マルチスケール |\n",
    "\n",
    "### 6.2 3D Gaussian Splatting (3DGS)\n",
    "\n",
    "2023年に登場した新しい手法で、NeRFの暗黙表現に代わり、**3Dガウシアン**を明示的に使用：\n",
    "\n",
    "- **表現**: 数百万個の3Dガウシアン（位置、共分散、色、不透明度）\n",
    "- **レンダリング**: ラスタライゼーションベース（非常に高速）\n",
    "- **学習**: 微分可能なスプラッティング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeRFファミリーの進化\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# タイムライン\n",
    "methods = [\n",
    "    (2020, 'NeRF', '暗黙表現\\nMLP', 'blue'),\n",
    "    (2021, 'Mip-NeRF', 'マルチスケール', 'green'),\n",
    "    (2021, 'NeRF++', 'unbounded', 'green'),\n",
    "    (2022, 'Instant-NGP', 'ハッシュ\\nエンコーディング', 'orange'),\n",
    "    (2022, 'Plenoxels', '明示的格子', 'orange'),\n",
    "    (2022, 'TensoRF', 'テンソル分解', 'orange'),\n",
    "    (2023, '3D Gaussian\\nSplatting', 'ガウシアン\\nスプラッティング', 'red'),\n",
    "]\n",
    "\n",
    "for year, name, desc, color in methods:\n",
    "    x = year - 2019.5\n",
    "    y = 0.5 + 0.3 * np.random.rand()\n",
    "    ax.scatter([x], [y], s=300, c=color, zorder=5)\n",
    "    ax.annotate(name, (x, y + 0.1), ha='center', fontsize=10, fontweight='bold')\n",
    "    ax.annotate(desc, (x, y - 0.15), ha='center', fontsize=8, style='italic')\n",
    "\n",
    "# 矢印\n",
    "ax.annotate('', xy=(4, 0.55), xytext=(0.5, 0.55),\n",
    "           arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "\n",
    "ax.set_xlim(0, 4.5)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xticks([0.5, 1.5, 2.5, 3.5])\n",
    "ax.set_xticklabels(['2020', '2021', '2022', '2023'])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('NeRFファミリーの進化', fontsize=14)\n",
    "ax.text(4.2, 0.55, '高速化\\n高品質化', ha='left', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 実用的なリソース\n",
    "\n",
    "**公式実装・チュートリアル**:\n",
    "- NeRF公式: github.com/bmild/nerf\n",
    "- Nerfstudio: nerfstudio.github.io (様々なNeRF手法の統一実装)\n",
    "- 3DGS: github.com/graphdeco-inria/gaussian-splatting\n",
    "\n",
    "**学習データセット**:\n",
    "- LLFF（forward-facing scenes）\n",
    "- NeRF Synthetic（合成シーン）\n",
    "- Mip-NeRF 360（360度シーン）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. まとめとセルフチェック\n",
    "\n",
    "### 学習内容の要約\n",
    "\n",
    "| コンポーネント | 役割 |\n",
    "|--------------|------|\n",
    "| MLP | シーンを暗黙的に表現 F(x,d)→(c,σ) |\n",
    "| 位置エンコーディング | 高周波の詳細を学習可能に |\n",
    "| 階層的サンプリング | 効率的なサンプル配置 |\n",
    "| ボリュームレンダリング | 微分可能な画像生成 |\n",
    "\n",
    "### 重要な数式\n",
    "\n",
    "$$\n",
    "\\text{NeRF}: F_\\theta(\\gamma(\\mathbf{x}), \\gamma(\\mathbf{d})) \\rightarrow (\\mathbf{c}, \\sigma)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{位置エンコーディング}: \\gamma(p) = (\\sin(2^0\\pi p), \\cos(2^0\\pi p), ..., \\sin(2^{L-1}\\pi p), \\cos(2^{L-1}\\pi p))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{レンダリング}: C = \\sum_i T_i \\alpha_i \\mathbf{c}_i, \\quad \\alpha_i = 1 - \\exp(-\\sigma_i \\delta_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# セルフチェッククイズ\n",
    "print(\"=\"*60)\n",
    "print(\"セルフチェッククイズ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "questions = [\n",
    "    {\n",
    "        \"q\": \"Q1: NeRFのMLPの入力は何か？\",\n",
    "        \"options\": [\"a) RGB画像\", \"b) 3D点群\", \"c) 位置(x,y,z)と視線方向(θ,φ)\", \"d) 深度マップ\"],\n",
    "        \"answer\": \"c\",\n",
    "        \"explanation\": \"NeRFは位置と視線方向を入力とし、その点での色と密度を出力します。\"\n",
    "    },\n",
    "    {\n",
    "        \"q\": \"Q2: 位置エンコーディングを使う主な理由は？\",\n",
    "        \"options\": [\"a) 次元削減\", \"b) ノイズ除去\", \"c) MLPの低周波バイアスを克服\", \"d) 計算高速化\"],\n",
    "        \"answer\": \"c\",\n",
    "        \"explanation\": \"MLPは本質的に低周波関数しか学習できないため、位置エンコーディングで高周波成分を明示的に与えます。\"\n",
    "    },\n",
    "    {\n",
    "        \"q\": \"Q3: 密度σと色cの入力依存性の違いは？\",\n",
    "        \"options\": [\"a) 両方とも位置のみ\", \"b) 両方とも方向のみ\", \n",
    "                   \"c) σは位置のみ、cは位置と方向\", \"d) σは方向のみ、cは位置のみ\"],\n",
    "        \"answer\": \"c\",\n",
    "        \"explanation\": \"密度（物体の存在）は視点に依存しませんが、色（見え方）は反射などにより視線方向で変化します。\"\n",
    "    },\n",
    "    {\n",
    "        \"q\": \"Q4: 階層的サンプリングでFineサンプルはどこに集中するか？\",\n",
    "        \"options\": [\"a) レイの始点付近\", \"b) レイの終点付近\", \n",
    "                   \"c) Coarseの重みが高い領域\", \"d) ランダムな位置\"],\n",
    "        \"answer\": \"c\",\n",
    "        \"explanation\": \"Coarseネットワークの出力（重み）に基づいて、物体がありそうな領域に集中的にサンプルします。\"\n",
    "    },\n",
    "    {\n",
    "        \"q\": \"Q5: 3D Gaussian Splattingの主な利点は？\",\n",
    "        \"options\": [\"a) より高い画質\", \"b) リアルタイムレンダリングが可能\", \n",
    "                   \"c) 学習データが少なくて済む\", \"d) 動的シーンのサポート\"],\n",
    "        \"answer\": \"b\",\n",
    "        \"explanation\": \"3DGSはラスタライゼーションベースのレンダリングを使用するため、NeRFよりも大幅に高速（リアルタイム）です。\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{q['q']}\")\n",
    "    for opt in q['options']:\n",
    "        print(f\"  {opt}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"解答は下のセルを実行してください\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答表示\n",
    "print(\"解答と解説:\")\n",
    "print(\"=\"*60)\n",
    "for i, q in enumerate(questions):\n",
    "    print(f\"\\nQ{i+1}: 正解は {q['answer']}\")\n",
    "    print(f\"   {q['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit 0.3 完了\n",
    "\n",
    "このノートブックで、3Dコンピュータビジョンの基礎からNeRFまでの学習が完了しました！\n",
    "\n",
    "**学習した内容（Unit 0.3 全体）**:\n",
    "\n",
    "1. **Phase 1 (50-52)**: 光学と画像形成\n",
    "   - 光学の基礎\n",
    "   - ピンホールカメラモデル\n",
    "   - レンズ歪みと補正\n",
    "\n",
    "2. **Phase 2 (53-54)**: 座標系と変換\n",
    "   - 3D座標変換と剛体運動\n",
    "   - カメラキャリブレーション\n",
    "\n",
    "3. **Phase 3 (55-57)**: マルチビュー幾何\n",
    "   - エピポーラ幾何\n",
    "   - ステレオ視と視差\n",
    "   - 三角測量と3D復元\n",
    "\n",
    "4. **Phase 4 (58-60)**: 3D再構成\n",
    "   - 特徴点検出とマッチング\n",
    "   - SfMパイプライン\n",
    "   - バンドル調整\n",
    "\n",
    "5. **Phase 5 (61-63)**: ニューラルレンダリングへの橋渡し\n",
    "   - Ray Castingと座標系\n",
    "   - ボリュームレンダリング\n",
    "   - **NeRF入門** ← 現在のノートブック\n",
    "\n",
    "---\n",
    "\n",
    "**次のステップ**:\n",
    "- 実際のNeRF実装（nerfstudio等）を試す\n",
    "- 3D Gaussian Splattingを学ぶ\n",
    "- 4Dビジョン（動的シーン）に進む\n",
    "\n",
    "---\n",
    "\n",
    "**ナビゲーション**\n",
    "\n",
    "[← 62. ボリュームレンダリング](./62_volume_rendering_v1.ipynb) | [Unit 0.3 完了!]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
