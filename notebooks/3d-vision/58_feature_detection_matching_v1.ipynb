{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 58. 特徴点検出とマッチング\n",
    "## Feature Detection and Matching\n",
    "\n",
    "---\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックを完了すると、以下ができるようになります：\n",
    "\n",
    "- [ ] 特徴点検出の基本原理（コーナー検出）を理解する\n",
    "- [ ] Harris コーナー検出を実装できる\n",
    "- [ ] スケール不変性の概念と SIFT の原理を理解する\n",
    "- [ ] 特徴量記述子の役割と種類を説明できる\n",
    "- [ ] 特徴点マッチングアルゴリズムを実装できる\n",
    "- [ ] RANSAC による外れ値除去を適用できる\n",
    "\n",
    "---\n",
    "\n",
    "## 前提知識\n",
    "\n",
    "- 画像処理の基礎（畳み込み、勾配）\n",
    "- 55: エピポーラ幾何の理論（マッチング検証用）\n",
    "- 線形代数：固有値、SVD\n",
    "\n",
    "**難易度**: ★★★☆☆（中級）  \n",
    "**推定学習時間**: 90-120分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 特徴点とは\n",
    "\n",
    "**特徴点（Feature Point / Keypoint）**は、画像内で他の領域と区別可能な特徴的な点です。\n",
    "\n",
    "### 良い特徴点の条件\n",
    "\n",
    "| 性質 | 説明 |\n",
    "|------|------|\n",
    "| **繰り返し検出可能** | 異なる画像でも同じ物理的な点を検出 |\n",
    "| **識別可能** | 周囲の情報で一意に特定できる |\n",
    "| **局所性** | 小さな領域の情報で検出・記述 |\n",
    "| **不変性** | 回転、スケール、照明変化に対してロバスト |\n",
    "\n",
    "### 特徴点の種類\n",
    "\n",
    "- **コーナー**: 2方向に強いエッジ（Harris, Shi-Tomasi）\n",
    "- **ブロブ**: 周囲と輝度が異なる領域（SIFT, SURF）\n",
    "- **エッジ**: 1方向に強い勾配（特徴点としては弱い）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from typing import Tuple, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_image(size: int = 256) -> np.ndarray:\n",
    "    \"\"\"テスト用の合成画像を生成\"\"\"\n",
    "    image = np.ones((size, size)) * 128\n",
    "    \n",
    "    # 長方形（エッジとコーナー）\n",
    "    image[50:150, 50:150] = 200\n",
    "    \n",
    "    # 小さな正方形\n",
    "    image[180:210, 180:210] = 50\n",
    "    \n",
    "    # 円（ブロブ）\n",
    "    y, x = np.ogrid[:size, :size]\n",
    "    mask = (x - 200)**2 + (y - 80)**2 < 25**2\n",
    "    image[mask] = 220\n",
    "    \n",
    "    # テクスチャ領域\n",
    "    np.random.seed(42)\n",
    "    texture = np.random.randn(60, 60) * 30\n",
    "    image[60:120, 170:230] += texture\n",
    "    \n",
    "    # ガウシアンブラーで滑らかに\n",
    "    image = gaussian_filter(image, sigma=1)\n",
    "    \n",
    "    return image.astype(np.float32)\n",
    "\n",
    "# テスト画像の生成と表示\n",
    "test_image = create_test_image()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_image, cmap='gray')\n",
    "plt.title('Test Image for Feature Detection')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Harris コーナー検出\n",
    "\n",
    "### 2.1 基本アイデア\n",
    "\n",
    "画像パッチを少しずらしたときの変化を調べます：\n",
    "\n",
    "- **平坦領域**: どの方向にずらしても変化なし\n",
    "- **エッジ**: エッジに沿った方向では変化なし、垂直方向では変化大\n",
    "- **コーナー**: どの方向にずらしても変化大\n",
    "\n",
    "### 2.2 数学的定式化\n",
    "\n",
    "変位 $(u, v)$ に対する二乗誤差和（SSD）：\n",
    "\n",
    "$$E(u, v) = \\sum_{x, y} w(x, y) [I(x+u, y+v) - I(x, y)]^2$$\n",
    "\n",
    "テイラー展開で近似：\n",
    "\n",
    "$$E(u, v) \\approx \\begin{pmatrix} u & v \\end{pmatrix} \\mathbf{M} \\begin{pmatrix} u \\\\ v \\end{pmatrix}$$\n",
    "\n",
    "### 2.3 構造テンソル（Second Moment Matrix）\n",
    "\n",
    "$$\\mathbf{M} = \\sum_{x, y} w(x, y) \\begin{pmatrix} I_x^2 & I_x I_y \\\\ I_x I_y & I_y^2 \\end{pmatrix} = \\begin{pmatrix} A & B \\\\ B & C \\end{pmatrix}$$\n",
    "\n",
    "### 2.4 Harris レスポンス関数\n",
    "\n",
    "$$R = \\det(\\mathbf{M}) - k \\cdot \\text{trace}(\\mathbf{M})^2 = \\lambda_1 \\lambda_2 - k(\\lambda_1 + \\lambda_2)^2$$\n",
    "\n",
    "- $R > 0$（大）: コーナー\n",
    "- $R < 0$（大）: エッジ\n",
    "- $|R|$ 小: 平坦領域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_gradients(image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"画像の勾配を計算（Sobel演算子）\"\"\"\n",
    "    # Sobelカーネル\n",
    "    sobel_x = np.array([[-1, 0, 1],\n",
    "                        [-2, 0, 2],\n",
    "                        [-1, 0, 1]]) / 8.0\n",
    "    \n",
    "    sobel_y = np.array([[-1, -2, -1],\n",
    "                        [0, 0, 0],\n",
    "                        [1, 2, 1]]) / 8.0\n",
    "    \n",
    "    Ix = ndimage.convolve(image.astype(float), sobel_x)\n",
    "    Iy = ndimage.convolve(image.astype(float), sobel_y)\n",
    "    \n",
    "    return Ix, Iy\n",
    "\n",
    "def harris_corner_detection(image: np.ndarray, \n",
    "                             k: float = 0.04,\n",
    "                             window_size: int = 5,\n",
    "                             threshold: float = 0.01) -> Tuple[np.ndarray, List[Tuple[int, int]]]:\n",
    "    \"\"\"Harris コーナー検出\n",
    "    \n",
    "    Args:\n",
    "        image: 入力画像（グレースケール）\n",
    "        k: Harris レスポンスのパラメータ（通常 0.04-0.06）\n",
    "        window_size: ガウシアン窓のサイズ\n",
    "        threshold: コーナー判定の閾値（最大レスポンスに対する割合）\n",
    "    \n",
    "    Returns:\n",
    "        R: Harris レスポンスマップ\n",
    "        corners: コーナー座標のリスト [(y, x), ...]\n",
    "    \"\"\"\n",
    "    # 1. 勾配の計算\n",
    "    Ix, Iy = compute_image_gradients(image)\n",
    "    \n",
    "    # 2. 構造テンソルの要素\n",
    "    Ixx = Ix ** 2\n",
    "    Iyy = Iy ** 2\n",
    "    Ixy = Ix * Iy\n",
    "    \n",
    "    # 3. ガウシアン重みで平滑化\n",
    "    sigma = window_size / 6.0\n",
    "    A = gaussian_filter(Ixx, sigma=sigma)\n",
    "    B = gaussian_filter(Ixy, sigma=sigma)\n",
    "    C = gaussian_filter(Iyy, sigma=sigma)\n",
    "    \n",
    "    # 4. Harris レスポンスの計算\n",
    "    det_M = A * C - B ** 2\n",
    "    trace_M = A + C\n",
    "    R = det_M - k * (trace_M ** 2)\n",
    "    \n",
    "    # 5. Non-Maximum Suppression\n",
    "    R_max = ndimage.maximum_filter(R, size=window_size)\n",
    "    R_nms = (R == R_max) & (R > threshold * R.max())\n",
    "    \n",
    "    # 6. コーナー座標の抽出\n",
    "    corners = list(zip(*np.where(R_nms)))\n",
    "    \n",
    "    return R, corners\n",
    "\n",
    "print(\"Harris コーナー検出の実装完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harris コーナー検出の実行\n",
    "harris_response, corners = harris_corner_detection(test_image, k=0.04, threshold=0.01)\n",
    "\n",
    "print(f\"検出されたコーナー数: {len(corners)}\")\n",
    "\n",
    "# 結果の可視化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 元画像\n",
    "axes[0].imshow(test_image, cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Harris レスポンス\n",
    "im = axes[1].imshow(harris_response, cmap='jet')\n",
    "axes[1].set_title('Harris Response')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "\n",
    "# コーナー検出結果\n",
    "axes[2].imshow(test_image, cmap='gray')\n",
    "for y, x in corners:\n",
    "    axes[2].plot(x, y, 'r.', markersize=8)\n",
    "axes[2].set_title(f'Detected Corners ({len(corners)} points)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_harris_eigenvalues():\n",
    "    \"\"\"Harris 検出の固有値空間での解釈\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # 固有値空間\n",
    "    lambda1 = np.linspace(0, 1, 100)\n",
    "    lambda2 = np.linspace(0, 1, 100)\n",
    "    L1, L2 = np.meshgrid(lambda1, lambda2)\n",
    "    \n",
    "    # Harris レスポンス\n",
    "    k = 0.04\n",
    "    R = L1 * L2 - k * (L1 + L2) ** 2\n",
    "    \n",
    "    # プロット\n",
    "    contour = ax.contourf(L1, L2, R, levels=50, cmap='RdBu_r')\n",
    "    ax.contour(L1, L2, R, levels=[0], colors='black', linewidths=2)\n",
    "    \n",
    "    plt.colorbar(contour, label='Harris Response R')\n",
    "    \n",
    "    # 領域のラベル\n",
    "    ax.text(0.1, 0.1, 'Flat\\n(R ≈ 0)', fontsize=12, ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    ax.text(0.8, 0.1, 'Edge\\n(R < 0)', fontsize=12, ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    ax.text(0.1, 0.8, 'Edge\\n(R < 0)', fontsize=12, ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    ax.text(0.8, 0.8, 'Corner\\n(R > 0)', fontsize=12, ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel('λ₁ (First Eigenvalue)', fontsize=12)\n",
    "    ax.set_ylabel('λ₂ (Second Eigenvalue)', fontsize=12)\n",
    "    ax.set_title('Harris Response in Eigenvalue Space', fontsize=14)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_harris_eigenvalues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. スケール不変特徴点（SIFT の概要）\n",
    "\n",
    "### 3.1 スケール不変性の必要性\n",
    "\n",
    "Harris コーナー検出は**スケール変化に弱い**です。同じオブジェクトでも、画像のスケールが変わると検出位置が変わってしまいます。\n",
    "\n",
    "### 3.2 SIFT (Scale-Invariant Feature Transform)\n",
    "\n",
    "David Lowe が2004年に提案した、スケールと回転に不変な特徴点検出・記述手法。\n",
    "\n",
    "#### ステップ1: スケール空間の構築\n",
    "\n",
    "$$L(x, y, \\sigma) = G(x, y, \\sigma) * I(x, y)$$\n",
    "\n",
    "異なるスケール $\\sigma$ でガウシアンブラーを適用。\n",
    "\n",
    "#### ステップ2: DoG (Difference of Gaussian)\n",
    "\n",
    "$$D(x, y, \\sigma) = L(x, y, k\\sigma) - L(x, y, \\sigma)$$\n",
    "\n",
    "DoG はラプラシアン・オブ・ガウシアン（LoG）の近似で、ブロブ検出に有効。\n",
    "\n",
    "#### ステップ3: 極値検出\n",
    "\n",
    "DoG 空間で $(x, y, \\sigma)$ の26近傍（同スケール8点 + 上下スケール各9点）で極値を検出。\n",
    "\n",
    "#### ステップ4: キーポイントの精緻化\n",
    "\n",
    "サブピクセル精度で位置を補正し、低コントラスト点やエッジ上の点を除去。\n",
    "\n",
    "#### ステップ5: 方向の割り当て\n",
    "\n",
    "キーポイント周辺の勾配ヒストグラムから主方向を決定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gaussian_pyramid(image: np.ndarray, \n",
    "                            n_octaves: int = 4, \n",
    "                            n_scales: int = 5,\n",
    "                            sigma: float = 1.6) -> List[List[np.ndarray]]:\n",
    "    \"\"\"ガウシアンピラミッドの構築\"\"\"\n",
    "    k = 2 ** (1.0 / (n_scales - 3))  # スケール間の比率\n",
    "    \n",
    "    pyramid = []\n",
    "    current_image = image.copy()\n",
    "    \n",
    "    for octave in range(n_octaves):\n",
    "        octave_images = []\n",
    "        current_sigma = sigma\n",
    "        \n",
    "        for scale in range(n_scales):\n",
    "            if scale == 0 and octave > 0:\n",
    "                # オクターブ間でダウンサンプリング\n",
    "                blurred = gaussian_filter(current_image, sigma=current_sigma)\n",
    "            else:\n",
    "                blurred = gaussian_filter(current_image, sigma=current_sigma)\n",
    "            \n",
    "            octave_images.append(blurred)\n",
    "            current_sigma *= k\n",
    "        \n",
    "        pyramid.append(octave_images)\n",
    "        \n",
    "        # 次のオクターブ用にダウンサンプリング\n",
    "        current_image = octave_images[-3][::2, ::2]  # 2倍ダウンサンプリング\n",
    "    \n",
    "    return pyramid\n",
    "\n",
    "def build_dog_pyramid(gaussian_pyramid: List[List[np.ndarray]]) -> List[List[np.ndarray]]:\n",
    "    \"\"\"Difference of Gaussian (DoG) ピラミッドの構築\"\"\"\n",
    "    dog_pyramid = []\n",
    "    \n",
    "    for octave_images in gaussian_pyramid:\n",
    "        dog_octave = []\n",
    "        for i in range(len(octave_images) - 1):\n",
    "            dog = octave_images[i + 1] - octave_images[i]\n",
    "            dog_octave.append(dog)\n",
    "        dog_pyramid.append(dog_octave)\n",
    "    \n",
    "    return dog_pyramid\n",
    "\n",
    "# ピラミッドの構築と可視化\n",
    "gaussian_pyr = build_gaussian_pyramid(test_image, n_octaves=3, n_scales=5)\n",
    "dog_pyr = build_dog_pyramid(gaussian_pyr)\n",
    "\n",
    "# 最初のオクターブを可視化\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for i, img in enumerate(gaussian_pyr[0]):\n",
    "    axes[0, i].imshow(img, cmap='gray')\n",
    "    axes[0, i].set_title(f'Gaussian σ={i}')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "for i, img in enumerate(dog_pyr[0]):\n",
    "    axes[1, i].imshow(img, cmap='RdBu_r')\n",
    "    axes[1, i].set_title(f'DoG {i}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[1, 4].axis('off')\n",
    "\n",
    "plt.suptitle('Gaussian Pyramid and Difference of Gaussian (First Octave)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dog_extrema(dog_pyramid: List[List[np.ndarray]], \n",
    "                        threshold: float = 0.03) -> List[Tuple[int, int, int, int]]:\n",
    "    \"\"\"DoG 空間での極値検出（簡略版）\n",
    "    \n",
    "    Returns:\n",
    "        keypoints: [(octave, scale, y, x), ...]\n",
    "    \"\"\"\n",
    "    keypoints = []\n",
    "    \n",
    "    for octave_idx, dog_octave in enumerate(dog_pyramid):\n",
    "        for scale_idx in range(1, len(dog_octave) - 1):\n",
    "            current = dog_octave[scale_idx]\n",
    "            prev = dog_octave[scale_idx - 1]\n",
    "            next_scale = dog_octave[scale_idx + 1]\n",
    "            \n",
    "            h, w = current.shape\n",
    "            \n",
    "            for y in range(1, h - 1):\n",
    "                for x in range(1, w - 1):\n",
    "                    val = current[y, x]\n",
    "                    \n",
    "                    if abs(val) < threshold:\n",
    "                        continue\n",
    "                    \n",
    "                    # 26近傍チェック\n",
    "                    neighbors = []\n",
    "                    for dy in range(-1, 2):\n",
    "                        for dx in range(-1, 2):\n",
    "                            neighbors.append(prev[y + dy, x + dx])\n",
    "                            if dy != 0 or dx != 0:\n",
    "                                neighbors.append(current[y + dy, x + dx])\n",
    "                            neighbors.append(next_scale[y + dy, x + dx])\n",
    "                    \n",
    "                    # 極値判定\n",
    "                    if val > 0 and val >= max(neighbors):\n",
    "                        keypoints.append((octave_idx, scale_idx, y, x))\n",
    "                    elif val < 0 and val <= min(neighbors):\n",
    "                        keypoints.append((octave_idx, scale_idx, y, x))\n",
    "    \n",
    "    return keypoints\n",
    "\n",
    "# 極値検出\n",
    "sift_keypoints = detect_dog_extrema(dog_pyr, threshold=0.02)\n",
    "print(f\"検出されたキーポイント数: {len(sift_keypoints)}\")\n",
    "\n",
    "# 可視化（最初のオクターブのみ）\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(test_image, cmap='gray')\n",
    "\n",
    "for octave, scale, y, x in sift_keypoints:\n",
    "    if octave == 0:\n",
    "        # スケールに応じた円のサイズ\n",
    "        radius = 3 * (scale + 1)\n",
    "        circle = plt.Circle((x, y), radius, fill=False, color='red', linewidth=1)\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "ax.set_title(f'SIFT-like Keypoints (Octave 0, {sum(1 for k in sift_keypoints if k[0]==0)} points)')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 特徴量記述子\n",
    "\n",
    "### 4.1 記述子の役割\n",
    "\n",
    "**特徴量記述子（Descriptor）**は、キーポイント周辺の情報をベクトル化したもの。マッチングに使用します。\n",
    "\n",
    "### 4.2 SIFT 記述子\n",
    "\n",
    "1. キーポイント周辺を 16×16 ピクセルの領域に分割\n",
    "2. 各 4×4 セルで勾配方向ヒストグラム（8方向）を計算\n",
    "3. 4×4×8 = 128 次元ベクトル\n",
    "4. 正規化して照明変化に対するロバスト性を確保\n",
    "\n",
    "### 4.3 その他の記述子\n",
    "\n",
    "| 記述子 | 次元 | 特徴 |\n",
    "|--------|------|------|\n",
    "| **SIFT** | 128 | 高精度、計算コスト高 |\n",
    "| **SURF** | 64/128 | SIFT より高速 |\n",
    "| **ORB** | 256 (binary) | 非常に高速、バイナリ |\n",
    "| **BRIEF** | 256 (binary) | シンプル、高速 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_histogram(image: np.ndarray, \n",
    "                                y: int, x: int,\n",
    "                                window_size: int = 16,\n",
    "                                n_bins: int = 8) -> np.ndarray:\n",
    "    \"\"\"勾配方向ヒストグラムを計算（簡略版SIFT記述子）\"\"\"\n",
    "    half = window_size // 2\n",
    "    \n",
    "    # 境界チェック\n",
    "    h, w = image.shape\n",
    "    if y - half < 0 or y + half >= h or x - half < 0 or x + half >= w:\n",
    "        return None\n",
    "    \n",
    "    # パッチの抽出\n",
    "    patch = image[y - half:y + half, x - half:x + half]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    Ix, Iy = compute_image_gradients(patch)\n",
    "    \n",
    "    # 勾配の大きさと方向\n",
    "    magnitude = np.sqrt(Ix**2 + Iy**2)\n",
    "    orientation = np.arctan2(Iy, Ix)  # -π to π\n",
    "    \n",
    "    # 4x4 セルに分割\n",
    "    cell_size = window_size // 4\n",
    "    descriptor = []\n",
    "    \n",
    "    for cy in range(4):\n",
    "        for cx in range(4):\n",
    "            cell_mag = magnitude[cy*cell_size:(cy+1)*cell_size,\n",
    "                                  cx*cell_size:(cx+1)*cell_size]\n",
    "            cell_ori = orientation[cy*cell_size:(cy+1)*cell_size,\n",
    "                                    cx*cell_size:(cx+1)*cell_size]\n",
    "            \n",
    "            # ヒストグラム\n",
    "            hist, _ = np.histogram(cell_ori.flatten(), \n",
    "                                    bins=n_bins, \n",
    "                                    range=(-np.pi, np.pi),\n",
    "                                    weights=cell_mag.flatten())\n",
    "            descriptor.extend(hist)\n",
    "    \n",
    "    descriptor = np.array(descriptor)\n",
    "    \n",
    "    # 正規化\n",
    "    norm = np.linalg.norm(descriptor)\n",
    "    if norm > 1e-6:\n",
    "        descriptor = descriptor / norm\n",
    "    \n",
    "    # クリッピングと再正規化（SIFT の手法）\n",
    "    descriptor = np.clip(descriptor, 0, 0.2)\n",
    "    norm = np.linalg.norm(descriptor)\n",
    "    if norm > 1e-6:\n",
    "        descriptor = descriptor / norm\n",
    "    \n",
    "    return descriptor\n",
    "\n",
    "def compute_descriptors(image: np.ndarray, \n",
    "                        keypoints: List[Tuple]) -> Tuple[List, np.ndarray]:\n",
    "    \"\"\"全キーポイントの記述子を計算\"\"\"\n",
    "    descriptors = []\n",
    "    valid_keypoints = []\n",
    "    \n",
    "    for kp in keypoints:\n",
    "        if len(kp) == 4:  # SIFT-like (octave, scale, y, x)\n",
    "            octave, scale, y, x = kp\n",
    "            # オクターブに応じてスケールアップ\n",
    "            y_scaled = y * (2 ** octave)\n",
    "            x_scaled = x * (2 ** octave)\n",
    "        else:  # Harris (y, x)\n",
    "            y_scaled, x_scaled = kp\n",
    "        \n",
    "        desc = compute_gradient_histogram(image, int(y_scaled), int(x_scaled))\n",
    "        \n",
    "        if desc is not None:\n",
    "            descriptors.append(desc)\n",
    "            valid_keypoints.append((y_scaled, x_scaled))\n",
    "    \n",
    "    return valid_keypoints, np.array(descriptors)\n",
    "\n",
    "# 記述子の計算\n",
    "kps, descs = compute_descriptors(test_image, corners)\n",
    "print(f\"記述子を計算したキーポイント: {len(kps)}\")\n",
    "print(f\"記述子の次元: {descs.shape[1] if len(descs) > 0 else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_descriptor(image: np.ndarray, y: int, x: int):\n",
    "    \"\"\"記述子の可視化\"\"\"\n",
    "    desc = compute_gradient_histogram(image, y, x)\n",
    "    \n",
    "    if desc is None:\n",
    "        print(\"キーポイントが画像境界に近すぎます\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 画像とキーポイント\n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].plot(x, y, 'r+', markersize=20, markeredgewidth=3)\n",
    "    rect = plt.Rectangle((x-8, y-8), 16, 16, fill=False, color='red', linewidth=2)\n",
    "    axes[0].add_patch(rect)\n",
    "    axes[0].set_title(f'Keypoint at ({x}, {y})')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # 16x16 パッチ\n",
    "    patch = image[y-8:y+8, x-8:x+8]\n",
    "    axes[1].imshow(patch, cmap='gray')\n",
    "    # 4x4 グリッド\n",
    "    for i in range(1, 4):\n",
    "        axes[1].axhline(y=i*4-0.5, color='red', linewidth=1)\n",
    "        axes[1].axvline(x=i*4-0.5, color='red', linewidth=1)\n",
    "    axes[1].set_title('16×16 Patch with 4×4 Grid')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # 記述子\n",
    "    desc_reshaped = desc.reshape(4, 4, 8)\n",
    "    for cy in range(4):\n",
    "        for cx in range(4):\n",
    "            hist = desc_reshaped[cy, cx]\n",
    "            # 極座標で表示\n",
    "            angles = np.linspace(-np.pi, np.pi, 9)[:-1]\n",
    "            for i, (angle, mag) in enumerate(zip(angles, hist)):\n",
    "                if mag > 0.01:\n",
    "                    center_x = cx + 0.5\n",
    "                    center_y = cy + 0.5\n",
    "                    dx = mag * np.cos(angle) * 0.4\n",
    "                    dy = mag * np.sin(angle) * 0.4\n",
    "                    axes[2].arrow(center_x, center_y, dx, dy, \n",
    "                                  head_width=0.05, head_length=0.02, \n",
    "                                  fc='blue', ec='blue')\n",
    "    \n",
    "    axes[2].set_xlim(0, 4)\n",
    "    axes[2].set_ylim(4, 0)\n",
    "    axes[2].set_aspect('equal')\n",
    "    axes[2].grid(True)\n",
    "    axes[2].set_title('Descriptor (Gradient Histograms)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# コーナーの1つで記述子を可視化\n",
    "if len(corners) > 0:\n",
    "    # 画像中央に近いコーナーを選択\n",
    "    center_y, center_x = test_image.shape[0] // 2, test_image.shape[1] // 2\n",
    "    valid_corners = [(y, x) for y, x in corners \n",
    "                     if 20 < y < test_image.shape[0]-20 and 20 < x < test_image.shape[1]-20]\n",
    "    if valid_corners:\n",
    "        y, x = valid_corners[0]\n",
    "        visualize_descriptor(test_image, y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 特徴点マッチング\n",
    "\n",
    "### 5.1 最近傍マッチング\n",
    "\n",
    "記述子間の距離（ユークリッド距離 or ハミング距離）が最小のペアをマッチとします。\n",
    "\n",
    "### 5.2 Lowe's Ratio Test\n",
    "\n",
    "誤マッチを減らすため、最近傍と2番目に近い点の距離比を使用：\n",
    "\n",
    "$$\\frac{d_1}{d_2} < \\text{ratio\\_threshold}$$\n",
    "\n",
    "通常、ratio_threshold = 0.7-0.8 を使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_descriptors(desc1: np.ndarray, desc2: np.ndarray,\n",
    "                       ratio_threshold: float = 0.75) -> List[Tuple[int, int, float]]:\n",
    "    \"\"\"記述子のマッチング（Lowe's ratio test）\n",
    "    \n",
    "    Args:\n",
    "        desc1: 画像1の記述子 (N1, D)\n",
    "        desc2: 画像2の記述子 (N2, D)\n",
    "        ratio_threshold: Lowe's ratio test の閾値\n",
    "    \n",
    "    Returns:\n",
    "        matches: [(idx1, idx2, distance), ...]\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    for i, d1 in enumerate(desc1):\n",
    "        # 全記述子との距離を計算\n",
    "        distances = np.linalg.norm(desc2 - d1, axis=1)\n",
    "        \n",
    "        # 最近傍と2番目に近いものを取得\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        \n",
    "        if len(sorted_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        best_idx = sorted_indices[0]\n",
    "        second_idx = sorted_indices[1]\n",
    "        \n",
    "        best_dist = distances[best_idx]\n",
    "        second_dist = distances[second_idx]\n",
    "        \n",
    "        # Lowe's ratio test\n",
    "        if second_dist > 1e-6 and best_dist / second_dist < ratio_threshold:\n",
    "            matches.append((i, best_idx, best_dist))\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def cross_check_matches(matches_12: List[Tuple], \n",
    "                        matches_21: List[Tuple]) -> List[Tuple]:\n",
    "    \"\"\"双方向マッチングによる検証\"\"\"\n",
    "    # 逆方向マッチの辞書を作成\n",
    "    reverse_matches = {m[0]: m[1] for m in matches_21}\n",
    "    \n",
    "    verified_matches = []\n",
    "    for idx1, idx2, dist in matches_12:\n",
    "        if idx2 in reverse_matches and reverse_matches[idx2] == idx1:\n",
    "            verified_matches.append((idx1, idx2, dist))\n",
    "    \n",
    "    return verified_matches\n",
    "\n",
    "print(\"マッチング関数の実装完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformed_image(image: np.ndarray, \n",
    "                              rotation: float = 15,\n",
    "                              scale: float = 0.9,\n",
    "                              translation: Tuple[int, int] = (20, 10)) -> np.ndarray:\n",
    "    \"\"\"変換を加えた画像を生成（マッチングテスト用）\"\"\"\n",
    "    from scipy.ndimage import rotate, zoom, shift\n",
    "    \n",
    "    # 回転\n",
    "    transformed = rotate(image, rotation, reshape=False, mode='constant', cval=128)\n",
    "    \n",
    "    # スケーリング（パディング付き）\n",
    "    if scale != 1.0:\n",
    "        h, w = transformed.shape\n",
    "        zoomed = zoom(transformed, scale)\n",
    "        \n",
    "        # 元のサイズに合わせてクロップまたはパディング\n",
    "        zh, zw = zoomed.shape\n",
    "        result = np.ones((h, w)) * 128\n",
    "        \n",
    "        if scale < 1.0:\n",
    "            # パディング\n",
    "            start_y = (h - zh) // 2\n",
    "            start_x = (w - zw) // 2\n",
    "            result[start_y:start_y+zh, start_x:start_x+zw] = zoomed\n",
    "        else:\n",
    "            # クロップ\n",
    "            start_y = (zh - h) // 2\n",
    "            start_x = (zw - w) // 2\n",
    "            result = zoomed[start_y:start_y+h, start_x:start_x+w]\n",
    "        \n",
    "        transformed = result\n",
    "    \n",
    "    # 平行移動\n",
    "    transformed = shift(transformed, translation, mode='constant', cval=128)\n",
    "    \n",
    "    return transformed.astype(np.float32)\n",
    "\n",
    "# 変換画像の生成\n",
    "image1 = test_image\n",
    "image2 = create_transformed_image(test_image, rotation=10, scale=0.95, translation=(15, -10))\n",
    "\n",
    "# 両画像で特徴点検出\n",
    "_, corners1 = harris_corner_detection(image1, k=0.04, threshold=0.01)\n",
    "_, corners2 = harris_corner_detection(image2, k=0.04, threshold=0.01)\n",
    "\n",
    "# 記述子の計算\n",
    "kps1, descs1 = compute_descriptors(image1, corners1)\n",
    "kps2, descs2 = compute_descriptors(image2, corners2)\n",
    "\n",
    "print(f\"画像1: {len(kps1)} キーポイント\")\n",
    "print(f\"画像2: {len(kps2)} キーポイント\")\n",
    "\n",
    "# マッチング\n",
    "if len(descs1) > 0 and len(descs2) > 0:\n",
    "    matches_12 = match_descriptors(descs1, descs2, ratio_threshold=0.8)\n",
    "    matches_21 = match_descriptors(descs2, descs1, ratio_threshold=0.8)\n",
    "    \n",
    "    # 双方向検証\n",
    "    verified_matches = cross_check_matches(matches_12, matches_21)\n",
    "    \n",
    "    print(f\"\\nマッチング結果:\")\n",
    "    print(f\"  1→2 マッチ: {len(matches_12)}\")\n",
    "    print(f\"  双方向検証後: {len(verified_matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matches(image1: np.ndarray, image2: np.ndarray,\n",
    "                       kps1: List, kps2: List,\n",
    "                       matches: List[Tuple],\n",
    "                       max_matches: int = 50):\n",
    "    \"\"\"マッチングの可視化\"\"\"\n",
    "    # 画像を横に並べる\n",
    "    h1, w1 = image1.shape\n",
    "    h2, w2 = image2.shape\n",
    "    \n",
    "    canvas = np.zeros((max(h1, h2), w1 + w2))\n",
    "    canvas[:h1, :w1] = image1\n",
    "    canvas[:h2, w1:w1+w2] = image2\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    ax.imshow(canvas, cmap='gray')\n",
    "    \n",
    "    # マッチを描画\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, min(len(matches), max_matches)))\n",
    "    \n",
    "    for i, (idx1, idx2, dist) in enumerate(matches[:max_matches]):\n",
    "        y1, x1 = kps1[idx1]\n",
    "        y2, x2 = kps2[idx2]\n",
    "        \n",
    "        x2_shifted = x2 + w1\n",
    "        \n",
    "        ax.plot([x1, x2_shifted], [y1, y2], '-', color=colors[i], linewidth=1, alpha=0.7)\n",
    "        ax.plot(x1, y1, 'o', color=colors[i], markersize=5)\n",
    "        ax.plot(x2_shifted, y2, 'o', color=colors[i], markersize=5)\n",
    "    \n",
    "    ax.set_title(f'Feature Matches ({len(matches)} total, showing {min(len(matches), max_matches)})', fontsize=14)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if len(verified_matches) > 0:\n",
    "    visualize_matches(image1, image2, kps1, kps2, verified_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. RANSAC による外れ値除去\n",
    "\n",
    "### 6.1 問題\n",
    "\n",
    "マッチング結果には誤対応（外れ値/outliers）が含まれます。これを除去する必要があります。\n",
    "\n",
    "### 6.2 RANSAC アルゴリズム\n",
    "\n",
    "```\n",
    "for i = 1 to max_iterations:\n",
    "    1. ランダムに最小サンプル（ホモグラフィなら4点）を選択\n",
    "    2. そのサンプルからモデルを推定\n",
    "    3. 全点に対してモデルを評価、インライア数をカウント\n",
    "    4. インライア数が最大なら、このモデルを保持\n",
    "\n",
    "最終的に、全インライアでモデルを再推定\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_homography(src_pts: np.ndarray, dst_pts: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"4点以上からホモグラフィを計算（DLT法）\"\"\"\n",
    "    n = len(src_pts)\n",
    "    A = np.zeros((2 * n, 9))\n",
    "    \n",
    "    for i in range(n):\n",
    "        x, y = src_pts[i]\n",
    "        u, v = dst_pts[i]\n",
    "        \n",
    "        A[2*i] = [-x, -y, -1, 0, 0, 0, u*x, u*y, u]\n",
    "        A[2*i + 1] = [0, 0, 0, -x, -y, -1, v*x, v*y, v]\n",
    "    \n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    H = Vt[-1].reshape(3, 3)\n",
    "    H = H / H[2, 2]\n",
    "    \n",
    "    return H\n",
    "\n",
    "def apply_homography(H: np.ndarray, pts: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"ホモグラフィを点に適用\"\"\"\n",
    "    n = len(pts)\n",
    "    pts_h = np.hstack([pts, np.ones((n, 1))])\n",
    "    transformed = (H @ pts_h.T).T\n",
    "    return transformed[:, :2] / transformed[:, 2:3]\n",
    "\n",
    "def ransac_homography(src_pts: np.ndarray, dst_pts: np.ndarray,\n",
    "                       threshold: float = 3.0,\n",
    "                       max_iterations: int = 1000) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"RANSACによるホモグラフィ推定\n",
    "    \n",
    "    Returns:\n",
    "        H: 最良のホモグラフィ\n",
    "        inlier_mask: インライアのマスク\n",
    "    \"\"\"\n",
    "    n_points = len(src_pts)\n",
    "    best_H = None\n",
    "    best_inliers = np.zeros(n_points, dtype=bool)\n",
    "    best_n_inliers = 0\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        # 1. ランダムに4点選択\n",
    "        indices = np.random.choice(n_points, 4, replace=False)\n",
    "        \n",
    "        # 2. ホモグラフィを計算\n",
    "        try:\n",
    "            H = compute_homography(src_pts[indices], dst_pts[indices])\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # 3. 全点に対して評価\n",
    "        projected = apply_homography(H, src_pts)\n",
    "        errors = np.linalg.norm(projected - dst_pts, axis=1)\n",
    "        \n",
    "        # 4. インライアの判定\n",
    "        inliers = errors < threshold\n",
    "        n_inliers = np.sum(inliers)\n",
    "        \n",
    "        # 5. 最良モデルの更新\n",
    "        if n_inliers > best_n_inliers:\n",
    "            best_n_inliers = n_inliers\n",
    "            best_H = H\n",
    "            best_inliers = inliers\n",
    "    \n",
    "    # 6. 全インライアでホモグラフィを再計算\n",
    "    if np.sum(best_inliers) >= 4:\n",
    "        best_H = compute_homography(src_pts[best_inliers], dst_pts[best_inliers])\n",
    "    \n",
    "    return best_H, best_inliers\n",
    "\n",
    "print(\"RANSAC の実装完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マッチング点を抽出\n",
    "if len(verified_matches) >= 4:\n",
    "    src_pts = np.array([[kps1[m[0]][1], kps1[m[0]][0]] for m in verified_matches])  # (x, y)\n",
    "    dst_pts = np.array([[kps2[m[1]][1], kps2[m[1]][0]] for m in verified_matches])\n",
    "    \n",
    "    # RANSACでホモグラフィを推定\n",
    "    H, inlier_mask = ransac_homography(src_pts, dst_pts, threshold=5.0, max_iterations=1000)\n",
    "    \n",
    "    print(f\"マッチ数: {len(verified_matches)}\")\n",
    "    print(f\"インライア数: {np.sum(inlier_mask)}\")\n",
    "    print(f\"外れ値数: {np.sum(~inlier_mask)}\")\n",
    "    print(f\"\\n推定されたホモグラフィ:\")\n",
    "    print(H)\n",
    "    \n",
    "    # インライアのみのマッチを可視化\n",
    "    inlier_matches = [m for m, is_inlier in zip(verified_matches, inlier_mask) if is_inlier]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 全マッチ\n",
    "    h1, w1 = image1.shape\n",
    "    h2, w2 = image2.shape\n",
    "    canvas1 = np.zeros((max(h1, h2), w1 + w2))\n",
    "    canvas1[:h1, :w1] = image1\n",
    "    canvas1[:h2, w1:] = image2\n",
    "    \n",
    "    axes[0].imshow(canvas1, cmap='gray')\n",
    "    for idx1, idx2, _ in verified_matches:\n",
    "        y1, x1 = kps1[idx1]\n",
    "        y2, x2 = kps2[idx2]\n",
    "        axes[0].plot([x1, x2 + w1], [y1, y2], 'r-', alpha=0.5, linewidth=1)\n",
    "    axes[0].set_title(f'All Matches ({len(verified_matches)})')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # インライアのみ\n",
    "    canvas2 = canvas1.copy()\n",
    "    axes[1].imshow(canvas2, cmap='gray')\n",
    "    for idx1, idx2, _ in inlier_matches:\n",
    "        y1, x1 = kps1[idx1]\n",
    "        y2, x2 = kps2[idx2]\n",
    "        axes[1].plot([x1, x2 + w1], [y1, y2], 'g-', alpha=0.7, linewidth=1)\n",
    "    axes[1].set_title(f'Inliers Only ({len(inlier_matches)})')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.suptitle('RANSAC: Outlier Removal', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"マッチが不足しています\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 各手法の比較\n",
    "\n",
    "### 7.1 特徴点検出器の比較\n",
    "\n",
    "| 手法 | スケール不変 | 回転不変 | 速度 | 用途 |\n",
    "|------|------------|---------|------|------|\n",
    "| **Harris** | × | △ | 高速 | リアルタイム |\n",
    "| **SIFT** | ○ | ○ | 遅い | 高精度マッチング |\n",
    "| **SURF** | ○ | ○ | 中程度 | バランス |\n",
    "| **ORB** | △ | ○ | 非常に高速 | モバイル/リアルタイム |\n",
    "| **FAST** | × | × | 最速 | SLAM |\n",
    "\n",
    "### 7.2 実用上の選択指針\n",
    "\n",
    "- **精度重視**: SIFT > SURF > ORB\n",
    "- **速度重視**: FAST > ORB > SURF > SIFT\n",
    "- **メモリ重視**: ORB（バイナリ記述子）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. まとめと次のステップ\n",
    "\n",
    "### 学んだこと\n",
    "\n",
    "1. **特徴点の概念**: 繰り返し検出可能で識別可能な点\n",
    "2. **Harris コーナー検出**: 構造テンソルの固有値解析\n",
    "3. **スケール不変性**: SIFT のガウシアンピラミッドとDoG\n",
    "4. **特徴量記述子**: 勾配ヒストグラムによる128次元ベクトル\n",
    "5. **マッチング**: 最近傍探索とLowe's ratio test\n",
    "6. **RANSAC**: 外れ値除去によるロバスト推定\n",
    "\n",
    "### 重要な数式\n",
    "\n",
    "| 概念 | 数式 |\n",
    "|------|------|\n",
    "| 構造テンソル | $\\mathbf{M} = \\sum w \\begin{pmatrix} I_x^2 & I_x I_y \\\\ I_x I_y & I_y^2 \\end{pmatrix}$ |\n",
    "| Harris レスポンス | $R = \\det(\\mathbf{M}) - k \\cdot \\text{trace}(\\mathbf{M})^2$ |\n",
    "| DoG | $D = L(x, y, k\\sigma) - L(x, y, \\sigma)$ |\n",
    "| Ratio test | $d_1 / d_2 < \\text{threshold}$ |\n",
    "\n",
    "### 次のノートブック\n",
    "\n",
    "**59. SfM パイプライン基礎**では：\n",
    "- Structure from Motion の全体像\n",
    "- インクリメンタル SfM\n",
    "- 初期化とカメラ追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 自己評価クイズ\n",
    "\n",
    "以下の質問に答えて理解度を確認しましょう：\n",
    "\n",
    "1. Harris コーナー検出で、構造テンソル $\\mathbf{M}$ の2つの固有値がともに大きい場合、その点は何を意味しますか？\n",
    "\n",
    "2. SIFT がスケール不変性を持つ仕組みを説明してください。\n",
    "\n",
    "3. Lowe's ratio test とは何ですか？なぜ有効なのですか？\n",
    "\n",
    "4. RANSAC がホモグラフィ推定に必要な理由は？\n",
    "\n",
    "5. ORB が SIFT より高速な理由は何ですか？\n",
    "\n",
    "6. 特徴量記述子の「正規化」が重要な理由は？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クイズの解答（隠し）\n",
    "def show_quiz_answers():\n",
    "    answers = \"\"\"\n",
    "    === 自己評価クイズ解答 ===\n",
    "    \n",
    "    1. 両固有値が大きい場合:\n",
    "       - 2つの主方向ともに強い変化がある\n",
    "       - これはコーナー（2方向にエッジがある点）を意味する\n",
    "       - Harris レスポンス R が大きな正の値になる\n",
    "    \n",
    "    2. SIFT のスケール不変性:\n",
    "       - ガウシアンピラミッドで複数スケールの画像を生成\n",
    "       - DoG（Difference of Gaussian）でブロブを検出\n",
    "       - スケール空間で極値を探すことで、特徴的なスケールを決定\n",
    "       - 検出されたスケールに応じて記述子を計算\n",
    "    \n",
    "    3. Lowe's ratio test:\n",
    "       - 最近傍と2番目に近い点の距離比を計算\n",
    "       - 比が閾値未満の場合のみマッチとして採用\n",
    "       - 有効な理由: 正しいマッチは2番目より明らかに近い\n",
    "       - 曖昧なマッチ（複数の似た候補がある）を排除\n",
    "    \n",
    "    4. RANSAC が必要な理由:\n",
    "       - 特徴点マッチングには誤対応（外れ値）が含まれる\n",
    "       - 外れ値があると最小二乗法が大きく歪む\n",
    "       - RANSAC はロバストに正しいモデルを推定\n",
    "       - インライアとアウトライアを分離できる\n",
    "    \n",
    "    5. ORB が高速な理由:\n",
    "       - バイナリ記述子（256ビット）を使用\n",
    "       - ハミング距離で高速に比較可能（XORとポップカウント）\n",
    "       - FAST コーナー検出器ベース（非常に高速）\n",
    "       - 浮動小数点演算が少ない\n",
    "    \n",
    "    6. 記述子の正規化が重要な理由:\n",
    "       - 照明変化への対応（全体的な明るさの変化を吸収）\n",
    "       - コントラスト変化への対応\n",
    "       - 記述子の比較を公平に行える\n",
    "       - SIFT では追加でクリッピング（0.2上限）も行う\n",
    "    \"\"\"\n",
    "    print(answers)\n",
    "\n",
    "# 解答を見るには以下のコメントを外して実行\n",
    "# show_quiz_answers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ナビゲーション\n",
    "\n",
    "- **前のノートブック**: [57. 三角測量と3D復元](57_triangulation_3d_reconstruction_v1.ipynb)\n",
    "- **次のノートブック**: [59. SfM パイプライン基礎](59_sfm_pipeline_basics_v1.ipynb)\n",
    "- **カリキュラム**: [CURRICULUM_UNIT_0.3.md](CURRICULUM_UNIT_0.3.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
