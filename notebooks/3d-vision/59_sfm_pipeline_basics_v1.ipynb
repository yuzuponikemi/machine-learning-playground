{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 59. SfM パイプライン基礎\n",
    "## Structure from Motion Pipeline Basics\n",
    "\n",
    "---\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "このノートブックを完了すると、以下ができるようになります：\n",
    "\n",
    "- [ ] Structure from Motion (SfM) の全体的なワークフローを理解する\n",
    "- [ ] 2視点からの初期化（エッセンシャル行列→カメラ姿勢→3D点）を実装できる\n",
    "- [ ] インクリメンタル SfM の概念を理解する\n",
    "- [ ] PnP 問題によるカメラ姿勢推定を理解する\n",
    "- [ ] トラック（マルチビュー対応）の概念を理解する\n",
    "- [ ] SfM システムの精度評価方法を理解する\n",
    "\n",
    "---\n",
    "\n",
    "## 前提知識\n",
    "\n",
    "- 55: エピポーラ幾何の理論\n",
    "- 57: 三角測量と3D復元\n",
    "- 58: 特徴点検出とマッチング\n",
    "\n",
    "**難易度**: ★★★★★（上級）  \n",
    "**推定学習時間**: 120-150分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Structure from Motion (SfM) とは\n",
    "\n",
    "**Structure from Motion (SfM)** は、複数の2D画像から3D構造（点群）とカメラ姿勢を同時に復元する技術です。\n",
    "\n",
    "### 入力と出力\n",
    "\n",
    "| 入力 | 出力 |\n",
    "|------|------|\n",
    "| 複数の2D画像 | 3D点群（Structure） |\n",
    "| カメラ内部パラメータ（既知または推定） | カメラ姿勢（Motion） |\n",
    "\n",
    "### SfM の種類\n",
    "\n",
    "| 種類 | 説明 | 用途 |\n",
    "|------|------|------|\n",
    "| **インクリメンタル SfM** | 画像を1枚ずつ追加 | COLMAP, VisualSFM |\n",
    "| **グローバル SfM** | 全画像を同時に処理 | 1DSfM, Theia |\n",
    "| **ハイブリッド** | 両方の組み合わせ | OpenMVG |\n",
    "\n",
    "### 応用分野\n",
    "\n",
    "- 3Dスキャン・モデリング\n",
    "- 自動運転（視覚オドメトリ）\n",
    "- AR/VR\n",
    "- ドローン測量\n",
    "- NeRF のカメラ姿勢推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from scipy.optimize import least_squares\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. インクリメンタル SfM パイプライン\n",
    "\n",
    "### 2.1 全体フロー\n",
    "\n",
    "```\n",
    "1. 特徴点検出・マッチング（全画像ペア）\n",
    "       ↓\n",
    "2. 初期化（最良の2画像ペアを選択）\n",
    "   - エッセンシャル行列の推定\n",
    "   - カメラ姿勢の復元\n",
    "   - 初期3D点の三角測量\n",
    "       ↓\n",
    "3. 画像の追加（繰り返し）\n",
    "   - PnP でカメラ姿勢を推定\n",
    "   - 新しい3D点を三角測量\n",
    "       ↓\n",
    "4. バンドル調整（最適化）\n",
    "       ↓\n",
    "5. 点群とカメラ姿勢の出力\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sfm_pipeline():\n",
    "    \"\"\"SfM パイプラインの概念図\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    steps = [\n",
    "        ('1. Feature Detection & Matching', 'Multiple images → Keypoints & Matches'),\n",
    "        ('2. Initialization (2-view)', 'Essential matrix → R, t → Triangulation'),\n",
    "        ('3. Add New Image (PnP)', '2D-3D correspondences → Camera pose'),\n",
    "        ('4. Triangulate New Points', 'New matches → New 3D points'),\n",
    "        ('5. Bundle Adjustment', 'Optimize cameras & points jointly'),\n",
    "        ('6. Output', '3D point cloud + Camera poses')\n",
    "    ]\n",
    "    \n",
    "    for ax, (title, desc) in zip(axes.flat, steps):\n",
    "        ax.text(0.5, 0.6, title, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "        ax.text(0.5, 0.4, desc, ha='center', va='center', fontsize=10, style='italic')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "        ax.add_patch(plt.Rectangle((0.05, 0.2), 0.9, 0.6, fill=False, edgecolor='blue', linewidth=2))\n",
    "    \n",
    "    plt.suptitle('Incremental SfM Pipeline', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_sfm_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 合成データの準備\n",
    "\n",
    "SfM パイプラインをテストするための合成シーンを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticScene:\n",
    "    \"\"\"合成3Dシーン（SfMテスト用）\"\"\"\n",
    "    \n",
    "    def __init__(self, n_points: int = 100, n_cameras: int = 5):\n",
    "        self.n_points = n_points\n",
    "        self.n_cameras = n_cameras\n",
    "        \n",
    "        # カメラ内部パラメータ\n",
    "        self.K = np.array([\n",
    "            [500, 0, 320],\n",
    "            [0, 500, 240],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        self.image_size = (640, 480)\n",
    "        \n",
    "        # 3D点を生成\n",
    "        self._generate_points()\n",
    "        \n",
    "        # カメラを配置\n",
    "        self._generate_cameras()\n",
    "        \n",
    "        # 投影を計算\n",
    "        self._project_points()\n",
    "    \n",
    "    def _generate_points(self):\n",
    "        \"\"\"ランダムな3D点を生成\"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.points_3d = np.random.randn(self.n_points, 3) * 2\n",
    "        self.points_3d[:, 2] = np.abs(self.points_3d[:, 2]) + 3  # Z > 3\n",
    "    \n",
    "    def _generate_cameras(self):\n",
    "        \"\"\"円周上にカメラを配置\"\"\"\n",
    "        self.cameras = []  # [(R, t), ...]\n",
    "        radius = 8\n",
    "        \n",
    "        for i in range(self.n_cameras):\n",
    "            angle = 2 * np.pi * i / self.n_cameras - np.pi / 2\n",
    "            \n",
    "            # カメラ位置（円周上）\n",
    "            C = np.array([radius * np.cos(angle), 0, radius * np.sin(angle)])\n",
    "            \n",
    "            # 原点を向くように回転\n",
    "            z_axis = -C / np.linalg.norm(C)\n",
    "            y_axis = np.array([0, 1, 0])\n",
    "            x_axis = np.cross(y_axis, z_axis)\n",
    "            x_axis = x_axis / np.linalg.norm(x_axis)\n",
    "            y_axis = np.cross(z_axis, x_axis)\n",
    "            \n",
    "            R = np.vstack([x_axis, y_axis, z_axis])\n",
    "            t = -R @ C\n",
    "            \n",
    "            self.cameras.append((R, t))\n",
    "    \n",
    "    def _project_points(self):\n",
    "        \"\"\"3D点を各カメラに投影\"\"\"\n",
    "        self.projections = []  # [[(u, v), ...], ...] for each camera\n",
    "        self.visibility = []  # [[True/False, ...], ...] for each camera\n",
    "        \n",
    "        for R, t in self.cameras:\n",
    "            P = self.K @ np.hstack([R, t.reshape(-1, 1)])\n",
    "            \n",
    "            cam_projs = []\n",
    "            cam_vis = []\n",
    "            \n",
    "            for X in self.points_3d:\n",
    "                X_h = np.append(X, 1)\n",
    "                x_h = P @ X_h\n",
    "                x = x_h[:2] / x_h[2]\n",
    "                \n",
    "                # 可視性チェック\n",
    "                visible = (0 <= x[0] < self.image_size[0] and \n",
    "                           0 <= x[1] < self.image_size[1] and\n",
    "                           x_h[2] > 0)  # 前方にある\n",
    "                \n",
    "                cam_projs.append(x)\n",
    "                cam_vis.append(visible)\n",
    "            \n",
    "            self.projections.append(np.array(cam_projs))\n",
    "            self.visibility.append(np.array(cam_vis))\n",
    "    \n",
    "    def get_projection_matrix(self, cam_idx: int) -> np.ndarray:\n",
    "        \"\"\"指定カメラの投影行列を取得\"\"\"\n",
    "        R, t = self.cameras[cam_idx]\n",
    "        return self.K @ np.hstack([R, t.reshape(-1, 1)])\n",
    "    \n",
    "    def get_visible_points(self, cam_idx: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"指定カメラで見える点を取得\"\"\"\n",
    "        vis = self.visibility[cam_idx]\n",
    "        return (self.projections[cam_idx][vis], \n",
    "                self.points_3d[vis],\n",
    "                np.where(vis)[0])\n",
    "    \n",
    "    def add_noise(self, noise_level: float = 1.0):\n",
    "        \"\"\"投影点にノイズを追加\"\"\"\n",
    "        for i in range(len(self.projections)):\n",
    "            noise = np.random.randn(*self.projections[i].shape) * noise_level\n",
    "            self.projections[i] = self.projections[i] + noise\n",
    "\n",
    "# シーンの作成\n",
    "scene = SyntheticScene(n_points=80, n_cameras=6)\n",
    "scene.add_noise(noise_level=0.5)\n",
    "\n",
    "print(f\"3D点数: {scene.n_points}\")\n",
    "print(f\"カメラ数: {scene.n_cameras}\")\n",
    "\n",
    "for i in range(scene.n_cameras):\n",
    "    visible_count = np.sum(scene.visibility[i])\n",
    "    print(f\"  カメラ{i}: {visible_count} 点が可視\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scene(scene: SyntheticScene):\n",
    "    \"\"\"合成シーンの可視化\"\"\"\n",
    "    fig = plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # 3Dビュー\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    \n",
    "    # 3D点\n",
    "    ax1.scatter(scene.points_3d[:, 0], scene.points_3d[:, 1], scene.points_3d[:, 2],\n",
    "                c='blue', s=20, alpha=0.5, label='3D Points')\n",
    "    \n",
    "    # カメラ\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, scene.n_cameras))\n",
    "    for i, (R, t) in enumerate(scene.cameras):\n",
    "        C = -R.T @ t  # カメラ中心\n",
    "        ax1.scatter(*C, color=colors[i], s=200, marker='o', label=f'Camera {i}')\n",
    "        \n",
    "        # カメラの向き\n",
    "        direction = R.T @ np.array([0, 0, 1])  # Z軸方向\n",
    "        ax1.quiver(*C, *direction * 1.5, color=colors[i], arrow_length_ratio=0.2)\n",
    "    \n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.set_zlabel('Z')\n",
    "    ax1.set_title('Synthetic Scene (Ground Truth)')\n",
    "    ax1.view_init(elev=30, azim=45)\n",
    "    \n",
    "    # 画像投影（最初のカメラ）\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    pts, _, _ = scene.get_visible_points(0)\n",
    "    ax2.scatter(pts[:, 0], pts[:, 1], c='red', s=10)\n",
    "    ax2.set_xlim(0, scene.image_size[0])\n",
    "    ax2.set_ylim(scene.image_size[1], 0)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.set_title('Camera 0 Projection (with noise)')\n",
    "    ax2.set_xlabel('u (pixels)')\n",
    "    ax2.set_ylabel('v (pixels)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_scene(scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 2視点初期化\n",
    "\n",
    "### 4.1 初期ペアの選択\n",
    "\n",
    "良い初期ペアの条件：\n",
    "- 十分なマッチ数\n",
    "- 適度な視差（ベースラインが狭すぎない）\n",
    "- 高いインライア率\n",
    "\n",
    "### 4.2 処理フロー\n",
    "\n",
    "1. 本質行列 $\\mathbf{E}$ の推定（5点法 or 8点法 + RANSAC）\n",
    "2. $\\mathbf{E}$ から $(\\mathbf{R}, \\mathbf{t})$ を復元\n",
    "3. 三角測量で初期3D点を復元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_points(pts: np.ndarray, K: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"画像座標を正規化座標に変換\"\"\"\n",
    "    pts_h = np.hstack([pts, np.ones((len(pts), 1))])\n",
    "    pts_norm = (np.linalg.inv(K) @ pts_h.T).T\n",
    "    return pts_norm[:, :2]\n",
    "\n",
    "def eight_point_algorithm(pts1: np.ndarray, pts2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"8点法による本質行列の推定（正規化座標）\"\"\"\n",
    "    n = len(pts1)\n",
    "    A = np.zeros((n, 9))\n",
    "    \n",
    "    for i in range(n):\n",
    "        x1, y1 = pts1[i]\n",
    "        x2, y2 = pts2[i]\n",
    "        A[i] = [x2*x1, x2*y1, x2, y2*x1, y2*y1, y2, x1, y1, 1]\n",
    "    \n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    E = Vt[-1].reshape(3, 3)\n",
    "    \n",
    "    # ランク2に射影\n",
    "    U, S, Vt = np.linalg.svd(E)\n",
    "    S = np.array([1, 1, 0])  # 特異値を正規化\n",
    "    E = U @ np.diag(S) @ Vt\n",
    "    \n",
    "    return E\n",
    "\n",
    "def decompose_essential_matrix(E: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"本質行列を分解して4つの解候補を返す\"\"\"\n",
    "    U, _, Vt = np.linalg.svd(E)\n",
    "    \n",
    "    if np.linalg.det(U) < 0:\n",
    "        U = -U\n",
    "    if np.linalg.det(Vt) < 0:\n",
    "        Vt = -Vt\n",
    "    \n",
    "    W = np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n",
    "    \n",
    "    R1 = U @ W @ Vt\n",
    "    R2 = U @ W.T @ Vt\n",
    "    t = U[:, 2]\n",
    "    \n",
    "    # det(R) = 1 を保証\n",
    "    if np.linalg.det(R1) < 0:\n",
    "        R1 = -R1\n",
    "    if np.linalg.det(R2) < 0:\n",
    "        R2 = -R2\n",
    "    \n",
    "    return [(R1, t), (R1, -t), (R2, t), (R2, -t)]\n",
    "\n",
    "def triangulate_dlt(P1: np.ndarray, P2: np.ndarray, \n",
    "                    x1: np.ndarray, x2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"DLTによる三角測量\"\"\"\n",
    "    A = np.array([\n",
    "        x1[0] * P1[2] - P1[0],\n",
    "        x1[1] * P1[2] - P1[1],\n",
    "        x2[0] * P2[2] - P2[0],\n",
    "        x2[1] * P2[2] - P2[1]\n",
    "    ])\n",
    "    \n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    X_h = Vt[-1]\n",
    "    return X_h[:3] / X_h[3]\n",
    "\n",
    "def check_cheirality(R: np.ndarray, t: np.ndarray, K: np.ndarray,\n",
    "                     pts1: np.ndarray, pts2: np.ndarray) -> int:\n",
    "    \"\"\"チェイラリティ条件をチェック（両カメラ前方にある点の数）\"\"\"\n",
    "    P1 = K @ np.hstack([np.eye(3), np.zeros((3, 1))])\n",
    "    P2 = K @ np.hstack([R, t.reshape(-1, 1)])\n",
    "    \n",
    "    n_valid = 0\n",
    "    for i in range(min(len(pts1), 50)):\n",
    "        X = triangulate_dlt(P1, P2, pts1[i], pts2[i])\n",
    "        \n",
    "        # カメラ1での深度\n",
    "        z1 = X[2]\n",
    "        \n",
    "        # カメラ2での深度\n",
    "        X_cam2 = R @ X + t\n",
    "        z2 = X_cam2[2]\n",
    "        \n",
    "        if z1 > 0 and z2 > 0:\n",
    "            n_valid += 1\n",
    "    \n",
    "    return n_valid\n",
    "\n",
    "def recover_pose(E: np.ndarray, K: np.ndarray, \n",
    "                 pts1: np.ndarray, pts2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"本質行列からカメラ姿勢を復元\"\"\"\n",
    "    solutions = decompose_essential_matrix(E)\n",
    "    \n",
    "    best_R, best_t = None, None\n",
    "    best_count = 0\n",
    "    \n",
    "    for R, t in solutions:\n",
    "        count = check_cheirality(R, t, K, pts1, pts2)\n",
    "        if count > best_count:\n",
    "            best_count = count\n",
    "            best_R, best_t = R, t\n",
    "    \n",
    "    return best_R, best_t\n",
    "\n",
    "print(\"2視点初期化関数の実装完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_two_view(scene: SyntheticScene, \n",
    "                        cam1_idx: int = 0, \n",
    "                        cam2_idx: int = 1) -> Dict:\n",
    "    \"\"\"2視点からSfMを初期化\"\"\"\n",
    "    K = scene.K\n",
    "    \n",
    "    # 両方のカメラで見える点を取得\n",
    "    vis1 = scene.visibility[cam1_idx]\n",
    "    vis2 = scene.visibility[cam2_idx]\n",
    "    common_vis = vis1 & vis2\n",
    "    \n",
    "    pts1 = scene.projections[cam1_idx][common_vis]\n",
    "    pts2 = scene.projections[cam2_idx][common_vis]\n",
    "    pts_3d_true = scene.points_3d[common_vis]\n",
    "    point_indices = np.where(common_vis)[0]\n",
    "    \n",
    "    print(f\"共通可視点: {len(pts1)}\")\n",
    "    \n",
    "    # 正規化座標に変換\n",
    "    pts1_norm = normalize_points(pts1, K)\n",
    "    pts2_norm = normalize_points(pts2, K)\n",
    "    \n",
    "    # 本質行列の推定\n",
    "    E = eight_point_algorithm(pts1_norm, pts2_norm)\n",
    "    \n",
    "    # カメラ姿勢の復元\n",
    "    R, t = recover_pose(E, K, pts1, pts2)\n",
    "    \n",
    "    # 三角測量\n",
    "    P1 = K @ np.hstack([np.eye(3), np.zeros((3, 1))])\n",
    "    P2 = K @ np.hstack([R, t.reshape(-1, 1)])\n",
    "    \n",
    "    points_3d_estimated = []\n",
    "    for i in range(len(pts1)):\n",
    "        X = triangulate_dlt(P1, P2, pts1[i], pts2[i])\n",
    "        points_3d_estimated.append(X)\n",
    "    \n",
    "    points_3d_estimated = np.array(points_3d_estimated)\n",
    "    \n",
    "    # スケール補正（真値との比較用）\n",
    "    # 真のスケールを適用\n",
    "    R_true1, t_true1 = scene.cameras[cam1_idx]\n",
    "    R_true2, t_true2 = scene.cameras[cam2_idx]\n",
    "    \n",
    "    # カメラ間の相対姿勢（真値）\n",
    "    R_rel_true = R_true2 @ R_true1.T\n",
    "    t_rel_true = t_true2 - R_rel_true @ t_true1\n",
    "    scale_true = np.linalg.norm(t_rel_true)\n",
    "    \n",
    "    # 推定されたスケールを真値に合わせる\n",
    "    t = t * scale_true\n",
    "    points_3d_estimated = points_3d_estimated * scale_true\n",
    "    \n",
    "    result = {\n",
    "        'cameras': {\n",
    "            cam1_idx: (np.eye(3), np.zeros(3)),\n",
    "            cam2_idx: (R, t)\n",
    "        },\n",
    "        'points_3d': points_3d_estimated,\n",
    "        'point_indices': point_indices,\n",
    "        'observations': {\n",
    "            cam1_idx: pts1,\n",
    "            cam2_idx: pts2\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 初期化の実行\n",
    "sfm_result = initialize_two_view(scene, cam1_idx=0, cam2_idx=1)\n",
    "\n",
    "print(f\"\\n初期化されたカメラ数: {len(sfm_result['cameras'])}\")\n",
    "print(f\"復元された3D点数: {len(sfm_result['points_3d'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reconstruction(scene: SyntheticScene, sfm_result: Dict):\n",
    "    \"\"\"復元結果の評価\"\"\"\n",
    "    # カメラ姿勢の誤差\n",
    "    print(\"=== カメラ姿勢の評価 ===\")\n",
    "    for cam_idx, (R_est, t_est) in sfm_result['cameras'].items():\n",
    "        R_true, t_true = scene.cameras[cam_idx]\n",
    "        \n",
    "        # カメラ0を基準とした相対姿勢で比較\n",
    "        if cam_idx == 0:\n",
    "            continue\n",
    "        \n",
    "        R_true0, t_true0 = scene.cameras[0]\n",
    "        R_rel_true = R_true @ R_true0.T\n",
    "        t_rel_true = t_true - R_rel_true @ t_true0\n",
    "        \n",
    "        # 回転誤差\n",
    "        R_error = R_est @ R_rel_true.T\n",
    "        angle_error = np.arccos(np.clip((np.trace(R_error) - 1) / 2, -1, 1))\n",
    "        \n",
    "        # 並進方向誤差\n",
    "        t_est_norm = t_est / np.linalg.norm(t_est)\n",
    "        t_true_norm = t_rel_true / np.linalg.norm(t_rel_true)\n",
    "        t_angle_error = np.arccos(np.clip(np.abs(np.dot(t_est_norm, t_true_norm)), -1, 1))\n",
    "        \n",
    "        print(f\"Camera {cam_idx}:\")\n",
    "        print(f\"  回転誤差: {np.degrees(angle_error):.2f}°\")\n",
    "        print(f\"  並進方向誤差: {np.degrees(t_angle_error):.2f}°\")\n",
    "    \n",
    "    # 3D点の誤差\n",
    "    print(\"\\n=== 3D点の評価 ===\")\n",
    "    \n",
    "    # 座標系の位置合わせ（Procrustes解析）\n",
    "    pts_est = sfm_result['points_3d']\n",
    "    pts_true = scene.points_3d[sfm_result['point_indices']]\n",
    "    \n",
    "    # カメラ0を原点とした座標系に変換\n",
    "    R0, t0 = scene.cameras[0]\n",
    "    pts_true_cam0 = (R0 @ pts_true.T).T + t0\n",
    "    \n",
    "    # 誤差計算\n",
    "    errors = np.linalg.norm(pts_est - pts_true_cam0, axis=1)\n",
    "    \n",
    "    print(f\"3D点数: {len(pts_est)}\")\n",
    "    print(f\"平均誤差: {np.mean(errors):.4f}\")\n",
    "    print(f\"中央値誤差: {np.median(errors):.4f}\")\n",
    "    print(f\"最大誤差: {np.max(errors):.4f}\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "errors = evaluate_reconstruction(scene, sfm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. PnP (Perspective-n-Point) 問題\n",
    "\n",
    "### 5.1 問題設定\n",
    "\n",
    "既知の3D点と対応する2D画像点から、カメラ姿勢を推定します。\n",
    "\n",
    "- **入力**: $n$ 組の 3D-2D 対応 $(\\mathbf{X}_i, \\mathbf{x}_i)$\n",
    "- **出力**: カメラ姿勢 $(\\mathbf{R}, \\mathbf{t})$\n",
    "\n",
    "### 5.2 最小ケース\n",
    "\n",
    "- **P3P**: 3点で最大4解\n",
    "- **P4P/P5P/P6P**: より多くの点でより安定\n",
    "\n",
    "### 5.3 EPnP アルゴリズム\n",
    "\n",
    "効率的な PnP ソルバー。$n \\geq 4$ 点から線形に解を求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnp_dlt(points_3d: np.ndarray, points_2d: np.ndarray, \n",
    "            K: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"DLT-based PnP (簡略版)\n",
    "    \n",
    "    6点以上から投影行列を推定し、R, t を分解\n",
    "    \"\"\"\n",
    "    n = len(points_3d)\n",
    "    assert n >= 6, \"6点以上必要です\"\n",
    "    \n",
    "    # 行列Aの構築\n",
    "    A = np.zeros((2 * n, 12))\n",
    "    \n",
    "    for i in range(n):\n",
    "        X, Y, Z = points_3d[i]\n",
    "        u, v = points_2d[i]\n",
    "        \n",
    "        A[2*i] = [-X, -Y, -Z, -1, 0, 0, 0, 0, u*X, u*Y, u*Z, u]\n",
    "        A[2*i+1] = [0, 0, 0, 0, -X, -Y, -Z, -1, v*X, v*Y, v*Z, v]\n",
    "    \n",
    "    # SVDで解く\n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    P = Vt[-1].reshape(3, 4)\n",
    "    \n",
    "    # P = K [R | t] から R, t を分解\n",
    "    M = np.linalg.inv(K) @ P[:, :3]\n",
    "    \n",
    "    # Mの特異値分解でRを抽出\n",
    "    U, S, Vt = np.linalg.svd(M)\n",
    "    R = U @ Vt\n",
    "    \n",
    "    # det(R) = 1 を保証\n",
    "    if np.linalg.det(R) < 0:\n",
    "        R = -R\n",
    "    \n",
    "    # スケールを復元\n",
    "    scale = np.mean(S)\n",
    "    t = np.linalg.inv(K) @ P[:, 3] / scale\n",
    "    \n",
    "    return R, t\n",
    "\n",
    "def pnp_ransac(points_3d: np.ndarray, points_2d: np.ndarray,\n",
    "               K: np.ndarray, threshold: float = 5.0,\n",
    "               max_iterations: int = 1000) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"RANSAC による PnP\n",
    "    \n",
    "    Returns:\n",
    "        R, t: カメラ姿勢\n",
    "        inlier_mask: インライアのマスク\n",
    "    \"\"\"\n",
    "    n = len(points_3d)\n",
    "    best_R, best_t = None, None\n",
    "    best_inliers = np.zeros(n, dtype=bool)\n",
    "    best_n_inliers = 0\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        # ランダムに6点選択\n",
    "        indices = np.random.choice(n, 6, replace=False)\n",
    "        \n",
    "        try:\n",
    "            R, t = pnp_dlt(points_3d[indices], points_2d[indices], K)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # 再投影誤差を計算\n",
    "        P = K @ np.hstack([R, t.reshape(-1, 1)])\n",
    "        projected = []\n",
    "        for X in points_3d:\n",
    "            X_h = np.append(X, 1)\n",
    "            x_h = P @ X_h\n",
    "            projected.append(x_h[:2] / x_h[2])\n",
    "        projected = np.array(projected)\n",
    "        \n",
    "        errors = np.linalg.norm(projected - points_2d, axis=1)\n",
    "        inliers = errors < threshold\n",
    "        n_inliers = np.sum(inliers)\n",
    "        \n",
    "        if n_inliers > best_n_inliers:\n",
    "            best_n_inliers = n_inliers\n",
    "            best_R, best_t = R, t\n",
    "            best_inliers = inliers\n",
    "    \n",
    "    # インライアで再推定\n",
    "    if np.sum(best_inliers) >= 6:\n",
    "        best_R, best_t = pnp_dlt(points_3d[best_inliers], \n",
    "                                  points_2d[best_inliers], K)\n",
    "    \n",
    "    return best_R, best_t, best_inliers\n",
    "\n",
    "print(\"PnP の実装完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. インクリメンタル SfM\n",
    "\n",
    "### 6.1 画像追加のフロー\n",
    "\n",
    "1. 新しい画像と既存の3D点のマッチングを見つける\n",
    "2. PnP でカメラ姿勢を推定\n",
    "3. 新しい3D点を三角測量\n",
    "4. バンドル調整で最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_camera_to_sfm(scene: SyntheticScene, sfm_result: Dict, \n",
    "                      new_cam_idx: int) -> Dict:\n",
    "    \"\"\"新しいカメラをSfM結果に追加\"\"\"\n",
    "    K = scene.K\n",
    "    \n",
    "    # 既存の3D点と新しいカメラでの観測点を対応付け\n",
    "    existing_point_indices = sfm_result['point_indices']\n",
    "    new_visibility = scene.visibility[new_cam_idx]\n",
    "    \n",
    "    # 共通点を見つける\n",
    "    common_indices = []\n",
    "    for i, pt_idx in enumerate(existing_point_indices):\n",
    "        if new_visibility[pt_idx]:\n",
    "            common_indices.append(i)\n",
    "    \n",
    "    common_indices = np.array(common_indices)\n",
    "    \n",
    "    if len(common_indices) < 6:\n",
    "        print(f\"共通点が不足: {len(common_indices)} < 6\")\n",
    "        return sfm_result\n",
    "    \n",
    "    # 3D点と2D観測点\n",
    "    points_3d = sfm_result['points_3d'][common_indices]\n",
    "    points_2d = scene.projections[new_cam_idx][existing_point_indices[common_indices]]\n",
    "    \n",
    "    print(f\"カメラ {new_cam_idx} 追加: 共通点 {len(common_indices)}\")\n",
    "    \n",
    "    # PnP でカメラ姿勢を推定\n",
    "    R, t, inliers = pnp_ransac(points_3d, points_2d, K, threshold=5.0)\n",
    "    \n",
    "    print(f\"  PnP インライア: {np.sum(inliers)} / {len(inliers)}\")\n",
    "    \n",
    "    # 結果を更新\n",
    "    sfm_result['cameras'][new_cam_idx] = (R, t)\n",
    "    sfm_result['observations'][new_cam_idx] = points_2d\n",
    "    \n",
    "    # 新しい3D点を三角測量（既存カメラとの共通点で未復元のもの）\n",
    "    new_points_3d = []\n",
    "    new_point_indices = []\n",
    "    \n",
    "    for exist_cam_idx, (R_exist, t_exist) in sfm_result['cameras'].items():\n",
    "        if exist_cam_idx == new_cam_idx:\n",
    "            continue\n",
    "        \n",
    "        vis_exist = scene.visibility[exist_cam_idx]\n",
    "        vis_new = scene.visibility[new_cam_idx]\n",
    "        \n",
    "        # 両方で見えるが未復元の点\n",
    "        for pt_idx in range(scene.n_points):\n",
    "            if vis_exist[pt_idx] and vis_new[pt_idx]:\n",
    "                if pt_idx not in sfm_result['point_indices']:\n",
    "                    # 三角測量\n",
    "                    P_exist = K @ np.hstack([R_exist, t_exist.reshape(-1, 1)])\n",
    "                    P_new = K @ np.hstack([R, t.reshape(-1, 1)])\n",
    "                    \n",
    "                    x_exist = scene.projections[exist_cam_idx][pt_idx]\n",
    "                    x_new = scene.projections[new_cam_idx][pt_idx]\n",
    "                    \n",
    "                    X = triangulate_dlt(P_exist, P_new, x_exist, x_new)\n",
    "                    \n",
    "                    # 深度チェック\n",
    "                    if X[2] > 0:\n",
    "                        new_points_3d.append(X)\n",
    "                        new_point_indices.append(pt_idx)\n",
    "    \n",
    "    # 新しい点を追加\n",
    "    if len(new_points_3d) > 0:\n",
    "        sfm_result['points_3d'] = np.vstack([sfm_result['points_3d'], \n",
    "                                              np.array(new_points_3d)])\n",
    "        sfm_result['point_indices'] = np.concatenate([sfm_result['point_indices'],\n",
    "                                                       np.array(new_point_indices)])\n",
    "        print(f\"  新しい3D点: {len(new_points_3d)}\")\n",
    "    \n",
    "    return sfm_result\n",
    "\n",
    "# 残りのカメラを追加\n",
    "print(\"\\n=== インクリメンタルSfM ===\")\n",
    "for cam_idx in range(2, scene.n_cameras):\n",
    "    sfm_result = add_camera_to_sfm(scene, sfm_result, cam_idx)\n",
    "\n",
    "print(f\"\\n最終結果:\")\n",
    "print(f\"  カメラ数: {len(sfm_result['cameras'])}\")\n",
    "print(f\"  3D点数: {len(sfm_result['points_3d'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sfm_result(scene: SyntheticScene, sfm_result: Dict):\n",
    "    \"\"\"SfM結果の可視化\"\"\"\n",
    "    fig = plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # 真値\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    \n",
    "    ax1.scatter(scene.points_3d[:, 0], scene.points_3d[:, 1], scene.points_3d[:, 2],\n",
    "                c='blue', s=10, alpha=0.3, label='GT Points')\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, scene.n_cameras))\n",
    "    for i, (R, t) in enumerate(scene.cameras):\n",
    "        C = -R.T @ t\n",
    "        ax1.scatter(*C, color=colors[i], s=100, marker='o')\n",
    "        direction = R.T @ np.array([0, 0, 1])\n",
    "        ax1.quiver(*C, *direction, color=colors[i], arrow_length_ratio=0.3)\n",
    "    \n",
    "    ax1.set_title('Ground Truth')\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.set_zlabel('Z')\n",
    "    ax1.view_init(elev=30, azim=45)\n",
    "    \n",
    "    # 推定結果\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    \n",
    "    ax2.scatter(sfm_result['points_3d'][:, 0], \n",
    "                sfm_result['points_3d'][:, 1], \n",
    "                sfm_result['points_3d'][:, 2],\n",
    "                c='red', s=10, alpha=0.3, label='Estimated Points')\n",
    "    \n",
    "    for cam_idx, (R, t) in sfm_result['cameras'].items():\n",
    "        C = -R.T @ t\n",
    "        ax2.scatter(*C, color=colors[cam_idx], s=100, marker='o', \n",
    "                    label=f'Camera {cam_idx}')\n",
    "        direction = R.T @ np.array([0, 0, 1])\n",
    "        ax2.quiver(*C, *direction, color=colors[cam_idx], arrow_length_ratio=0.3)\n",
    "    \n",
    "    ax2.set_title('SfM Reconstruction')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "    ax2.set_zlabel('Z')\n",
    "    ax2.view_init(elev=30, azim=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_sfm_result(scene, sfm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. トラック（マルチビュー対応）\n",
    "\n",
    "### 7.1 トラックとは\n",
    "\n",
    "**トラック**は、同じ3D点に対応する複数の画像での2D観測点の集合です。\n",
    "\n",
    "```\n",
    "Track:\n",
    "  3D Point X_i\n",
    "    ├── Image 0: (u0, v0)\n",
    "    ├── Image 1: (u1, v1)\n",
    "    ├── Image 3: (u3, v3)\n",
    "    └── Image 5: (u5, v5)\n",
    "```\n",
    "\n",
    "### 7.2 トラックの構築\n",
    "\n",
    "1. 全画像ペアで特徴点マッチング\n",
    "2. 推移的なマッチング（A-B, B-C → A-C）を構築\n",
    "3. 矛盾するマッチを除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    \"\"\"特徴点のトラック（マルチビュー対応）\"\"\"\n",
    "    \n",
    "    def __init__(self, track_id: int):\n",
    "        self.track_id = track_id\n",
    "        self.observations = {}  # {cam_idx: (u, v), ...}\n",
    "        self.point_3d = None\n",
    "    \n",
    "    def add_observation(self, cam_idx: int, uv: Tuple[float, float]):\n",
    "        self.observations[cam_idx] = uv\n",
    "    \n",
    "    def get_camera_indices(self) -> List[int]:\n",
    "        return list(self.observations.keys())\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.observations)\n",
    "\n",
    "def build_tracks(scene: SyntheticScene) -> List[Track]:\n",
    "    \"\"\"合成シーンからトラックを構築\"\"\"\n",
    "    tracks = []\n",
    "    \n",
    "    for pt_idx in range(scene.n_points):\n",
    "        track = Track(pt_idx)\n",
    "        \n",
    "        for cam_idx in range(scene.n_cameras):\n",
    "            if scene.visibility[cam_idx][pt_idx]:\n",
    "                uv = scene.projections[cam_idx][pt_idx]\n",
    "                track.add_observation(cam_idx, tuple(uv))\n",
    "        \n",
    "        if len(track) >= 2:  # 少なくとも2視点で見える\n",
    "            tracks.append(track)\n",
    "    \n",
    "    return tracks\n",
    "\n",
    "# トラックの構築と統計\n",
    "tracks = build_tracks(scene)\n",
    "\n",
    "print(f\"トラック数: {len(tracks)}\")\n",
    "\n",
    "track_lengths = [len(t) for t in tracks]\n",
    "print(f\"トラック長の分布:\")\n",
    "for length in range(2, max(track_lengths) + 1):\n",
    "    count = track_lengths.count(length)\n",
    "    if count > 0:\n",
    "        print(f\"  {length}視点: {count} トラック\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 再投影誤差の計算\n",
    "\n",
    "### 8.1 定義\n",
    "\n",
    "3D点 $\\mathbf{X}$ をカメラ $i$ に投影した点と、実際の観測点との差：\n",
    "\n",
    "$$e_i = \\|\\mathbf{x}_i - \\pi(\\mathbf{P}_i, \\mathbf{X})\\|$$\n",
    "\n",
    "### 8.2 全体の誤差\n",
    "\n",
    "$$E = \\sum_i \\sum_j \\|\\mathbf{x}_{ij} - \\pi(\\mathbf{P}_i, \\mathbf{X}_j)\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reprojection_error(sfm_result: Dict, scene: SyntheticScene) -> Dict:\n",
    "    \"\"\"再投影誤差を計算\"\"\"\n",
    "    K = scene.K\n",
    "    errors_per_camera = defaultdict(list)\n",
    "    all_errors = []\n",
    "    \n",
    "    for i, pt_idx in enumerate(sfm_result['point_indices']):\n",
    "        X = sfm_result['points_3d'][i]\n",
    "        X_h = np.append(X, 1)\n",
    "        \n",
    "        for cam_idx, (R, t) in sfm_result['cameras'].items():\n",
    "            if not scene.visibility[cam_idx][pt_idx]:\n",
    "                continue\n",
    "            \n",
    "            P = K @ np.hstack([R, t.reshape(-1, 1)])\n",
    "            x_proj_h = P @ X_h\n",
    "            x_proj = x_proj_h[:2] / x_proj_h[2]\n",
    "            \n",
    "            x_obs = scene.projections[cam_idx][pt_idx]\n",
    "            error = np.linalg.norm(x_proj - x_obs)\n",
    "            \n",
    "            errors_per_camera[cam_idx].append(error)\n",
    "            all_errors.append(error)\n",
    "    \n",
    "    return {\n",
    "        'per_camera': {k: np.array(v) for k, v in errors_per_camera.items()},\n",
    "        'all': np.array(all_errors)\n",
    "    }\n",
    "\n",
    "reproj_errors = compute_reprojection_error(sfm_result, scene)\n",
    "\n",
    "print(\"=== 再投影誤差 ===\")\n",
    "print(f\"全体:\")\n",
    "print(f\"  平均: {np.mean(reproj_errors['all']):.2f} pixels\")\n",
    "print(f\"  中央値: {np.median(reproj_errors['all']):.2f} pixels\")\n",
    "print(f\"  最大: {np.max(reproj_errors['all']):.2f} pixels\")\n",
    "\n",
    "print(f\"\\nカメラ別:\")\n",
    "for cam_idx in sorted(reproj_errors['per_camera'].keys()):\n",
    "    errors = reproj_errors['per_camera'][cam_idx]\n",
    "    print(f\"  Camera {cam_idx}: mean={np.mean(errors):.2f}, n={len(errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reprojection_errors(reproj_errors: Dict):\n",
    "    \"\"\"再投影誤差の可視化\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 全体のヒストグラム\n",
    "    axes[0].hist(reproj_errors['all'], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(x=np.mean(reproj_errors['all']), color='red', linestyle='--',\n",
    "                    label=f\"Mean: {np.mean(reproj_errors['all']):.2f}px\")\n",
    "    axes[0].set_xlabel('Reprojection Error (pixels)', fontsize=12)\n",
    "    axes[0].set_ylabel('Count', fontsize=12)\n",
    "    axes[0].set_title('Distribution of Reprojection Errors', fontsize=12)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # カメラ別の箱ひげ図\n",
    "    camera_indices = sorted(reproj_errors['per_camera'].keys())\n",
    "    data = [reproj_errors['per_camera'][i] for i in camera_indices]\n",
    "    \n",
    "    axes[1].boxplot(data, labels=[f'Cam {i}' for i in camera_indices])\n",
    "    axes[1].set_xlabel('Camera', fontsize=12)\n",
    "    axes[1].set_ylabel('Reprojection Error (pixels)', fontsize=12)\n",
    "    axes[1].set_title('Reprojection Errors by Camera', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_reprojection_errors(reproj_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. まとめと次のステップ\n",
    "\n",
    "### 学んだこと\n",
    "\n",
    "1. **SfM の概要**: 画像からStructure（3D点群）とMotion（カメラ姿勢）を復元\n",
    "2. **インクリメンタル SfM**: 2視点で初期化し、画像を1枚ずつ追加\n",
    "3. **2視点初期化**: 本質行列 → カメラ姿勢 → 三角測量\n",
    "4. **PnP 問題**: 既知の3D点から新しいカメラ姿勢を推定\n",
    "5. **トラック**: マルチビューでの特徴点対応\n",
    "6. **再投影誤差**: 復元品質の評価指標\n",
    "\n",
    "### SfM パイプラインの要点\n",
    "\n",
    "| ステップ | 入力 | 出力 | 主要アルゴリズム |\n",
    "|----------|------|------|------------------|\n",
    "| 特徴点検出 | 画像 | キーポイント | SIFT, ORB |\n",
    "| マッチング | キーポイント | 対応点 | FLANN, Ratio test |\n",
    "| 初期化 | 2画像の対応点 | 初期カメラ・3D点 | 8点法, RANSAC |\n",
    "| カメラ追加 | 3D-2D対応 | 新カメラ姿勢 | PnP, RANSAC |\n",
    "| 点追加 | 新カメラ + 既存カメラ | 新3D点 | 三角測量 |\n",
    "| 最適化 | 全カメラ・全点 | 最適化された復元 | バンドル調整 |\n",
    "\n",
    "### 次のノートブック\n",
    "\n",
    "**60. バンドル調整**では：\n",
    "- 再投影誤差の最小化\n",
    "- スパース性の活用\n",
    "- Levenberg-Marquardt法\n",
    "- Ceres Solver の紹介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. 自己評価クイズ\n",
    "\n",
    "以下の質問に答えて理解度を確認しましょう：\n",
    "\n",
    "1. Structure from Motion の「Structure」と「Motion」はそれぞれ何を指しますか？\n",
    "\n",
    "2. インクリメンタル SfM と グローバル SfM の違いは？\n",
    "\n",
    "3. 2視点初期化で本質行列から4つの解候補が出る理由と、正しい解を選ぶ方法は？\n",
    "\n",
    "4. PnP 問題とは何ですか？なぜ SfM で重要ですか？\n",
    "\n",
    "5. トラックとは何ですか？長いトラックが望ましい理由は？\n",
    "\n",
    "6. 再投影誤差が大きい場合、考えられる原因は？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クイズの解答（隠し）\n",
    "def show_quiz_answers():\n",
    "    answers = \"\"\"\n",
    "    === 自己評価クイズ解答 ===\n",
    "    \n",
    "    1. Structure と Motion:\n",
    "       - Structure: 3D点群（シーンの幾何学的構造）\n",
    "       - Motion: カメラの姿勢（位置と向き）\n",
    "       - 両者を同時に復元するのがSfM\n",
    "    \n",
    "    2. インクリメンタル vs グローバル:\n",
    "       - インクリメンタル: 画像を1枚ずつ追加、累積的な誤差が問題\n",
    "       - グローバル: 全画像の関係を同時に考慮、初期化が難しい\n",
    "       - インクリメンタルは実装が容易で広く使われている\n",
    "    \n",
    "    3. 4つの解候補:\n",
    "       - 本質行列のSVD分解での符号の不定性から4通りの(R, t)が出る\n",
    "       - 正しい解を選ぶ: チェイラリティ条件（両カメラの前方に点がある）\n",
    "       - 三角測量して両カメラでZ > 0の点が最も多い解を選択\n",
    "    \n",
    "    4. PnP 問題:\n",
    "       - 既知の3D点と対応する2D画像点からカメラ姿勢を推定\n",
    "       - SfMでの重要性: 新しい画像を追加する際に必要\n",
    "       - 最低6点（DLT）または4点（P3P）で解ける\n",
    "    \n",
    "    5. トラック:\n",
    "       - 同じ3D点に対応する複数画像での2D観測点の集合\n",
    "       - 長いトラックが望ましい理由:\n",
    "         - より多くの拘束条件を提供\n",
    "         - バンドル調整での安定性向上\n",
    "         - ドリフトの抑制\n",
    "    \n",
    "    6. 再投影誤差が大きい原因:\n",
    "       - 特徴点マッチングの誤り（外れ値）\n",
    "       - カメラキャリブレーションの誤差\n",
    "       - レンズ歪みの補正不足\n",
    "       - 動く物体（静的シーン仮定の違反）\n",
    "       - バンドル調整の未実行または収束不足\n",
    "    \"\"\"\n",
    "    print(answers)\n",
    "\n",
    "# 解答を見るには以下のコメントを外して実行\n",
    "# show_quiz_answers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ナビゲーション\n",
    "\n",
    "- **前のノートブック**: [58. 特徴点検出とマッチング](58_feature_detection_matching_v1.ipynb)\n",
    "- **次のノートブック**: [60. バンドル調整](60_bundle_adjustment_v1.ipynb)\n",
    "- **カリキュラム**: [CURRICULUM_UNIT_0.3.md](CURRICULUM_UNIT_0.3.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
