{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 115: 高度な最適化テクニック\n",
    "\n",
    "## Advanced Optimization Techniques\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Phase 10「最適化手法」** の第6章として、最新の高度な最適化テクニックを学びます。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **Gradient Clipping** を理解する\n",
    "2. **SAM（Sharpness-Aware Minimization）** を学ぶ\n",
    "3. **LARS/LAMB** を理解する\n",
    "4. **Lookahead** オプティマイザを学ぶ\n",
    "5. **Gradient Accumulation** を理解する\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 110-114 の内容\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [Gradient Clipping](#1-gradient-clipping)\n",
    "2. [SAM（Sharpness-Aware Minimization）](#2-samsharpness-aware-minimization)\n",
    "3. [LARS（Layer-wise Adaptive Rate Scaling）](#3-larslayer-wise-adaptive-rate-scaling)\n",
    "4. [LAMB（Layer-wise Adaptive Moments for Batch training）](#4-lamblayer-wise-adaptive-moments-for-batch-training)\n",
    "5. [Lookahead Optimizer](#5-lookahead-optimizer)\n",
    "6. [Gradient Accumulation](#6-gradient-accumulation)\n",
    "7. [混合精度学習](#7-混合精度学習)\n",
    "8. [演習問題](#8-演習問題)\n",
    "9. [まとめ](#9-まとめ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Gradient Clipping\n",
    "\n",
    "### 1.1 問題: 勾配爆発\n",
    "\n",
    "RNNなどでは勾配が非常に大きくなることがあり、学習が不安定になります。\n",
    "\n",
    "### 1.2 解決策\n",
    "\n",
    "勾配のノルムが閾値を超えた場合、スケーリングします。\n",
    "\n",
    "$$\n",
    "g \\leftarrow \\begin{cases}\n",
    "g & \\text{if } \\|g\\| \\le \\tau \\\\\n",
    "\\frac{\\tau}{\\|g\\|} g & \\text{if } \\|g\\| > \\tau\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grad_norm(gradients, max_norm=1.0):\n",
    "    \"\"\"\n",
    "    勾配クリッピング（ノルムベース）\n",
    "    \n",
    "    Args:\n",
    "        gradients: 勾配のリスト\n",
    "        max_norm: 最大ノルム\n",
    "    \n",
    "    Returns:\n",
    "        クリッピング後の勾配, 元のノルム\n",
    "    \"\"\"\n",
    "    # 全勾配のノルムを計算\n",
    "    total_norm = np.sqrt(sum(np.sum(g**2) for g in gradients))\n",
    "    \n",
    "    # クリッピング係数\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    if clip_coef < 1:\n",
    "        gradients = [g * clip_coef for g in gradients]\n",
    "    \n",
    "    return gradients, total_norm\n",
    "\n",
    "\n",
    "def clip_grad_value(gradients, clip_value=1.0):\n",
    "    \"\"\"\n",
    "    勾配クリッピング（値ベース）\n",
    "    各要素を [-clip_value, clip_value] にクリップ\n",
    "    \"\"\"\n",
    "    return [np.clip(g, -clip_value, clip_value) for g in gradients]\n",
    "\n",
    "\n",
    "# デモンストレーション\n",
    "np.random.seed(42)\n",
    "\n",
    "# 大きな勾配をシミュレート\n",
    "gradients = [np.random.randn(100) * 10]  # ノルム約100\n",
    "\n",
    "print(f\"元の勾配ノルム: {np.linalg.norm(gradients[0]):.2f}\")\n",
    "\n",
    "# ノルムベースクリッピング\n",
    "clipped_norm, original_norm = clip_grad_norm(gradients.copy(), max_norm=1.0)\n",
    "print(f\"クリッピング後のノルム: {np.linalg.norm(clipped_norm[0]):.2f}\")\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(gradients[0], bins=30, alpha=0.7, label='元の勾配')\n",
    "axes[0].hist(clipped_norm[0], bins=30, alpha=0.7, label='クリッピング後')\n",
    "axes[0].set_xlabel('勾配値')\n",
    "axes[0].set_ylabel('頻度')\n",
    "axes[0].set_title('ノルムベースクリッピング')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 値ベースクリッピング\n",
    "clipped_value = clip_grad_value(gradients.copy(), clip_value=5.0)\n",
    "axes[1].hist(gradients[0], bins=30, alpha=0.7, label='元の勾配')\n",
    "axes[1].hist(clipped_value[0], bins=30, alpha=0.7, label='クリッピング後')\n",
    "axes[1].set_xlabel('勾配値')\n",
    "axes[1].set_ylabel('頻度')\n",
    "axes[1].set_title('値ベースクリッピング')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. SAM（Sharpness-Aware Minimization）\n",
    "\n",
    "### 2.1 アイデア\n",
    "\n",
    "**損失曲面の「鋭さ」** を考慮した最適化。平坦な極小点に収束することで汎化性能を向上。\n",
    "\n",
    "### 2.2 数式\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\max_{\\|\\epsilon\\| \\le \\rho} L(\\theta + \\epsilon)\n",
    "$$\n",
    "\n",
    "2ステップの近似：\n",
    "1. 摂動を計算: $\\epsilon = \\rho \\cdot \\nabla L / \\|\\nabla L\\|$\n",
    "2. 摂動位置での勾配で更新: $\\theta \\leftarrow \\theta - \\eta \\nabla L(\\theta + \\epsilon)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM:\n",
    "    \"\"\"\n",
    "    Sharpness-Aware Minimization\n",
    "    \n",
    "    1. 現在の勾配でε方向を計算\n",
    "    2. θ+ε位置での勾配で更新\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_optimizer, rho=0.05):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_optimizer: ベースのオプティマイザ（SGD, Adam等）\n",
    "            rho: 摂動の大きさ\n",
    "        \"\"\"\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.rho = rho\n",
    "    \n",
    "    def first_step(self, params, gradients):\n",
    "        \"\"\"摂動を計算してパラメータに適用\"\"\"\n",
    "        # 勾配ノルム\n",
    "        grad_norm = np.sqrt(sum(np.sum(g**2) for g in gradients))\n",
    "        \n",
    "        # 摂動を計算\n",
    "        self.e_w = [self.rho * g / (grad_norm + 1e-12) for g in gradients]\n",
    "        \n",
    "        # パラメータに摂動を加える\n",
    "        for p, e in zip(params, self.e_w):\n",
    "            p += e\n",
    "    \n",
    "    def second_step(self, params, gradients):\n",
    "        \"\"\"摂動を元に戻し、ベースオプティマイザで更新\"\"\"\n",
    "        # 摂動を元に戻す\n",
    "        for p, e in zip(params, self.e_w):\n",
    "            p -= e\n",
    "        \n",
    "        # ベースオプティマイザで更新\n",
    "        self.base_optimizer.update(params, gradients)\n",
    "\n",
    "\n",
    "# SAMの効果を可視化\n",
    "\n",
    "def sharp_loss(x, y):\n",
    "    \"\"\"鋭い極小点を持つ損失関数\"\"\"\n",
    "    return (x - 1)**2 + 10 * np.sin(5 * x)**2 + (y - 1)**2 + 10 * np.sin(5 * y)**2\n",
    "\n",
    "def sharp_loss_grad(x, y):\n",
    "    dx = 2 * (x - 1) + 100 * np.sin(5 * x) * np.cos(5 * x)\n",
    "    dy = 2 * (y - 1) + 100 * np.sin(5 * y) * np.cos(5 * y)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "\n",
    "# 損失曲面の可視化\n",
    "x = np.linspace(-1, 3, 200)\n",
    "y = np.linspace(-1, 3, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = sharp_loss(X, Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contour = ax.contour(X, Y, Z, levels=50, cmap='viridis')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('鋭い極小点を持つ損失曲面')\n",
    "plt.colorbar(contour)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【SAMの利点】\")\n",
    "print(\"- 平坦な極小点に収束しやすい\")\n",
    "print(\"- 汎化性能が向上\")\n",
    "print(\"- 計算コストは約2倍（2回の順伝播/逆伝播）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. LARS（Layer-wise Adaptive Rate Scaling）\n",
    "\n",
    "### 3.1 アイデア\n",
    "\n",
    "大きなバッチサイズでの学習を可能にするため、**層ごとに学習率を調整** します。\n",
    "\n",
    "### 3.2 数式\n",
    "\n",
    "$$\n",
    "\\lambda^l = \\eta \\cdot \\frac{\\|\\theta^l\\|}{\\|\\nabla L^l\\| + \\beta \\|\\theta^l\\|}\n",
    "$$\n",
    "\n",
    "各層の重みと勾配のノルムの比率で学習率をスケーリング。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LARS:\n",
    "    \"\"\"\n",
    "    Layer-wise Adaptive Rate Scaling\n",
    "    \n",
    "    大バッチ学習のための層ごとの学習率スケーリング\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.1, momentum=0.9, weight_decay=1e-4, trust_coef=0.001):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.trust_coef = trust_coef\n",
    "        self.velocities = {}\n",
    "    \n",
    "    def update(self, layer_params, layer_grads, layer_names):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layer_params: 層ごとのパラメータのリスト\n",
    "            layer_grads: 層ごとの勾配のリスト\n",
    "            layer_names: 層の名前のリスト\n",
    "        \"\"\"\n",
    "        for name, param, grad in zip(layer_names, layer_params, layer_grads):\n",
    "            # 層ごとのノルム\n",
    "            param_norm = np.linalg.norm(param)\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "            \n",
    "            # 信頼比率（trust ratio）\n",
    "            if param_norm > 0 and grad_norm > 0:\n",
    "                local_lr = self.trust_coef * param_norm / (grad_norm + self.weight_decay * param_norm)\n",
    "            else:\n",
    "                local_lr = 1.0\n",
    "            \n",
    "            # Weight decay を加える\n",
    "            grad = grad + self.weight_decay * param\n",
    "            \n",
    "            # Momentum\n",
    "            if name not in self.velocities:\n",
    "                self.velocities[name] = np.zeros_like(param)\n",
    "            \n",
    "            self.velocities[name] = self.momentum * self.velocities[name] + self.lr * local_lr * grad\n",
    "            \n",
    "            # パラメータ更新\n",
    "            param -= self.velocities[name]\n",
    "\n",
    "\n",
    "print(\"【LARSの用途】\")\n",
    "print(\"- 大規模なバッチサイズ（8192, 32768等）での学習\")\n",
    "print(\"- ImageNet学習の高速化（30分で学習完了等）\")\n",
    "print(\"\")\n",
    "print(\"【なぜ大バッチで問題が起きるか】\")\n",
    "print(\"- 学習率を線形にスケールすると発散\")\n",
    "print(\"- 層によって最適な学習率が異なる\")\n",
    "print(\"- LARSは各層の状態に応じて学習率を調整\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. LAMB（Layer-wise Adaptive Moments for Batch training）\n",
    "\n",
    "### 4.1 アイデア\n",
    "\n",
    "LARSをAdamに拡張したもの。BERTなどの大規模モデルの学習に使用。\n",
    "\n",
    "### 4.2 数式\n",
    "\n",
    "$$\n",
    "r = \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon} + \\lambda \\theta\n",
    "$$\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\cdot \\frac{\\|\\theta\\|}{\\|r\\|} \\cdot r\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAMB:\n",
    "    \"\"\"\n",
    "    Layer-wise Adaptive Moments for Batch training\n",
    "    \n",
    "    LARS + Adam\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-6, weight_decay=0.01):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "    \n",
    "    def update(self, layer_params, layer_grads, layer_names):\n",
    "        self.t += 1\n",
    "        \n",
    "        for name, param, grad in zip(layer_names, layer_params, layer_grads):\n",
    "            # Adamのモーメント\n",
    "            if name not in self.m:\n",
    "                self.m[name] = np.zeros_like(param)\n",
    "                self.v[name] = np.zeros_like(param)\n",
    "            \n",
    "            self.m[name] = self.beta1 * self.m[name] + (1 - self.beta1) * grad\n",
    "            self.v[name] = self.beta2 * self.v[name] + (1 - self.beta2) * grad ** 2\n",
    "            \n",
    "            m_hat = self.m[name] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[name] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Adam更新 + Weight Decay\n",
    "            r = m_hat / (np.sqrt(v_hat) + self.eps) + self.weight_decay * param\n",
    "            \n",
    "            # 層ごとの信頼比率\n",
    "            param_norm = np.linalg.norm(param)\n",
    "            r_norm = np.linalg.norm(r)\n",
    "            \n",
    "            if param_norm > 0 and r_norm > 0:\n",
    "                trust_ratio = param_norm / r_norm\n",
    "            else:\n",
    "                trust_ratio = 1.0\n",
    "            \n",
    "            # パラメータ更新\n",
    "            param -= self.lr * trust_ratio * r\n",
    "\n",
    "\n",
    "print(\"【LAMBの用途】\")\n",
    "print(\"- BERT, GPTなどの大規模言語モデルの学習\")\n",
    "print(\"- バッチサイズ 64K 以上での学習\")\n",
    "print(\"\")\n",
    "print(\"【LARS vs LAMB】\")\n",
    "print(\"- LARS: SGD + Momentum ベース → 画像分類向け\")\n",
    "print(\"- LAMB: Adam ベース → NLP/Transformer向け\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Lookahead Optimizer\n",
    "\n",
    "### 5.1 アイデア\n",
    "\n",
    "「速い重み」と「遅い重み」の2つを維持し、定期的に補間することで学習を安定化。\n",
    "\n",
    "### 5.2 アルゴリズム\n",
    "\n",
    "1. 内部ループ: ベースオプティマイザでk回更新\n",
    "2. 外部ループ: 遅い重みを速い重みの方向に補間\n",
    "\n",
    "$$\n",
    "\\phi_{t+1} = \\phi_t + \\alpha (\\theta_{t,k} - \\phi_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead:\n",
    "    \"\"\"\n",
    "    Lookahead Optimizer\n",
    "    \n",
    "    速い重みθと遅い重みφを維持\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_optimizer, k=5, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_optimizer: 内部ループのオプティマイザ\n",
    "            k: 内部ループの回数\n",
    "            alpha: 補間係数\n",
    "        \"\"\"\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.step_counter = 0\n",
    "        self.slow_params = None\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        # 初期化\n",
    "        if self.slow_params is None:\n",
    "            self.slow_params = [p.copy() for p in params]\n",
    "        \n",
    "        # ベースオプティマイザで更新（内部ループ）\n",
    "        self.base_optimizer.update(params, gradients)\n",
    "        \n",
    "        self.step_counter += 1\n",
    "        \n",
    "        # k回ごとに遅い重みを更新（外部ループ）\n",
    "        if self.step_counter % self.k == 0:\n",
    "            for slow_p, fast_p in zip(self.slow_params, params):\n",
    "                # 遅い重みを速い重みの方向に補間\n",
    "                slow_p += self.alpha * (fast_p - slow_p)\n",
    "                # 速い重みを遅い重みにリセット\n",
    "                fast_p[:] = slow_p\n",
    "\n",
    "\n",
    "# Lookaheadの効果を可視化\n",
    "\n",
    "def noisy_loss(x, y):\n",
    "    return (x - 2)**2 + (y - 2)**2 + 0.5 * np.sin(10 * x) * np.sin(10 * y)\n",
    "\n",
    "def noisy_loss_grad(x, y):\n",
    "    dx = 2 * (x - 2) + 5 * np.cos(10 * x) * np.sin(10 * y)\n",
    "    dy = 2 * (y - 2) + 5 * np.sin(10 * x) * np.cos(10 * y)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "\n",
    "# 等高線\n",
    "x = np.linspace(-1, 5, 200)\n",
    "y = np.linspace(-1, 5, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = noisy_loss(X, Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contour = ax.contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.7)\n",
    "ax.scatter([2], [2], color='red', s=100, marker='*', zorder=5, label='最適点')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('ノイズのある損失曲面')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【Lookaheadの利点】\")\n",
    "print(\"- 学習の安定化\")\n",
    "print(\"- 過度な振動の抑制\")\n",
    "print(\"- 任意のベースオプティマイザと組み合わせ可能\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Gradient Accumulation\n",
    "\n",
    "### 6.1 アイデア\n",
    "\n",
    "メモリ制約がある場合、小さなバッチの勾配を複数回累積してから更新。\n",
    "\n",
    "→ 実効的なバッチサイズを大きくする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAccumulator:\n",
    "    \"\"\"\n",
    "    勾配累積\n",
    "    \n",
    "    小さなバッチの勾配を累積し、一定回数後に更新\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, accumulation_steps=4):\n",
    "        self.optimizer = optimizer\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.accumulated_grads = None\n",
    "        self.step_counter = 0\n",
    "    \n",
    "    def step(self, params, gradients):\n",
    "        # 勾配を累積\n",
    "        if self.accumulated_grads is None:\n",
    "            self.accumulated_grads = [np.zeros_like(g) for g in gradients]\n",
    "        \n",
    "        for acc_g, g in zip(self.accumulated_grads, gradients):\n",
    "            acc_g += g / self.accumulation_steps\n",
    "        \n",
    "        self.step_counter += 1\n",
    "        \n",
    "        # accumulation_steps回ごとに実際に更新\n",
    "        if self.step_counter % self.accumulation_steps == 0:\n",
    "            self.optimizer.update(params, self.accumulated_grads)\n",
    "            self.accumulated_grads = None\n",
    "            return True  # 更新が行われた\n",
    "        \n",
    "        return False  # 累積中\n",
    "\n",
    "\n",
    "print(\"【Gradient Accumulationの使い方】\")\n",
    "print(\"\")\n",
    "print(\"例: 目標バッチサイズ 256、GPU メモリで 64 しか入らない場合\")\n",
    "print(\"\")\n",
    "print(\"accumulation_steps = 256 // 64 = 4\")\n",
    "print(\"\")\n",
    "print(\"for i, (x, y) in enumerate(dataloader):  # batch_size=64\")\n",
    "print(\"    loss = model(x, y)\")\n",
    "print(\"    loss.backward()  # 勾配を計算\")\n",
    "print(\"    \")\n",
    "print(\"    if (i + 1) % 4 == 0:  # 4回ごとに更新\")\n",
    "print(\"        optimizer.step()\")\n",
    "print(\"        optimizer.zero_grad()\")\n",
    "print(\"\")\n",
    "print(\"→ 実効バッチサイズ: 64 × 4 = 256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 混合精度学習\n",
    "\n",
    "### 7.1 アイデア\n",
    "\n",
    "計算にFP16（半精度）を使用し、勾配累積やパラメータ更新にはFP32を使用。\n",
    "\n",
    "→ メモリ削減と計算高速化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"【混合精度学習のメリット】\")\n",
    "print(\"\")\n",
    "print(\"1. メモリ使用量の削減\")\n",
    "print(\"   - FP16は FP32 の半分のメモリ\")\n",
    "print(\"   - より大きなバッチサイズが可能\")\n",
    "print(\"\")\n",
    "print(\"2. 計算速度の向上\")\n",
    "print(\"   - Tensor Cores（NVIDIA GPU）による高速化\")\n",
    "print(\"   - 最大2-3倍のスピードアップ\")\n",
    "print(\"\")\n",
    "print(\"3. 注意点\")\n",
    "print(\"   - 勾配のアンダーフロー → Loss Scaling で対処\")\n",
    "print(\"   - 精度の損失 → マスターウェイトをFP32で維持\")\n",
    "print(\"\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\")\n",
    "print(\"【Loss Scaling】\")\n",
    "print(\"\")\n",
    "print(\"1. 損失を大きな値でスケール: loss_scaled = loss * scale\")\n",
    "print(\"2. 逆伝播\")\n",
    "print(\"3. 勾配を元に戻す: grad = grad / scale\")\n",
    "print(\"4. 更新\")\n",
    "print(\"\")\n",
    "print(\"→ 小さな勾配がアンダーフローするのを防ぐ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 演習問題\n",
    "\n",
    "### 演習 8.1: SAMの実装と比較\n",
    "\n",
    "SGDとSAMを比較し、SAMが平坦な極小点に収束することを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.1: 解答欄\n",
    "\n",
    "# TODO: SAMの完全な実装とテスト\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 8.2: Lookahead + Adam (Ranger)\n",
    "\n",
    "Lookahead と Adam を組み合わせた「Ranger」オプティマイザを実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.2: 解答欄\n",
    "\n",
    "# TODO: Ranger = Lookahead(RAdam) の実装\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. まとめ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **Gradient Clipping**: 勾配爆発を防ぐ\n",
    "\n",
    "2. **SAM**: 平坦な極小点への収束で汎化性能向上\n",
    "\n",
    "3. **LARS/LAMB**: 大バッチ学習のための層ごとの学習率調整\n",
    "\n",
    "4. **Lookahead**: 速い重みと遅い重みの組み合わせで安定化\n",
    "\n",
    "5. **Gradient Accumulation**: 実効バッチサイズの拡大\n",
    "\n",
    "6. **混合精度学習**: メモリ効率と速度の向上\n",
    "\n",
    "### 高度なテクニックの選択指針\n",
    "\n",
    "| テクニック | 用途 |\n",
    "|-----------|------|\n",
    "| Gradient Clipping | RNN、不安定な学習 |\n",
    "| SAM | 汎化性能が重要な場合 |\n",
    "| LARS | 大バッチ画像分類 |\n",
    "| LAMB | 大バッチNLP/Transformer |\n",
    "| Lookahead | 学習の安定化 |\n",
    "| Gradient Accumulation | メモリ制約下での大バッチ |\n",
    "| 混合精度 | 高速化、メモリ効率 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Foret, P., et al. (2021). Sharpness-aware minimization for efficiently improving generalization. *ICLR*.\n",
    "2. You, Y., et al. (2017). Large batch training of convolutional networks. *arXiv:1708.03888*.\n",
    "3. You, Y., et al. (2020). Large batch optimization for deep learning: Training BERT in 76 minutes. *ICLR*.\n",
    "4. Zhang, M., et al. (2019). Lookahead optimizer: k steps forward, 1 step back. *NeurIPS*.\n",
    "5. Micikevicius, P., et al. (2018). Mixed precision training. *ICLR*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
