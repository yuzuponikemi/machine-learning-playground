{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 116: オプティマイザの選び方と実践ガイド\n",
    "\n",
    "## Optimizer Selection Guide and Best Practices\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Phase 10「最適化手法」** の最終章として、これまで学んだ内容を統合し、実践的なオプティマイザ選択ガイドを提供します。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. タスクに応じた **オプティマイザの選択基準** を理解する\n",
    "2. **ハイパーパラメータチューニング** のベストプラクティスを学ぶ\n",
    "3. **デバッグと診断** の方法を習得する\n",
    "4. **実践的なレシピ** を身につける\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 110-115 の内容\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [オプティマイザ選択フローチャート](#1-オプティマイザ選択フローチャート)\n",
    "2. [タスク別推奨設定](#2-タスク別推奨設定)\n",
    "3. [ハイパーパラメータチューニング](#3-ハイパーパラメータチューニング)\n",
    "4. [学習のデバッグと診断](#4-学習のデバッグと診断)\n",
    "5. [実践的なレシピ集](#5-実践的なレシピ集)\n",
    "6. [よくある問題と解決策](#6-よくある問題と解決策)\n",
    "7. [Phase 10 総まとめ](#7-phase-10-総まとめ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. オプティマイザ選択フローチャート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"オプティマイザ選択フローチャート\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "                              [START]\n",
    "                                 |\n",
    "                    ┌────────────┴────────────┐\n",
    "                    │   どんなタスク？          │\n",
    "                    └────────────┬────────────┘\n",
    "          ┌──────────────────────┼──────────────────────┐\n",
    "          │                      │                      │\n",
    "     [画像分類]             [NLP/Transformer]      [その他/不明]\n",
    "          │                      │                      │\n",
    "     ┌────┴────┐            ┌────┴────┐                 │\n",
    "     │バッチは？│            │バッチは？│                 │\n",
    "     └────┬────┘            └────┬────┘                 │\n",
    "    ┌─────┴─────┐          ┌─────┴─────┐                │\n",
    "  [小~中]    [大]        [小~中]     [大]               │\n",
    "    │         │            │          │                 │\n",
    "  SGD+       LARS        AdamW       LAMB               │\n",
    " Momentum                                               │\n",
    "                                                        │\n",
    "                                              ┌─────────┴─────────┐\n",
    "                                              │  まずは Adam で   │\n",
    "                                              │  試してみる       │\n",
    "                                              └───────────────────┘\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"【補足】\")\n",
    "print(\"- 小~中バッチ: 32-256\")\n",
    "print(\"- 大バッチ: 1024以上\")\n",
    "print(\"- 迷ったら Adam から始める\")\n",
    "print(\"- ファインチューニングでは AdamW + 小さな学習率\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. タスク別推奨設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"タスク別推奨設定\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "\n",
    "print(\"【1. 画像分類（CNN）】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"オプティマイザ: SGD + Momentum (0.9)\")\n",
    "print(\"学習率: 0.1\")\n",
    "print(\"Weight Decay: 1e-4\")\n",
    "print(\"スケジューラ: MultiStepLR (milestones=[30, 60, 90])\")\n",
    "print(\"バッチサイズ: 128-256\")\n",
    "print(\"エポック数: 90-200\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"【2. 物体検出（YOLO, Faster R-CNN等）】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"オプティマイザ: SGD + Momentum (0.9)\")\n",
    "print(\"学習率: 0.01\")\n",
    "print(\"Weight Decay: 5e-4\")\n",
    "print(\"スケジューラ: Warmup + Step/Cosine\")\n",
    "print(\"バッチサイズ: 16-64（メモリ制約）\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"【3. NLP - Transformer（BERT等）】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"オプティマイザ: AdamW\")\n",
    "print(\"学習率: 1e-4 ~ 5e-4\")\n",
    "print(\"β1=0.9, β2=0.999\")\n",
    "print(\"Weight Decay: 0.01\")\n",
    "print(\"スケジューラ: Linear Warmup + Linear Decay\")\n",
    "print(\"Warmup: 全ステップの10%\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"【4. ファインチューニング】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"オプティマイザ: AdamW\")\n",
    "print(\"学習率: 1e-5 ~ 3e-5（事前学習時の1/10程度）\")\n",
    "print(\"Weight Decay: 0.01\")\n",
    "print(\"スケジューラ: Warmup + Cosine/Linear Decay\")\n",
    "print(\"エポック数: 3-10\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"【5. GAN】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"オプティマイザ: Adam（G, D両方）\")\n",
    "print(\"学習率: 2e-4\")\n",
    "print(\"β1=0.5, β2=0.999（標準とは異なる）\")\n",
    "print(\"Weight Decay: 0\")\n",
    "print(\"バッチサイズ: 64-128\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"【6. 強化学習（PPO等）】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"オプティマイザ: Adam\")\n",
    "print(\"学習率: 3e-4\")\n",
    "print(\"ε: 1e-5 (より大きい値)\")\n",
    "print(\"Gradient Clipping: max_norm=0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ハイパーパラメータチューニング\n",
    "\n",
    "### 3.1 学習率の探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_range_test(loss_fn, grad_fn, start, start_lr=1e-7, end_lr=10, num_iter=100):\n",
    "    \"\"\"\n",
    "    Learning Rate Range Test\n",
    "    \n",
    "    学習率を指数的に増加させながら損失を記録\n",
    "    \"\"\"\n",
    "    pos = np.array(start, dtype=float)\n",
    "    \n",
    "    # 学習率の系列\n",
    "    mult = (end_lr / start_lr) ** (1 / num_iter)\n",
    "    lrs = [start_lr * (mult ** i) for i in range(num_iter)]\n",
    "    \n",
    "    losses = []\n",
    "    smooth_loss = 0\n",
    "    beta = 0.98\n",
    "    \n",
    "    for i, lr in enumerate(lrs):\n",
    "        # 損失と勾配を計算\n",
    "        loss = loss_fn(pos[0], pos[1])\n",
    "        grad = grad_fn(pos[0], pos[1])\n",
    "        \n",
    "        # 指数移動平均\n",
    "        smooth_loss = beta * smooth_loss + (1 - beta) * loss\n",
    "        smooth_loss_corrected = smooth_loss / (1 - beta ** (i + 1))\n",
    "        losses.append(smooth_loss_corrected)\n",
    "        \n",
    "        # 発散チェック\n",
    "        if i > 0 and smooth_loss_corrected > 4 * losses[0]:\n",
    "            break\n",
    "        \n",
    "        # 更新\n",
    "        pos = pos - lr * grad\n",
    "    \n",
    "    return lrs[:len(losses)], losses\n",
    "\n",
    "\n",
    "# テスト関数\n",
    "def rosenbrock(x, y):\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_grad(x, y):\n",
    "    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dy = 200 * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "\n",
    "# LR Range Test\n",
    "lrs, losses = lr_range_test(rosenbrock, rosenbrock_grad, start=(-1.0, 1.0), \n",
    "                            start_lr=1e-6, end_lr=1, num_iter=100)\n",
    "\n",
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(lrs, losses, 'b-', linewidth=2)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('学習率（対数スケール）')\n",
    "ax.set_ylabel('損失')\n",
    "ax.set_title('Learning Rate Range Test')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 推奨学習率の範囲をマーク\n",
    "min_idx = np.argmin(losses)\n",
    "suggested_lr = lrs[min_idx] / 10  # 最小点の1/10\n",
    "ax.axvline(x=suggested_lr, color='red', linestyle='--', label=f'推奨: {suggested_lr:.2e}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【LR Range Testの使い方】\")\n",
    "print(\"1. 損失が最も急激に減少する領域を見つける\")\n",
    "print(\"2. その領域の1/10程度を初期学習率として設定\")\n",
    "print(f\"3. この例では推奨学習率: {suggested_lr:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ハイパーパラメータの優先順位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ハイパーパラメータチューニングの優先順位\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"【優先度: 高】\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. 学習率 (lr)\")\n",
    "print(\"   - 最も重要なパラメータ\")\n",
    "print(\"   - LR Range Testで探索\")\n",
    "print(\"\")\n",
    "print(\"2. バッチサイズ\")\n",
    "print(\"   - メモリが許す限り大きく\")\n",
    "print(\"   - ただし学習率の調整も必要\")\n",
    "print(\"\")\n",
    "print(\"【優先度: 中】\")\n",
    "print(\"-\" * 40)\n",
    "print(\"3. Weight Decay\")\n",
    "print(\"   - 1e-4 ~ 1e-2 の範囲で探索\")\n",
    "print(\"\")\n",
    "print(\"4. モーメンタム係数 (β1)\")\n",
    "print(\"   - 通常 0.9 で固定\")\n",
    "print(\"   - 特殊な場合のみ変更\")\n",
    "print(\"\")\n",
    "print(\"【優先度: 低】\")\n",
    "print(\"-\" * 40)\n",
    "print(\"5. Adam の β2\")\n",
    "print(\"   - 通常 0.999 で固定\")\n",
    "print(\"\")\n",
    "print(\"6. Adam の ε\")\n",
    "print(\"   - 通常 1e-8 で固定\")\n",
    "print(\"   - 精度問題がある場合のみ変更\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 バッチサイズと学習率のスケーリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"【バッチサイズと学習率のスケーリング則】\")\n",
    "print(\"\")\n",
    "print(\"バッチサイズを k 倍にしたとき：\")\n",
    "print(\"\")\n",
    "print(\"  1. Linear Scaling Rule:\")\n",
    "print(\"     lr_new = lr_base × k\")\n",
    "print(\"     例: batch 256→1024 (4倍) なら lr も4倍\")\n",
    "print(\"\")\n",
    "print(\"  2. Square Root Scaling:\")\n",
    "print(\"     lr_new = lr_base × √k\")\n",
    "print(\"     例: batch 256→1024 (4倍) なら lr は2倍\")\n",
    "print(\"\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\")\n",
    "print(\"【どちらを使うか】\")\n",
    "print(\"\")\n",
    "print(\"- Linear Scaling: SGD + Warmup で一般的\")\n",
    "print(\"- Square Root: Adam で推奨されることも\")\n",
    "print(\"- 大バッチ (>1024): LARS/LAMB を使用\")\n",
    "print(\"\")\n",
    "print(\"【注意】\")\n",
    "print(\"- 学習率を大きくしたら必ず Warmup を使う\")\n",
    "print(\"- 線形スケーリングは無限には続かない\")\n",
    "\n",
    "# 可視化\n",
    "batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "base_lr = 0.01\n",
    "base_batch = 32\n",
    "\n",
    "linear_lrs = [base_lr * (bs / base_batch) for bs in batch_sizes]\n",
    "sqrt_lrs = [base_lr * np.sqrt(bs / base_batch) for bs in batch_sizes]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(batch_sizes, linear_lrs, 'b-o', linewidth=2, label='Linear Scaling')\n",
    "ax.plot(batch_sizes, sqrt_lrs, 'r-s', linewidth=2, label='Square Root Scaling')\n",
    "\n",
    "ax.set_xlabel('バッチサイズ')\n",
    "ax.set_ylabel('学習率')\n",
    "ax.set_title('バッチサイズと学習率のスケーリング')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 学習のデバッグと診断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"学習曲線の診断\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"【パターン1: 損失が減少しない】\")\n",
    "print(\"-\" * 40)\n",
    "print(\"症状: 損失がほぼ一定\")\n",
    "print(\"原因: 学習率が小さすぎる / 勾配消失\")\n",
    "print(\"対策: \")\n",
    "print(\"  - 学習率を10倍にしてみる\")\n",
    "print(\"  - 勾配のノルムを確認\")\n",
    "print(\"  - 初期化を確認\")\n",
    "print(\"\")\n",
    "print(\"【パターン2: 損失が振動/発散】\")\n",
    "print(\"-\" * 40)\n",
    "print(\"症状: 損失が大きく上下 / NaN\")\n",
    "print(\"原因: 学習率が大きすぎる\")\n",
    "print(\"対策: \")\n",
    "print(\"  - 学習率を1/10にしてみる\")\n",
    "print(\"  - Gradient Clipping を追加\")\n",
    "print(\"  - Warmup を追加\")\n",
    "print(\"\")\n",
    "print(\"【パターン3: 訓練損失↓、検証損失↑】\")\n",
    "print(\"-\" * 40)\n",
    "print(\"症状: 過学習\")\n",
    "print(\"対策: \")\n",
    "print(\"  - Weight Decay を増やす\")\n",
    "print(\"  - Dropout を追加/強化\")\n",
    "print(\"  - データ拡張を強化\")\n",
    "print(\"  - Early Stopping\")\n",
    "print(\"\")\n",
    "print(\"【パターン4: 両方の損失が高止まり】\")\n",
    "print(\"-\" * 40)\n",
    "print(\"症状: アンダーフィット\")\n",
    "print(\"対策: \")\n",
    "print(\"  - モデルの容量を増やす\")\n",
    "print(\"  - 正則化を減らす\")\n",
    "print(\"  - 学習を長くする\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線のパターンを可視化\n",
    "\n",
    "np.random.seed(42)\n",
    "epochs = np.arange(100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# パターン1: 学習率が小さすぎる\n",
    "train_loss_1 = 2.5 - 0.005 * epochs + np.random.randn(100) * 0.05\n",
    "val_loss_1 = 2.5 - 0.004 * epochs + np.random.randn(100) * 0.08\n",
    "axes[0, 0].plot(epochs, train_loss_1, 'b-', label='Train')\n",
    "axes[0, 0].plot(epochs, val_loss_1, 'r-', label='Val')\n",
    "axes[0, 0].set_title('パターン1: 学習率が小さすぎる')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# パターン2: 学習率が大きすぎる\n",
    "train_loss_2 = 2.0 + 0.5 * np.sin(epochs * 0.5) + np.random.randn(100) * 0.3\n",
    "axes[0, 1].plot(epochs, train_loss_2, 'b-')\n",
    "axes[0, 1].set_title('パターン2: 学習率が大きすぎる（振動）')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# パターン3: 過学習\n",
    "train_loss_3 = 2.0 * np.exp(-0.05 * epochs) + 0.1 + np.random.randn(100) * 0.02\n",
    "val_loss_3 = 2.0 * np.exp(-0.03 * epochs) + 0.3 + 0.01 * epochs + np.random.randn(100) * 0.05\n",
    "axes[1, 0].plot(epochs, train_loss_3, 'b-', label='Train')\n",
    "axes[1, 0].plot(epochs, val_loss_3, 'r-', label='Val')\n",
    "axes[1, 0].set_title('パターン3: 過学習')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# パターン4: 良好な学習\n",
    "train_loss_4 = 2.0 * np.exp(-0.04 * epochs) + 0.1 + np.random.randn(100) * 0.02\n",
    "val_loss_4 = 2.0 * np.exp(-0.035 * epochs) + 0.15 + np.random.randn(100) * 0.03\n",
    "axes[1, 1].plot(epochs, train_loss_4, 'b-', label='Train')\n",
    "axes[1, 1].plot(epochs, val_loss_4, 'r-', label='Val')\n",
    "axes[1, 1].set_title('良好な学習')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 実践的なレシピ集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"実践的なレシピ集\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"【レシピ1: ResNet on ImageNet】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "optimizer = SGD(params, lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_one_epoch()\n",
    "    scheduler.step()\n",
    "\"\"\")\n",
    "\n",
    "print(\"【レシピ2: BERT Fine-tuning】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "optimizer = AdamW(params, lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "total_steps = len(dataloader) * num_epochs\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\"\"\")\n",
    "\n",
    "print(\"【レシピ3: Vision Transformer (ViT)】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "optimizer = AdamW(params, lr=1e-3, betas=(0.9, 0.999), weight_decay=0.3)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=300, eta_min=1e-5)\n",
    "warmup_scheduler = LinearWarmup(optimizer, warmup_epochs=5)\n",
    "\n",
    "for epoch in range(300):\n",
    "    if epoch < 5:\n",
    "        warmup_scheduler.step()\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\"\"\")\n",
    "\n",
    "print(\"【レシピ4: Fast.ai スタイル (One Cycle)】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "# LR Range Test で max_lr を決定\n",
    "max_lr = 1e-2\n",
    "\n",
    "optimizer = Adam(params, lr=max_lr/25)  # 初期LRは max_lr/div_factor\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=len(dataloader) * num_epochs,\n",
    "    pct_start=0.3,\n",
    "    div_factor=25,\n",
    "    final_div_factor=1e4\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. よくある問題と解決策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"よくある問題と解決策\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "\n",
    "print(\"【問題1: NaN が発生する】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"チェックリスト:\")\n",
    "print(\"  □ 学習率が大きすぎないか\")\n",
    "print(\"  □ データに inf/nan が含まれていないか\")\n",
    "print(\"  □ 損失関数で log(0) が発生していないか\")\n",
    "print(\"  □ 勾配爆発が起きていないか\")\n",
    "print(\"解決策:\")\n",
    "print(\"  - Gradient Clipping を追加\")\n",
    "print(\"  - 学習率を下げる\")\n",
    "print(\"  - 混合精度の場合は Loss Scaling を確認\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"【問題2: 損失が一定で減少しない】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"チェックリスト:\")\n",
    "print(\"  □ モデルの勾配が流れているか\")\n",
    "print(\"  □ データローダーが正しく動作しているか\")\n",
    "print(\"  □ 損失関数の実装が正しいか\")\n",
    "print(\"解決策:\")\n",
    "print(\"  - 1バッチだけで過学習できるか確認\")\n",
    "print(\"  - 勾配のノルムをログに出力\")\n",
    "print(\"  - 学習率を上げてみる\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"【問題3: 訓練精度は高いが検証精度が低い】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"チェックリスト:\")\n",
    "print(\"  □ データ拡張は適切か\")\n",
    "print(\"  □ 正則化（Dropout, Weight Decay）は十分か\")\n",
    "print(\"  □ 訓練/検証データの分布は同じか\")\n",
    "print(\"解決策:\")\n",
    "print(\"  - Weight Decay を増やす（0.01 → 0.1）\")\n",
    "print(\"  - Dropout を追加/強化\")\n",
    "print(\"  - データ拡張を強化\")\n",
    "print(\"  - モデルを小さくする\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"【問題4: 学習が遅い】\")\n",
    "print(\"-\" * 60)\n",
    "print(\"チェックリスト:\")\n",
    "print(\"  □ 学習率は最適か\")\n",
    "print(\"  □ バッチサイズは適切か\")\n",
    "print(\"  □ データローダーがボトルネックでないか\")\n",
    "print(\"解決策:\")\n",
    "print(\"  - LR Range Test で最適な学習率を探す\")\n",
    "print(\"  - バッチサイズを大きくする\")\n",
    "print(\"  - num_workers を増やす\")\n",
    "print(\"  - 混合精度学習を使用する\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Phase 10 総まとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Phase 10: Optimization Methods 総まとめ\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"【学んだ内容】\")\n",
    "print(\"\")\n",
    "print(\"Notebook 110: 最適化の基礎と勾配降下法\")\n",
    "print(\"  - 最適化問題の定式化\")\n",
    "print(\"  - 勾配降下法のアルゴリズム\")\n",
    "print(\"  - 学習率の影響\")\n",
    "print(\"  - バッチサイズの選択\")\n",
    "print(\"\")\n",
    "print(\"Notebook 111: Momentum と Nesterov\")\n",
    "print(\"  - SGDの問題点（振動）\")\n",
    "print(\"  - Momentum による改善\")\n",
    "print(\"  - Nesterov の先読み\")\n",
    "print(\"\")\n",
    "print(\"Notebook 112: 適応学習率手法\")\n",
    "print(\"  - Adagrad の仕組みと問題点\")\n",
    "print(\"  - RMSprop による改善\")\n",
    "print(\"  - Adam の完全な理解\")\n",
    "print(\"  - AdamW と Weight Decay\")\n",
    "print(\"\")\n",
    "print(\"Notebook 113: 学習率スケジューリング\")\n",
    "print(\"  - Step Decay, Exponential Decay\")\n",
    "print(\"  - Cosine Annealing\")\n",
    "print(\"  - Warmup の重要性\")\n",
    "print(\"  - One Cycle Policy\")\n",
    "print(\"\")\n",
    "print(\"Notebook 114: 正則化と最適化\")\n",
    "print(\"  - L1/L2 正則化\")\n",
    "print(\"  - Weight Decay vs L2\")\n",
    "print(\"  - Dropout\")\n",
    "print(\"  - Batch Normalization\")\n",
    "print(\"\")\n",
    "print(\"Notebook 115: 高度な最適化テクニック\")\n",
    "print(\"  - Gradient Clipping\")\n",
    "print(\"  - SAM\")\n",
    "print(\"  - LARS/LAMB\")\n",
    "print(\"  - Lookahead\")\n",
    "print(\"\")\n",
    "print(\"Notebook 116: オプティマイザの選び方と実践\")\n",
    "print(\"  - 選択フローチャート\")\n",
    "print(\"  - タスク別推奨設定\")\n",
    "print(\"  - デバッグと診断\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"クイックリファレンス: オプティマイザ選択表\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(f\"{'タスク':<25} {'オプティマイザ':<15} {'学習率':<12} {'備考'}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'画像分類 (CNN)':<25} {'SGD+Momentum':<15} {'0.1':<12} {'Step Decay'}\")\n",
    "print(f\"{'NLP (Transformer)':<25} {'AdamW':<15} {'1e-4~5e-4':<12} {'Warmup必須'}\")\n",
    "print(f\"{'ファインチューニング':<25} {'AdamW':<15} {'1e-5~3e-5':<12} {'元の1/10'}\")\n",
    "print(f\"{'GAN':<25} {'Adam':<15} {'2e-4':<12} {'β1=0.5'}\")\n",
    "print(f\"{'強化学習':<25} {'Adam':<15} {'3e-4':<12} {'Grad Clip'}\")\n",
    "print(f\"{'大バッチ (>1K) CNN':<25} {'LARS':<15} {'スケール':<12} {'Warmup必須'}\")\n",
    "print(f\"{'大バッチ Transformer':<25} {'LAMB':<15} {'スケール':<12} {'Warmup必須'}\")\n",
    "print(f\"{'汎化重視':<25} {'SAM+base':<15} {'base依存':<12} {'2x計算コスト'}\")\n",
    "print(\"-\"*80)\n",
    "print(\"\")\n",
    "print(\"【迷ったときのデフォルト】\")\n",
    "print(\"\")\n",
    "print(\"optimizer = AdamW(params, lr=1e-3, weight_decay=0.01)\")\n",
    "print(\"scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "### 基本文献\n",
    "1. Ruder, S. (2016). An overview of gradient descent optimization algorithms. *arXiv:1609.04747*.\n",
    "2. Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. *ICLR*.\n",
    "\n",
    "### 発展文献\n",
    "3. Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. *ICLR*.\n",
    "4. Smith, L. N. (2017). Cyclical learning rates for training neural networks. *WACV*.\n",
    "5. You, Y., et al. (2020). Large batch optimization for deep learning: Training BERT in 76 minutes. *ICLR*.\n",
    "6. Foret, P., et al. (2021). Sharpness-aware minimization for efficiently improving generalization. *ICLR*.\n",
    "\n",
    "### 実践ガイド\n",
    "7. PyTorch Recipes: https://pytorch.org/tutorials/recipes/recipes_index.html\n",
    "8. Fast.ai Course: https://www.fast.ai/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
