{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 112: 適応学習率手法 ― Adagrad, RMSprop, Adam\n",
    "\n",
    "## Adaptive Learning Rate Methods\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Phase 10「最適化手法」** の第3章として、パラメータごとに学習率を自動調整する **適応学習率手法** を学びます。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **Adagrad** の仕組みと問題点を理解する\n",
    "2. **RMSprop** による改良を理解する\n",
    "3. **Adam** の完全な理解と実装\n",
    "4. **AdamW** と Weight Decay の違いを理解する\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 110-111 の内容\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [適応学習率の必要性](#1-適応学習率の必要性)\n",
    "2. [Adagrad](#2-adagrad)\n",
    "3. [RMSprop](#3-rmsprop)\n",
    "4. [Adam](#4-adam)\n",
    "5. [AdamW と Weight Decay](#5-adamw-と-weight-decay)\n",
    "6. [手法の比較](#6-手法の比較)\n",
    "7. [演習問題](#7-演習問題)\n",
    "8. [まとめと次のステップ](#8-まとめと次のステップ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 適応学習率の必要性\n",
    "\n",
    "### 1.1 固定学習率の問題\n",
    "\n",
    "SGDやMomentumでは、すべてのパラメータに同じ学習率を適用します。しかし：\n",
    "\n",
    "- **頻繁に更新されるパラメータ**: 小さな学習率が望ましい\n",
    "- **まれに更新されるパラメータ**: 大きな学習率が望ましい\n",
    "\n",
    "例: 単語埋め込み（word embedding）では、頻出語と稀少語で勾配の規模が大きく異なる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータごとの勾配規模の違い\n",
    "\n",
    "# シミュレーション: 2つのパラメータ、異なる勾配規模\n",
    "n_steps = 1000\n",
    "np.random.seed(42)\n",
    "\n",
    "# パラメータ1: 頻繁に大きな勾配\n",
    "grads_param1 = np.random.randn(n_steps) * 10\n",
    "\n",
    "# パラメータ2: まれに勾配（スパース）\n",
    "grads_param2 = np.zeros(n_steps)\n",
    "sparse_indices = np.random.choice(n_steps, size=50, replace=False)\n",
    "grads_param2[sparse_indices] = np.random.randn(50) * 5\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(grads_param1, alpha=0.7)\n",
    "axes[0].set_xlabel('ステップ')\n",
    "axes[0].set_ylabel('勾配')\n",
    "axes[0].set_title('パラメータ1: 頻繁な勾配')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(grads_param2, alpha=0.7)\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('勾配')\n",
    "axes[1].set_title('パラメータ2: スパースな勾配')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【問題】\")\n",
    "print(f\"パラメータ1の勾配RMS: {np.sqrt(np.mean(grads_param1**2)):.2f}\")\n",
    "print(f\"パラメータ2の勾配RMS: {np.sqrt(np.mean(grads_param2**2)):.2f}\")\n",
    "print(\"\")\n",
    "print(\"同じ学習率では最適な更新ができない\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Adagrad\n",
    "\n",
    "### 2.1 アイデア\n",
    "\n",
    "**Adagrad** (Adaptive Gradient) は、過去の勾配の二乗和を追跡し、それで学習率を割ることでパラメータごとの学習率を調整します。\n",
    "\n",
    "### 2.2 数式\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_t &= G_{t-1} + g_t^2 \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot g_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $G_t$: 過去の勾配の二乗の累積和\n",
    "- $\\epsilon$: ゼロ除算を防ぐ小さな値（例: $10^{-8}$）\n",
    "- 勾配が大きいパラメータは学習率が自動的に小さくなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    \"\"\"\n",
    "    Adagrad optimizer\n",
    "    \n",
    "    G = G + g²\n",
    "    θ = θ - η * g / √(G + ε)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.01, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.G = None  # 勾配の二乗の累積\n",
    "    \n",
    "    def update(self, param, grad):\n",
    "        if self.G is None:\n",
    "            self.G = np.zeros_like(param)\n",
    "        \n",
    "        self.G += grad ** 2\n",
    "        param -= self.lr * grad / (np.sqrt(self.G) + self.eps)\n",
    "        \n",
    "        return param\n",
    "\n",
    "\n",
    "def adagrad_optimize(grad_fn, start, lr=0.5, n_steps=100, **kwargs):\n",
    "    \"\"\"Adagrad最適化\"\"\"\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    G = np.zeros_like(pos)\n",
    "    eps = 1e-8\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_fn(pos[0], pos[1], **kwargs)\n",
    "        G += grad ** 2\n",
    "        pos = pos - lr * grad / (np.sqrt(G) + eps)\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "print(\"Adagrad を定義しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagradの効果を可視化\n",
    "\n",
    "def pathological_loss(x, y, condition_number=50):\n",
    "    return x**2 + condition_number * y**2\n",
    "\n",
    "def pathological_grad(x, y, condition_number=50):\n",
    "    return np.array([2*x, 2*condition_number*y])\n",
    "\n",
    "\n",
    "# SGD vs Adagrad\n",
    "start = (3, 1)\n",
    "n_steps = 100\n",
    "\n",
    "# SGD\n",
    "path_sgd = [np.array(start)]\n",
    "pos = np.array(start, dtype=float)\n",
    "for _ in range(n_steps):\n",
    "    grad = pathological_grad(pos[0], pos[1])\n",
    "    pos = pos - 0.01 * grad\n",
    "    path_sgd.append(pos.copy())\n",
    "path_sgd = np.array(path_sgd)\n",
    "\n",
    "# Adagrad\n",
    "path_adagrad = adagrad_optimize(pathological_grad, start, lr=0.5, n_steps=n_steps)\n",
    "\n",
    "# 可視化\n",
    "x = np.linspace(-4, 4, 200)\n",
    "y = np.linspace(-2, 2, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = pathological_loss(X, Y)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 軌跡\n",
    "axes[0].contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.7)\n",
    "axes[0].plot(path_sgd[:, 0], path_sgd[:, 1], 'r.-', markersize=2, linewidth=1, alpha=0.7, label='SGD')\n",
    "axes[0].plot(path_adagrad[:, 0], path_adagrad[:, 1], 'b.-', markersize=2, linewidth=1, alpha=0.7, label='Adagrad')\n",
    "axes[0].scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('SGD vs Adagrad')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(-1, 4)\n",
    "axes[0].set_ylim(-1.5, 1.5)\n",
    "\n",
    "# 損失の推移\n",
    "losses_sgd = [pathological_loss(p[0], p[1]) for p in path_sgd]\n",
    "losses_adagrad = [pathological_loss(p[0], p[1]) for p in path_adagrad]\n",
    "\n",
    "axes[1].semilogy(losses_sgd, 'r-', linewidth=2, label='SGD')\n",
    "axes[1].semilogy(losses_adagrad, 'b-', linewidth=2, label='Adagrad')\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('損失（対数スケール）')\n",
    "axes[1].set_title('収束速度の比較')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【Adagradの効果】\")\n",
    "print(\"- y方向: 勾配が大きい → 学習率が自動的に減少\")\n",
    "print(\"- x方向: 勾配が小さい → 相対的に大きな学習率を維持\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Adagradの問題点\n",
    "\n",
    "**学習率が単調減少**: $G_t$ は累積なので増加し続け、学習率 $\\eta / \\sqrt{G_t}$ は減少し続ける\n",
    "\n",
    "→ 学習の後半で学習率が小さくなりすぎ、収束が止まる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagradの学習率減衰問題\n",
    "\n",
    "# シミュレーション: 一定の勾配を受け続けた場合\n",
    "n_steps = 500\n",
    "grad = 1.0  # 一定の勾配\n",
    "G = 0\n",
    "effective_lrs = []\n",
    "lr = 0.5\n",
    "eps = 1e-8\n",
    "\n",
    "for t in range(1, n_steps + 1):\n",
    "    G += grad ** 2\n",
    "    effective_lr = lr / (np.sqrt(G) + eps)\n",
    "    effective_lrs.append(effective_lr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(effective_lrs, 'b-', linewidth=2)\n",
    "ax.set_xlabel('ステップ')\n",
    "ax.set_ylabel('実効学習率')\n",
    "ax.set_title('Adagradの学習率減衰')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 理論曲線\n",
    "t = np.arange(1, n_steps + 1)\n",
    "theoretical = lr / np.sqrt(t)\n",
    "ax.plot(t, theoretical, 'r--', linewidth=2, label=r'$\\eta / \\sqrt{t}$')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"【問題】\")\n",
    "print(f\"ステップ 1: 学習率 = {effective_lrs[0]:.4f}\")\n",
    "print(f\"ステップ 100: 学習率 = {effective_lrs[99]:.4f}\")\n",
    "print(f\"ステップ 500: 学習率 = {effective_lrs[-1]:.4f}\")\n",
    "print(\"\")\n",
    "print(\"学習が進むにつれて更新量が小さくなりすぎる\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. RMSprop\n",
    "\n",
    "### 3.1 アイデア\n",
    "\n",
    "**RMSprop** (Root Mean Square Propagation) は、Adagradの問題を解決するために **指数移動平均** を使用します。\n",
    "\n",
    "### 3.2 数式\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_t &= \\rho \\cdot v_{t-1} + (1 - \\rho) \\cdot g_t^2 \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} \\cdot g_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $\\rho$: 減衰率（通常 0.9 または 0.99）\n",
    "- 指数移動平均により、古い勾配の影響が減衰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "    \"\"\"\n",
    "    RMSprop optimizer\n",
    "    \n",
    "    v = ρ * v + (1 - ρ) * g²\n",
    "    θ = θ - η * g / √(v + ε)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.01, rho=0.9, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, param, grad):\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(param)\n",
    "        \n",
    "        self.v = self.rho * self.v + (1 - self.rho) * grad ** 2\n",
    "        param -= self.lr * grad / (np.sqrt(self.v) + self.eps)\n",
    "        \n",
    "        return param\n",
    "\n",
    "\n",
    "def rmsprop_optimize(grad_fn, start, lr=0.1, rho=0.9, n_steps=100, **kwargs):\n",
    "    \"\"\"RMSprop最適化\"\"\"\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    v = np.zeros_like(pos)\n",
    "    eps = 1e-8\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_fn(pos[0], pos[1], **kwargs)\n",
    "        v = rho * v + (1 - rho) * grad ** 2\n",
    "        pos = pos - lr * grad / (np.sqrt(v) + eps)\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "print(\"RMSprop を定義しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad vs RMSprop: 学習率の推移\n",
    "\n",
    "n_steps = 500\n",
    "grad = 1.0\n",
    "lr = 0.5\n",
    "eps = 1e-8\n",
    "rho = 0.9\n",
    "\n",
    "# Adagrad\n",
    "G = 0\n",
    "lrs_adagrad = []\n",
    "for t in range(n_steps):\n",
    "    G += grad ** 2\n",
    "    lrs_adagrad.append(lr / (np.sqrt(G) + eps))\n",
    "\n",
    "# RMSprop\n",
    "v = 0\n",
    "lrs_rmsprop = []\n",
    "for t in range(n_steps):\n",
    "    v = rho * v + (1 - rho) * grad ** 2\n",
    "    lrs_rmsprop.append(lr / (np.sqrt(v) + eps))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(lrs_adagrad, 'b-', linewidth=2, label='Adagrad')\n",
    "ax.plot(lrs_rmsprop, 'r-', linewidth=2, label='RMSprop')\n",
    "ax.set_xlabel('ステップ')\n",
    "ax.set_ylabel('実効学習率')\n",
    "ax.set_title('Adagrad vs RMSprop: 学習率の推移')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【RMSpropの利点】\")\n",
    "print(\"- 学習率が安定する（単調減少しない）\")\n",
    "print(f\"- RMSpropの定常状態学習率: {lrs_rmsprop[-1]:.4f}\")\n",
    "print(f\"- Adagradの最終学習率: {lrs_adagrad[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Adam\n",
    "\n",
    "### 4.1 アイデア\n",
    "\n",
    "**Adam** (Adaptive Moment Estimation) は、Momentum と RMSprop を組み合わせた手法です。\n",
    "\n",
    "- **1次モーメント** (m): 勾配の指数移動平均 → Momentumの効果\n",
    "- **2次モーメント** (v): 勾配の二乗の指数移動平均 → RMSpropの効果\n",
    "\n",
    "### 4.2 数式\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\quad \\text{(バイアス補正)} \\\\\n",
    "\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\quad \\text{(バイアス補正)} \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "デフォルト: $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam optimizer\n",
    "    \n",
    "    m = β1 * m + (1 - β1) * g\n",
    "    v = β2 * v + (1 - β2) * g²\n",
    "    m_hat = m / (1 - β1^t)\n",
    "    v_hat = v / (1 - β2^t)\n",
    "    θ = θ - η * m_hat / (√v_hat + ε)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def update(self, param, grad):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(param)\n",
    "            self.v = np.zeros_like(param)\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        # 1次モーメント（勾配の平均）\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "        \n",
    "        # 2次モーメント（勾配の二乗の平均）\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * grad ** 2\n",
    "        \n",
    "        # バイアス補正\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        \n",
    "        # パラメータ更新\n",
    "        param -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "        \n",
    "        return param\n",
    "\n",
    "\n",
    "def adam_optimize(grad_fn, start, lr=0.1, beta1=0.9, beta2=0.999, n_steps=100, **kwargs):\n",
    "    \"\"\"Adam最適化\"\"\"\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    m = np.zeros_like(pos)\n",
    "    v = np.zeros_like(pos)\n",
    "    eps = 1e-8\n",
    "    \n",
    "    for t in range(1, n_steps + 1):\n",
    "        grad = grad_fn(pos[0], pos[1], **kwargs)\n",
    "        \n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * grad ** 2\n",
    "        \n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        pos = pos - lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "print(\"Adam を定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 バイアス補正の必要性\n",
    "\n",
    "初期状態では $m_0 = v_0 = 0$ なので、最初の数ステップでは $m_t, v_t$ が過小評価されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バイアス補正の効果\n",
    "\n",
    "n_steps = 50\n",
    "true_mean = 1.0  # 真の勾配平均\n",
    "beta1 = 0.9\n",
    "\n",
    "m = 0\n",
    "m_raw = []  # 補正なし\n",
    "m_corrected = []  # 補正あり\n",
    "\n",
    "for t in range(1, n_steps + 1):\n",
    "    m = beta1 * m + (1 - beta1) * true_mean\n",
    "    m_raw.append(m)\n",
    "    m_corrected.append(m / (1 - beta1 ** t))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(m_raw, 'b-', linewidth=2, label='補正なし m')\n",
    "ax.plot(m_corrected, 'r-', linewidth=2, label='補正あり m / (1-β^t)')\n",
    "ax.axhline(y=true_mean, color='green', linestyle='--', linewidth=2, label=f'真の平均 = {true_mean}')\n",
    "\n",
    "ax.set_xlabel('ステップ')\n",
    "ax.set_ylabel('m の推定値')\n",
    "ax.set_title('Adamのバイアス補正')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"【バイアス補正の効果】\")\n",
    "print(f\"ステップ 1:\")\n",
    "print(f\"  補正なし: {m_raw[0]:.4f}\")\n",
    "print(f\"  補正あり: {m_corrected[0]:.4f}\")\n",
    "print(f\"  真の値:   {true_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全手法の比較\n",
    "\n",
    "start = (3, 1)\n",
    "n_steps = 100\n",
    "\n",
    "# 各手法での最適化\n",
    "path_sgd = []\n",
    "pos = np.array(start, dtype=float)\n",
    "path_sgd.append(pos.copy())\n",
    "for _ in range(n_steps):\n",
    "    grad = pathological_grad(pos[0], pos[1])\n",
    "    pos = pos - 0.01 * grad\n",
    "    path_sgd.append(pos.copy())\n",
    "path_sgd = np.array(path_sgd)\n",
    "\n",
    "path_adagrad = adagrad_optimize(pathological_grad, start, lr=0.5, n_steps=n_steps)\n",
    "path_rmsprop = rmsprop_optimize(pathological_grad, start, lr=0.1, rho=0.9, n_steps=n_steps)\n",
    "path_adam = adam_optimize(pathological_grad, start, lr=0.1, n_steps=n_steps)\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 軌跡\n",
    "axes[0].contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.7)\n",
    "axes[0].plot(path_sgd[:, 0], path_sgd[:, 1], 'r.-', markersize=2, linewidth=1, alpha=0.7, label='SGD')\n",
    "axes[0].plot(path_adagrad[:, 0], path_adagrad[:, 1], 'g.-', markersize=2, linewidth=1, alpha=0.7, label='Adagrad')\n",
    "axes[0].plot(path_rmsprop[:, 0], path_rmsprop[:, 1], 'b.-', markersize=2, linewidth=1, alpha=0.7, label='RMSprop')\n",
    "axes[0].plot(path_adam[:, 0], path_adam[:, 1], 'm.-', markersize=2, linewidth=1, alpha=0.7, label='Adam')\n",
    "axes[0].scatter([0], [0], color='black', s=100, marker='*', zorder=5)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('最適化手法の比較')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(-1, 4)\n",
    "axes[0].set_ylim(-1.5, 1.5)\n",
    "\n",
    "# 損失の推移\n",
    "losses = {\n",
    "    'SGD': [pathological_loss(p[0], p[1]) for p in path_sgd],\n",
    "    'Adagrad': [pathological_loss(p[0], p[1]) for p in path_adagrad],\n",
    "    'RMSprop': [pathological_loss(p[0], p[1]) for p in path_rmsprop],\n",
    "    'Adam': [pathological_loss(p[0], p[1]) for p in path_adam],\n",
    "}\n",
    "\n",
    "colors = {'SGD': 'r', 'Adagrad': 'g', 'RMSprop': 'b', 'Adam': 'm'}\n",
    "for name, loss in losses.items():\n",
    "    axes[1].semilogy(loss, color=colors[name], linewidth=2, label=name)\n",
    "\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('損失（対数スケール）')\n",
    "axes[1].set_title('収束速度の比較')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【最終損失】\")\n",
    "for name, loss in losses.items():\n",
    "    print(f\"{name:10s}: {loss[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. AdamW と Weight Decay\n",
    "\n",
    "### 5.1 L2正則化 vs Weight Decay\n",
    "\n",
    "**L2正則化**: 損失関数に $\\frac{\\lambda}{2} \\|\\theta\\|^2$ を加える\n",
    "\n",
    "**Weight Decay**: パラメータ更新時に $\\theta \\leftarrow (1 - \\lambda) \\theta$ を適用\n",
    "\n",
    "SGDでは同等ですが、Adamでは異なる効果を持ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2正則化 vs Weight Decay の違い\n",
    "\n",
    "print(\"【SGDの場合】\")\n",
    "print(\"\")\n",
    "print(\"L2正則化:\")\n",
    "print(\"  L' = L + (λ/2)||θ||²\")\n",
    "print(\"  ∇L' = ∇L + λθ\")\n",
    "print(\"  θ = θ - η(∇L + λθ) = θ - η∇L - ηλθ\")\n",
    "print(\"\")\n",
    "print(\"Weight Decay:\")\n",
    "print(\"  θ = θ - η∇L - ηλθ\")\n",
    "print(\"\")\n",
    "print(\"→ 両者は同等\")\n",
    "print(\"\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\")\n",
    "print(\"【Adamの場合】\")\n",
    "print(\"\")\n",
    "print(\"L2正則化:\")\n",
    "print(\"  θ = θ - η * m_hat / √v_hat  (mとvは正則化項を含む勾配から計算)\")\n",
    "print(\"  → 正則化項も適応的にスケーリングされる\")\n",
    "print(\"\")\n",
    "print(\"Weight Decay (AdamW):\")\n",
    "print(\"  θ = θ - η * m_hat / √v_hat - ηλθ\")\n",
    "print(\"  → 正則化項は適応的スケーリングの影響を受けない\")\n",
    "print(\"\")\n",
    "print(\"→ AdamWの方が正則化効果が安定\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW:\n",
    "    \"\"\"\n",
    "    AdamW optimizer (Adam with decoupled Weight Decay)\n",
    "    \n",
    "    θ = θ - η * m_hat / (√v_hat + ε) - η * λ * θ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def update(self, param, grad):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(param)\n",
    "            self.v = np.zeros_like(param)\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        # 1次・2次モーメント\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * grad ** 2\n",
    "        \n",
    "        # バイアス補正\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        \n",
    "        # AdamW: Weight DecayをAdam更新とは別に適用\n",
    "        param -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "        param -= self.lr * self.weight_decay * param  # 分離されたWeight Decay\n",
    "        \n",
    "        return param\n",
    "\n",
    "\n",
    "print(\"AdamW を定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 手法の比較\n",
    "\n",
    "### 6.1 Rosenbrock関数での比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x, y):\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_grad(x, y):\n",
    "    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dy = 200 * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "\n",
    "# 各手法での最適化\n",
    "start = (-1.5, 1.5)\n",
    "n_steps = 2000\n",
    "\n",
    "paths = {\n",
    "    'SGD': [],\n",
    "    'Adagrad': adagrad_optimize(rosenbrock_grad, start, lr=0.5, n_steps=n_steps),\n",
    "    'RMSprop': rmsprop_optimize(rosenbrock_grad, start, lr=0.01, rho=0.9, n_steps=n_steps),\n",
    "    'Adam': adam_optimize(rosenbrock_grad, start, lr=0.01, n_steps=n_steps),\n",
    "}\n",
    "\n",
    "# SGD\n",
    "pos = np.array(start, dtype=float)\n",
    "paths['SGD'].append(pos.copy())\n",
    "for _ in range(n_steps):\n",
    "    grad = rosenbrock_grad(pos[0], pos[1])\n",
    "    pos = pos - 0.001 * grad\n",
    "    paths['SGD'].append(pos.copy())\n",
    "paths['SGD'] = np.array(paths['SGD'])\n",
    "\n",
    "# 可視化\n",
    "x = np.linspace(-2, 2, 200)\n",
    "y = np.linspace(-1, 3, 200)\n",
    "X_r, Y_r = np.meshgrid(x, y)\n",
    "Z_r = rosenbrock(X_r, Y_r)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 軌跡\n",
    "colors = {'SGD': 'r', 'Adagrad': 'g', 'RMSprop': 'b', 'Adam': 'm'}\n",
    "axes[0].contour(X_r, Y_r, np.log(Z_r + 1), levels=30, cmap='viridis', alpha=0.7)\n",
    "\n",
    "for name, path in paths.items():\n",
    "    axes[0].plot(path[:, 0], path[:, 1], color=colors[name], linewidth=1, alpha=0.7, label=name)\n",
    "\n",
    "axes[0].scatter([1], [1], color='black', s=100, marker='*', zorder=5, label='最適点')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Rosenbrock関数での最適化')\n",
    "axes[0].legend()\n",
    "\n",
    "# 損失の推移\n",
    "for name, path in paths.items():\n",
    "    losses = [rosenbrock(p[0], p[1]) for p in path]\n",
    "    axes[1].semilogy(losses, color=colors[name], linewidth=2, label=name)\n",
    "\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('損失（対数スケール）')\n",
    "axes[1].set_title('収束速度の比較')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【最終位置】\")\n",
    "for name, path in paths.items():\n",
    "    dist = np.linalg.norm(path[-1] - np.array([1, 1]))\n",
    "    print(f\"{name:10s}: ({path[-1, 0]:.4f}, {path[-1, 1]:.4f}), 最適点からの距離: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 オプティマイザの特性まとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# オプティマイザの特性比較表\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"オプティマイザの特性比較\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(f\"{'手法':<12} {'適応学習率':<10} {'モーメンタム':<12} {'メモリ':<10} {'推奨用途'}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'SGD':<12} {'×':<10} {'×':<12} {'θのみ':<10} {'凸問題、ファインチューニング'}\")\n",
    "print(f\"{'Momentum':<12} {'×':<10} {'○':<12} {'θ, v':<10} {'一般的な学習'}\")\n",
    "print(f\"{'Adagrad':<12} {'○':<10} {'×':<12} {'θ, G':<10} {'スパースデータ'}\")\n",
    "print(f\"{'RMSprop':<12} {'○':<10} {'×':<12} {'θ, v':<10} {'RNN'}\")\n",
    "print(f\"{'Adam':<12} {'○':<10} {'○':<12} {'θ, m, v':<10} {'一般的なデフォルト'}\")\n",
    "print(f\"{'AdamW':<12} {'○':<10} {'○':<12} {'θ, m, v':<10} {'Transformer, ViT'}\")\n",
    "print(\"-\"*80)\n",
    "print(\"\")\n",
    "print(\"【デフォルトハイパーパラメータ】\")\n",
    "print(\"  Adam/AdamW: lr=0.001, β1=0.9, β2=0.999, ε=1e-8\")\n",
    "print(\"  RMSprop:    lr=0.01, ρ=0.9\")\n",
    "print(\"  Momentum:   lr=0.01, μ=0.9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 演習問題\n",
    "\n",
    "### 演習 7.1: Adadelta の実装\n",
    "\n",
    "Adadeltaは学習率パラメータを必要としないオプティマイザです。実装してください。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[g^2]_t &= \\rho E[g^2]_{t-1} + (1-\\rho) g_t^2 \\\\\n",
    "\\Delta\\theta_t &= -\\frac{\\sqrt{E[\\Delta\\theta^2]_{t-1} + \\epsilon}}{\\sqrt{E[g^2]_t + \\epsilon}} g_t \\\\\n",
    "E[\\Delta\\theta^2]_t &= \\rho E[\\Delta\\theta^2]_{t-1} + (1-\\rho) \\Delta\\theta_t^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.1: 解答欄\n",
    "\n",
    "class Adadelta:\n",
    "    def __init__(self, rho=0.9, eps=1e-8):\n",
    "        # TODO: 実装\n",
    "        pass\n",
    "    \n",
    "    def update(self, param, grad):\n",
    "        # TODO: 実装\n",
    "        pass\n",
    "\n",
    "# TODO: テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 7.2: β1, β2 の影響\n",
    "\n",
    "Adamのハイパーパラメータ β1, β2 を変えて、収束速度への影響を観察してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.2: 解答欄\n",
    "\n",
    "# TODO: β1 = [0.5, 0.9, 0.99], β2 = [0.9, 0.999, 0.9999] を比較\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 7.3: NAdam の実装\n",
    "\n",
    "NAdam は Adam に Nesterov モーメンタムを組み合わせた手法です。実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.3: 解答欄\n",
    "\n",
    "class NAdam:\n",
    "    # TODO: 実装\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **Adagrad**: \n",
    "   - 過去の勾配二乗和で学習率を調整\n",
    "   - 問題: 学習率が単調減少\n",
    "\n",
    "2. **RMSprop**:\n",
    "   - 指数移動平均を使用して問題を解決\n",
    "   - 安定した適応学習率\n",
    "\n",
    "3. **Adam**:\n",
    "   - Momentum + RMSprop\n",
    "   - バイアス補正が重要\n",
    "   - 最も広く使われるオプティマイザ\n",
    "\n",
    "4. **AdamW**:\n",
    "   - Weight Decayを分離\n",
    "   - Transformerなどの大規模モデルで推奨\n",
    "\n",
    "### 次のノートブック（113: 学習率スケジューリング）への橋渡し\n",
    "\n",
    "適応学習率は便利ですが、以下の課題が残ります：\n",
    "\n",
    "- **初期学習率の設定**: まだ手動で選ぶ必要がある\n",
    "- **学習の進行に応じた調整**: Warmup, Decay などのスケジュール\n",
    "\n",
    "次のノートブックでは、**学習率スケジューリング** を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Duchi, J., et al. (2011). Adaptive subgradient methods for online learning and stochastic optimization. *JMLR*.\n",
    "2. Tieleman, T., & Hinton, G. (2012). Lecture 6.5—RMSprop. *COURSERA: Neural Networks for Machine Learning*.\n",
    "3. Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. *ICLR*.\n",
    "4. Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. *ICLR*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
