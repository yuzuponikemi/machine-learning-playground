{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 110: 最適化の基礎 ― 勾配降下法を理解する\n",
    "\n",
    "## Optimization Fundamentals: Understanding Gradient Descent\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Phase 10「最適化手法」** の第1章として、機械学習における最適化の基礎と勾配降下法の本質を理解します。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **最適化問題** の定式化を理解する\n",
    "2. **勾配降下法** のアルゴリズムと直感を習得する\n",
    "3. **学習率** の役割と影響を理解する\n",
    "4. **バッチサイズ** と確率的勾配降下法の関係を理解する\n",
    "5. **損失曲面** の可視化と最適化の挙動を観察する\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- 微分の基礎（Notebook 70）\n",
    "- 勾配ベクトルの概念\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [最適化問題とは](#1-最適化問題とは)\n",
    "2. [勾配降下法の原理](#2-勾配降下法の原理)\n",
    "3. [学習率の影響](#3-学習率の影響)\n",
    "4. [バッチ勾配降下法 vs 確率的勾配降下法](#4-バッチ勾配降下法-vs-確率的勾配降下法)\n",
    "5. [損失曲面の可視化](#5-損失曲面の可視化)\n",
    "6. [凸関数と非凸関数](#6-凸関数と非凸関数)\n",
    "7. [収束条件と停止基準](#7-収束条件と停止基準)\n",
    "8. [演習問題](#8-演習問題)\n",
    "9. [まとめと次のステップ](#9-まとめと次のステップ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 最適化問題とは\n",
    "\n",
    "### 1.1 機械学習における最適化\n",
    "\n",
    "機械学習の本質は **最適化問題** を解くことです。\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min_{\\theta} L(\\theta)\n",
    "$$\n",
    "\n",
    "- $\\theta$: モデルのパラメータ（重み、バイアス）\n",
    "- $L(\\theta)$: 損失関数（目的関数）\n",
    "- $\\theta^*$: 最適なパラメータ\n",
    "\n",
    "### 1.2 最適化の課題\n",
    "\n",
    "1. **高次元空間**: ニューラルネットワークは数百万〜数十億のパラメータを持つ\n",
    "2. **非凸性**: 損失曲面には多くの局所解が存在\n",
    "3. **計算コスト**: 大規模データセットでの効率的な計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化問題の例: 2次関数の最小化\n",
    "\n",
    "def quadratic(x):\n",
    "    \"\"\"f(x) = x² - 4x + 4 = (x - 2)²\"\"\"\n",
    "    return x**2 - 4*x + 4\n",
    "\n",
    "def quadratic_grad(x):\n",
    "    \"\"\"f'(x) = 2x - 4\"\"\"\n",
    "    return 2*x - 4\n",
    "\n",
    "# 可視化\n",
    "x = np.linspace(-1, 5, 100)\n",
    "y = quadratic(x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 関数のプロット\n",
    "axes[0].plot(x, y, 'b-', linewidth=2, label=r'$f(x) = (x-2)^2$')\n",
    "axes[0].axvline(x=2, color='red', linestyle='--', label=r'最小点 $x^* = 2$')\n",
    "axes[0].scatter([2], [0], color='red', s=100, zorder=5)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_title('最適化問題: 関数の最小値を見つける')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 勾配のプロット\n",
    "grad = quadratic_grad(x)\n",
    "axes[1].plot(x, grad, 'g-', linewidth=2, label=r\"$f'(x) = 2x - 4$\")\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1].axvline(x=2, color='red', linestyle='--', label=r\"$f'(x^*) = 0$\")\n",
    "axes[1].fill_between(x[x < 2], 0, grad[x < 2], alpha=0.3, color='blue', label='勾配 < 0: 右へ移動')\n",
    "axes[1].fill_between(x[x > 2], 0, grad[x > 2], alpha=0.3, color='orange', label='勾配 > 0: 左へ移動')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel(\"f'(x)\")\n",
    "axes[1].set_title('勾配: 最小点では勾配がゼロ')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【ポイント】\")\n",
    "print(\"- 勾配が負 → パラメータを増やす方向が良い\")\n",
    "print(\"- 勾配が正 → パラメータを減らす方向が良い\")\n",
    "print(\"- 勾配がゼロ → 極値（最小点または最大点）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 勾配降下法の原理\n",
    "\n",
    "### 2.1 アルゴリズム\n",
    "\n",
    "勾配降下法（Gradient Descent）は、勾配の **逆方向** にパラメータを更新することで関数を最小化します。\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)\n",
    "$$\n",
    "\n",
    "- $\\eta$ (イータ): 学習率（Learning Rate）\n",
    "- $\\nabla L$: 損失関数の勾配\n",
    "\n",
    "### 2.2 直感的な理解\n",
    "\n",
    "山を下りる際、最も急な斜面を下っていくイメージです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_1d(f, grad_f, x0, lr=0.1, n_steps=20):\n",
    "    \"\"\"\n",
    "    1次元の勾配降下法\n",
    "    \n",
    "    Args:\n",
    "        f: 目的関数\n",
    "        grad_f: 勾配関数\n",
    "        x0: 初期値\n",
    "        lr: 学習率\n",
    "        n_steps: ステップ数\n",
    "    \n",
    "    Returns:\n",
    "        履歴（位置、関数値、勾配）\n",
    "    \"\"\"\n",
    "    history = {'x': [x0], 'f': [f(x0)], 'grad': [grad_f(x0)]}\n",
    "    x = x0\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_f(x)\n",
    "        x = x - lr * grad  # 更新則\n",
    "        history['x'].append(x)\n",
    "        history['f'].append(f(x))\n",
    "        history['grad'].append(grad_f(x))\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# 勾配降下法の実行\n",
    "history = gradient_descent_1d(quadratic, quadratic_grad, x0=0.0, lr=0.3, n_steps=15)\n",
    "\n",
    "# 可視化\n",
    "x = np.linspace(-1, 5, 100)\n",
    "y = quadratic(x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 最適化の軌跡\n",
    "axes[0].plot(x, y, 'b-', linewidth=2, alpha=0.5)\n",
    "axes[0].plot(history['x'], history['f'], 'ro-', markersize=8, linewidth=2, label='勾配降下の軌跡')\n",
    "axes[0].scatter([history['x'][0]], [history['f'][0]], color='green', s=150, zorder=5, label=f'開始点 x={history[\"x\"][0]:.1f}')\n",
    "axes[0].scatter([history['x'][-1]], [history['f'][-1]], color='red', s=150, marker='*', zorder=5, label=f'終了点 x={history[\"x\"][-1]:.3f}')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_title('勾配降下法の軌跡')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 収束の様子\n",
    "steps = range(len(history['x']))\n",
    "axes[1].plot(steps, history['x'], 'bo-', label='x')\n",
    "axes[1].axhline(y=2, color='red', linestyle='--', label='最適値 x*=2')\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('x')\n",
    "axes[1].set_title('パラメータの収束')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【更新の履歴（最初の5ステップ）】\")\n",
    "print(f\"{'Step':>5} {'x':>10} {'f(x)':>10} {'grad':>10}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(min(6, len(history['x']))):\n",
    "    print(f\"{i:>5} {history['x'][i]:>10.4f} {history['f'][i]:>10.4f} {history['grad'][i]:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 多次元への拡張\n",
    "\n",
    "2次元以上の場合、勾配はベクトルになります。\n",
    "\n",
    "$$\n",
    "\\nabla L(\\theta) = \\begin{pmatrix} \\frac{\\partial L}{\\partial \\theta_1} \\\\ \\frac{\\partial L}{\\partial \\theta_2} \\\\ \\vdots \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x, y):\n",
    "    \"\"\"Rosenbrock関数: 最適化のベンチマーク関数\"\"\"\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_grad(x, y):\n",
    "    \"\"\"Rosenbrock関数の勾配\"\"\"\n",
    "    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dy = 200 * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "\n",
    "def gradient_descent_2d(f, grad_f, start, lr=0.001, n_steps=1000):\n",
    "    \"\"\"2次元の勾配降下法\"\"\"\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_f(pos[0], pos[1])\n",
    "        pos = pos - lr * grad\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "# Rosenbrock関数での最適化\n",
    "path = gradient_descent_2d(rosenbrock, rosenbrock_grad, start=(-1.5, 1.5), lr=0.001, n_steps=5000)\n",
    "\n",
    "# 可視化\n",
    "x = np.linspace(-2, 2, 200)\n",
    "y = np.linspace(-1, 3, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosenbrock(X, Y)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 等高線図\n",
    "contour = axes[0].contour(X, Y, np.log(Z + 1), levels=30, cmap='viridis')\n",
    "axes[0].plot(path[:, 0], path[:, 1], 'r.-', markersize=2, linewidth=1, alpha=0.7)\n",
    "axes[0].scatter([path[0, 0]], [path[0, 1]], color='green', s=100, zorder=5, label='開始点')\n",
    "axes[0].scatter([1], [1], color='red', s=100, marker='*', zorder=5, label='最適点 (1, 1)')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Rosenbrock関数での勾配降下')\n",
    "axes[0].legend()\n",
    "\n",
    "# 損失の推移\n",
    "losses = [rosenbrock(p[0], p[1]) for p in path]\n",
    "axes[1].semilogy(losses, linewidth=2)\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('損失（対数スケール）')\n",
    "axes[1].set_title('損失の推移')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"開始点: {path[0]}\")\n",
    "print(f\"終了点: {path[-1]}\")\n",
    "print(f\"最適点: (1, 1)\")\n",
    "print(f\"最終損失: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 学習率の影響\n",
    "\n",
    "### 3.1 学習率とは\n",
    "\n",
    "学習率 $\\eta$ は、各ステップでどれだけ大きく移動するかを制御するハイパーパラメータです。\n",
    "\n",
    "- **大きすぎる**: 発振、発散\n",
    "- **小さすぎる**: 収束が遅い\n",
    "- **適切**: 効率的に最適解に到達"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率の比較\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.1]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "x_range = np.linspace(-1, 5, 100)\n",
    "y_range = quadratic(x_range)\n",
    "\n",
    "for ax, lr, color in zip(axes, learning_rates, colors):\n",
    "    history = gradient_descent_1d(quadratic, quadratic_grad, x0=0.0, lr=lr, n_steps=20)\n",
    "    \n",
    "    ax.plot(x_range, y_range, 'b-', linewidth=2, alpha=0.3)\n",
    "    ax.plot(history['x'], history['f'], 'o-', color=color, markersize=8, linewidth=2)\n",
    "    ax.axvline(x=2, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.set_title(f'学習率 η = {lr}')\n",
    "    ax.set_xlim(-2, 6)\n",
    "    ax.set_ylim(-1, 20)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 最終位置を表示\n",
    "    final_x = history['x'][-1]\n",
    "    ax.annotate(f'x = {final_x:.2f}', (final_x, history['f'][-1]), \n",
    "                textcoords=\"offset points\", xytext=(10, 10), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【学習率の影響】\")\n",
    "print(f\"η = 0.01: 収束が遅い（慎重すぎる）\")\n",
    "print(f\"η = 0.1:  適切な収束\")\n",
    "print(f\"η = 0.5:  速い収束\")\n",
    "print(f\"η = 1.1:  発振（大きすぎる）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 収束速度の詳細比較\n",
    "learning_rates = [0.01, 0.1, 0.3, 0.5, 0.9]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for lr in learning_rates:\n",
    "    history = gradient_descent_1d(quadratic, quadratic_grad, x0=0.0, lr=lr, n_steps=30)\n",
    "    error = np.abs(np.array(history['x']) - 2)  # 最適値との差\n",
    "    ax.semilogy(error, linewidth=2, label=f'η = {lr}')\n",
    "\n",
    "ax.set_xlabel('ステップ')\n",
    "ax.set_ylabel('|x - x*|（最適値との差）')\n",
    "ax.set_title('学習率と収束速度の関係')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(\"- 学習率が大きいほど初期の収束は速い\")\n",
    "print(\"- ただし大きすぎると発振や発散のリスク\")\n",
    "print(\"- η = 1.0 を超えると不安定になる（この関数では）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 学習率と固有値の関係\n",
    "\n",
    "二次関数の場合、収束条件は損失関数のヘッセ行列の固有値に依存します。\n",
    "\n",
    "$$\n",
    "0 < \\eta < \\frac{2}{\\lambda_{\\max}}\n",
    "$$\n",
    "\n",
    "- $\\lambda_{\\max}$: ヘッセ行列の最大固有値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 楕円形の二次関数での学習率の影響\n",
    "\n",
    "def ellipse_loss(x, y, a=1, b=10):\n",
    "    \"\"\"楕円形の損失関数: L = a*x² + b*y²\"\"\"\n",
    "    return a * x**2 + b * y**2\n",
    "\n",
    "def ellipse_grad(x, y, a=1, b=10):\n",
    "    \"\"\"楕円形損失関数の勾配\"\"\"\n",
    "    return np.array([2*a*x, 2*b*y])\n",
    "\n",
    "\n",
    "# 異なる学習率での軌跡\n",
    "starts = [(-3, 1)]\n",
    "lrs = [0.01, 0.05, 0.09, 0.11]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 等高線の準備\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = ellipse_loss(X, Y)\n",
    "\n",
    "for ax, lr in zip(axes, lrs):\n",
    "    path = []\n",
    "    pos = np.array(starts[0], dtype=float)\n",
    "    path.append(pos.copy())\n",
    "    \n",
    "    for _ in range(50):\n",
    "        grad = ellipse_grad(pos[0], pos[1])\n",
    "        pos = pos - lr * grad\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    path = np.array(path)\n",
    "    \n",
    "    ax.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "    ax.plot(path[:, 0], path[:, 1], 'ro-', markersize=4, linewidth=1)\n",
    "    ax.scatter([0], [0], color='red', s=100, marker='*', zorder=5)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    \n",
    "    # 収束/発散の判定\n",
    "    final_loss = ellipse_loss(path[-1, 0], path[-1, 1])\n",
    "    status = \"収束\" if final_loss < 0.01 else (\"発散\" if final_loss > 100 else \"振動\")\n",
    "    ax.set_title(f'η = {lr} ({status})')\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-2, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【楕円形損失関数の特性】\")\n",
    "print(f\"ヘッセ行列: diag(2, 20)\")\n",
    "print(f\"最大固有値 λ_max = 20\")\n",
    "print(f\"理論上の安定条件: η < 2/20 = 0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. バッチ勾配降下法 vs 確率的勾配降下法\n",
    "\n",
    "### 4.1 勾配降下法の種類\n",
    "\n",
    "| 手法 | データ使用量 | 特徴 |\n",
    "|------|------------|------|\n",
    "| **Batch GD** | 全データ | 安定だが遅い |\n",
    "| **SGD** | 1サンプル | 速いがノイジー |\n",
    "| **Mini-batch GD** | 一部のデータ | バランスが良い |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形回帰での比較\n",
    "\n",
    "# データ生成\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 1)\n",
    "true_w = 3.0\n",
    "true_b = 1.0\n",
    "y = true_w * X + true_b + np.random.randn(n_samples, 1) * 0.5\n",
    "\n",
    "\n",
    "def compute_loss(w, b, X, y):\n",
    "    \"\"\"MSE損失\"\"\"\n",
    "    pred = w * X + b\n",
    "    return np.mean((pred - y) ** 2)\n",
    "\n",
    "\n",
    "def compute_grad(w, b, X, y):\n",
    "    \"\"\"MSE損失の勾配\"\"\"\n",
    "    pred = w * X + b\n",
    "    error = pred - y\n",
    "    grad_w = 2 * np.mean(error * X)\n",
    "    grad_b = 2 * np.mean(error)\n",
    "    return grad_w, grad_b\n",
    "\n",
    "\n",
    "def train_batch_gd(X, y, lr=0.1, n_epochs=50):\n",
    "    \"\"\"バッチ勾配降下法\"\"\"\n",
    "    w, b = 0.0, 0.0\n",
    "    history = {'loss': [], 'w': [], 'b': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        loss = compute_loss(w, b, X, y)\n",
    "        grad_w, grad_b = compute_grad(w, b, X, y)\n",
    "        \n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "        \n",
    "        history['loss'].append(loss)\n",
    "        history['w'].append(w)\n",
    "        history['b'].append(b)\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "def train_sgd(X, y, lr=0.01, n_epochs=50):\n",
    "    \"\"\"確率的勾配降下法\"\"\"\n",
    "    w, b = 0.0, 0.0\n",
    "    history = {'loss': [], 'w': [], 'b': []}\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # エポック開始時の損失を記録\n",
    "        loss = compute_loss(w, b, X, y)\n",
    "        history['loss'].append(loss)\n",
    "        history['w'].append(w)\n",
    "        history['b'].append(b)\n",
    "        \n",
    "        # ランダムな順序でサンプルを処理\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        for i in indices:\n",
    "            xi, yi = X[i:i+1], y[i:i+1]\n",
    "            grad_w, grad_b = compute_grad(w, b, xi, yi)\n",
    "            w -= lr * grad_w\n",
    "            b -= lr * grad_b\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "def train_minibatch_gd(X, y, batch_size=16, lr=0.1, n_epochs=50):\n",
    "    \"\"\"ミニバッチ勾配降下法\"\"\"\n",
    "    w, b = 0.0, 0.0\n",
    "    history = {'loss': [], 'w': [], 'b': []}\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        loss = compute_loss(w, b, X, y)\n",
    "        history['loss'].append(loss)\n",
    "        history['w'].append(w)\n",
    "        history['b'].append(b)\n",
    "        \n",
    "        indices = np.random.permutation(n_samples)\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_idx = indices[start:end]\n",
    "            xi, yi = X[batch_idx], y[batch_idx]\n",
    "            grad_w, grad_b = compute_grad(w, b, xi, yi)\n",
    "            w -= lr * grad_w\n",
    "            b -= lr * grad_b\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "# 学習\n",
    "w_batch, b_batch, hist_batch = train_batch_gd(X, y, lr=0.3, n_epochs=50)\n",
    "w_sgd, b_sgd, hist_sgd = train_sgd(X, y, lr=0.01, n_epochs=50)\n",
    "w_mini, b_mini, hist_mini = train_minibatch_gd(X, y, batch_size=16, lr=0.1, n_epochs=50)\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 損失の推移\n",
    "axes[0].plot(hist_batch['loss'], 'b-', linewidth=2, label='Batch GD')\n",
    "axes[0].plot(hist_sgd['loss'], 'g-', linewidth=2, alpha=0.7, label='SGD')\n",
    "axes[0].plot(hist_mini['loss'], 'r-', linewidth=2, label='Mini-batch GD')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('損失の推移')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# パラメータ空間での軌跡\n",
    "axes[1].plot(hist_batch['w'], hist_batch['b'], 'b.-', linewidth=2, markersize=4, label='Batch GD')\n",
    "axes[1].plot(hist_sgd['w'], hist_sgd['b'], 'g.-', linewidth=1, markersize=2, alpha=0.5, label='SGD')\n",
    "axes[1].plot(hist_mini['w'], hist_mini['b'], 'r.-', linewidth=2, markersize=4, label='Mini-batch GD')\n",
    "axes[1].scatter([true_w], [true_b], color='black', s=200, marker='*', zorder=5, label=f'真の値 ({true_w}, {true_b})')\n",
    "axes[1].set_xlabel('w')\n",
    "axes[1].set_ylabel('b')\n",
    "axes[1].set_title('パラメータ空間での軌跡')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【結果】\")\n",
    "print(f\"真の値:      w = {true_w:.2f}, b = {true_b:.2f}\")\n",
    "print(f\"Batch GD:    w = {w_batch:.2f}, b = {b_batch:.2f}\")\n",
    "print(f\"SGD:         w = {w_sgd:.2f}, b = {b_sgd:.2f}\")\n",
    "print(f\"Mini-batch:  w = {w_mini:.2f}, b = {b_mini:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 バッチサイズの影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズの比較\n",
    "batch_sizes = [1, 8, 32, 100]  # 100 = full batch\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    _, _, history = train_minibatch_gd(X, y, batch_size=bs, lr=0.1 if bs > 1 else 0.01, n_epochs=50)\n",
    "    label = f'batch_size={bs}' if bs < 100 else 'Full batch'\n",
    "    axes[0].plot(history['loss'], linewidth=2, label=label)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('バッチサイズと収束')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 勾配のノイズ（分散）\n",
    "n_simulations = 100\n",
    "grad_vars = []\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    grads = []\n",
    "    for _ in range(n_simulations):\n",
    "        idx = np.random.choice(n_samples, bs, replace=False)\n",
    "        grad_w, _ = compute_grad(2.0, 0.5, X[idx], y[idx])  # 固定点での勾配\n",
    "        grads.append(grad_w)\n",
    "    grad_vars.append(np.var(grads))\n",
    "\n",
    "axes[1].bar(range(len(batch_sizes)), grad_vars, tick_label=[str(bs) for bs in batch_sizes])\n",
    "axes[1].set_xlabel('バッチサイズ')\n",
    "axes[1].set_ylabel('勾配の分散')\n",
    "axes[1].set_title('バッチサイズと勾配のノイズ')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(\"- バッチサイズが小さい → 勾配のノイズが大きい\")\n",
    "print(\"- ノイズは正則化効果があり、汎化性能向上に寄与することも\")\n",
    "print(\"- 大きなバッチは安定だがメモリ消費が多い\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 損失曲面の可視化\n",
    "\n",
    "### 5.1 3D損失曲面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ空間での損失曲面\n",
    "w_range = np.linspace(-1, 5, 100)\n",
    "b_range = np.linspace(-2, 4, 100)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "Z = np.zeros_like(W)\n",
    "\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        Z[i, j] = compute_loss(W[i, j], B[i, j], X, y)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3Dプロット\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "surf = ax1.plot_surface(W, B, Z, cmap='viridis', alpha=0.8, edgecolor='none')\n",
    "ax1.set_xlabel('w')\n",
    "ax1.set_ylabel('b')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('損失曲面 (3D)')\n",
    "ax1.view_init(elev=30, azim=45)\n",
    "\n",
    "# 等高線プロット\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "contour = ax2.contour(W, B, Z, levels=30, cmap='viridis')\n",
    "ax2.scatter([true_w], [true_b], color='red', s=100, marker='*', zorder=5, label='真の値')\n",
    "ax2.set_xlabel('w')\n",
    "ax2.set_ylabel('b')\n",
    "ax2.set_title('損失曲面 (等高線)')\n",
    "ax2.legend()\n",
    "plt.colorbar(contour, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 凸関数と非凸関数\n",
    "\n",
    "### 6.1 凸関数の定義\n",
    "\n",
    "関数 $f$ が **凸** であるとは、任意の2点 $x, y$ と $0 \\le \\lambda \\le 1$ に対して：\n",
    "\n",
    "$$\n",
    "f(\\lambda x + (1-\\lambda) y) \\le \\lambda f(x) + (1-\\lambda) f(y)\n",
    "$$\n",
    "\n",
    "幾何学的には、グラフ上の任意の2点を結ぶ線分がグラフより上にあることを意味します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 凸関数 vs 非凸関数\n",
    "\n",
    "def convex_func(x):\n",
    "    \"\"\"凸関数: f(x) = x²\"\"\"\n",
    "    return x**2\n",
    "\n",
    "def nonconvex_func(x):\n",
    "    \"\"\"非凸関数: f(x) = x⁴ - 2x² + 0.5\"\"\"\n",
    "    return x**4 - 2*x**2 + 0.5\n",
    "\n",
    "x = np.linspace(-2, 2, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 凸関数\n",
    "y_convex = convex_func(x)\n",
    "axes[0].plot(x, y_convex, 'b-', linewidth=2)\n",
    "\n",
    "# 凸性の可視化: 2点を結ぶ線分\n",
    "x1, x2 = -1.5, 1.0\n",
    "y1, y2 = convex_func(x1), convex_func(x2)\n",
    "axes[0].plot([x1, x2], [y1, y2], 'r--', linewidth=2, label='2点を結ぶ線分')\n",
    "axes[0].scatter([x1, x2], [y1, y2], color='red', s=100, zorder=5)\n",
    "axes[0].fill_between(np.linspace(x1, x2, 100), \n",
    "                     convex_func(np.linspace(x1, x2, 100)),\n",
    "                     np.linspace(y1, y2, 100),\n",
    "                     alpha=0.3, color='green', label='線分より下')\n",
    "\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_title('凸関数: 局所最小 = 大域最小')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 非凸関数\n",
    "y_nonconvex = nonconvex_func(x)\n",
    "axes[1].plot(x, y_nonconvex, 'b-', linewidth=2)\n",
    "\n",
    "# 極値点をマーク\n",
    "x_min_local = np.array([-1, 1])\n",
    "y_min_local = nonconvex_func(x_min_local)\n",
    "axes[1].scatter(x_min_local, y_min_local, color='green', s=100, zorder=5, label='局所最小点')\n",
    "\n",
    "x_max_local = np.array([0])\n",
    "y_max_local = nonconvex_func(x_max_local)\n",
    "axes[1].scatter(x_max_local, y_max_local, color='red', s=100, zorder=5, label='局所最大点')\n",
    "\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('f(x)')\n",
    "axes[1].set_title('非凸関数: 複数の極値')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【凸関数の利点】\")\n",
    "print(\"- 局所最小点 = 大域最小点（唯一解）\")\n",
    "print(\"- 勾配降下法で必ず最適解に到達\")\n",
    "print(\"\")\n",
    "print(\"【非凸関数の課題】\")\n",
    "print(\"- 複数の局所最小点が存在\")\n",
    "print(\"- 初期値によって到達する解が異なる\")\n",
    "print(\"- ニューラルネットワークの損失関数は一般に非凸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非凸関数での勾配降下法の挙動\n",
    "\n",
    "def nonconvex_grad(x):\n",
    "    return 4*x**3 - 4*x\n",
    "\n",
    "# 異なる初期値からの最適化\n",
    "initial_points = [-2.0, -0.5, 0.5, 2.0]\n",
    "colors = ['red', 'green', 'blue', 'orange']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x = np.linspace(-2.5, 2.5, 200)\n",
    "y = nonconvex_func(x)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x, y, 'k-', linewidth=2, alpha=0.5)\n",
    "\n",
    "for x0, color in zip(initial_points, colors):\n",
    "    history = gradient_descent_1d(nonconvex_func, nonconvex_grad, x0=x0, lr=0.1, n_steps=30)\n",
    "    \n",
    "    axes[0].plot(history['x'], history['f'], 'o-', color=color, markersize=6, \n",
    "                 linewidth=2, label=f'x₀ = {x0}')\n",
    "    \n",
    "    axes[1].plot(history['x'], 'o-', color=color, markersize=6, \n",
    "                 linewidth=2, label=f'x₀ = {x0}')\n",
    "\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_title('非凸関数での最適化軌跡')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].axhline(y=-1, color='gray', linestyle='--', alpha=0.5, label='局所最小 x=-1')\n",
    "axes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='局所最小 x=1')\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('x')\n",
    "axes[1].set_title('パラメータの収束')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(\"初期値によって収束先が異なる：\")\n",
    "for x0, color in zip(initial_points, colors):\n",
    "    history = gradient_descent_1d(nonconvex_func, nonconvex_grad, x0=x0, lr=0.1, n_steps=30)\n",
    "    print(f\"  x₀ = {x0:5.1f} → x* = {history['x'][-1]:6.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 収束条件と停止基準\n",
    "\n",
    "### 7.1 一般的な停止基準\n",
    "\n",
    "1. **反復回数**: 最大エポック数に達したら停止\n",
    "2. **勾配ノルム**: $\\|\\nabla L\\| < \\epsilon$\n",
    "3. **パラメータ変化**: $\\|\\theta_{t+1} - \\theta_t\\| < \\epsilon$\n",
    "4. **損失変化**: $|L_{t+1} - L_t| < \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_stopping(f, grad_f, x0, lr=0.1, max_steps=1000, \n",
    "                                   grad_tol=1e-6, param_tol=1e-8, loss_tol=1e-10):\n",
    "    \"\"\"\n",
    "    停止条件付き勾配降下法\n",
    "    \n",
    "    Returns:\n",
    "        final_x, history, stop_reason\n",
    "    \"\"\"\n",
    "    history = {'x': [x0], 'f': [f(x0)], 'grad_norm': [abs(grad_f(x0))]}\n",
    "    x = x0\n",
    "    stop_reason = \"max_steps\"\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = abs(grad)\n",
    "        \n",
    "        # 勾配ノルムによる停止\n",
    "        if grad_norm < grad_tol:\n",
    "            stop_reason = f\"勾配ノルム < {grad_tol}\"\n",
    "            break\n",
    "        \n",
    "        # パラメータ更新\n",
    "        x_new = x - lr * grad\n",
    "        \n",
    "        # パラメータ変化による停止\n",
    "        if abs(x_new - x) < param_tol:\n",
    "            stop_reason = f\"パラメータ変化 < {param_tol}\"\n",
    "            break\n",
    "        \n",
    "        # 損失変化による停止\n",
    "        loss_old = f(x)\n",
    "        loss_new = f(x_new)\n",
    "        if abs(loss_new - loss_old) < loss_tol:\n",
    "            stop_reason = f\"損失変化 < {loss_tol}\"\n",
    "            break\n",
    "        \n",
    "        x = x_new\n",
    "        history['x'].append(x)\n",
    "        history['f'].append(f(x))\n",
    "        history['grad_norm'].append(grad_norm)\n",
    "    \n",
    "    return x, history, stop_reason\n",
    "\n",
    "\n",
    "# 実行\n",
    "x_final, history, reason = gradient_descent_with_stopping(\n",
    "    quadratic, quadratic_grad, x0=0.0, lr=0.3, \n",
    "    max_steps=1000, grad_tol=1e-6\n",
    ")\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "steps = range(len(history['x']))\n",
    "\n",
    "axes[0].plot(steps, history['x'], 'b.-')\n",
    "axes[0].axhline(y=2, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('ステップ')\n",
    "axes[0].set_ylabel('x')\n",
    "axes[0].set_title('パラメータの収束')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].semilogy(steps, history['f'], 'b.-')\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('f(x)')\n",
    "axes[1].set_title('損失')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].semilogy(steps, history['grad_norm'], 'g.-')\n",
    "axes[2].axhline(y=1e-6, color='red', linestyle='--', label='停止閾値')\n",
    "axes[2].set_xlabel('ステップ')\n",
    "axes[2].set_ylabel('|勾配|')\n",
    "axes[2].set_title('勾配ノルム')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"【結果】\")\n",
    "print(f\"最終値: x = {x_final:.8f}\")\n",
    "print(f\"ステップ数: {len(history['x']) - 1}\")\n",
    "print(f\"停止理由: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 演習問題\n",
    "\n",
    "### 演習 8.1: Booth関数の最適化\n",
    "\n",
    "Booth関数 $f(x, y) = (x + 2y - 7)^2 + (2x + y - 5)^2$ を勾配降下法で最適化してください。\n",
    "最適点は $(1, 3)$ です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.1: 解答欄\n",
    "\n",
    "def booth(x, y):\n",
    "    # TODO: 実装\n",
    "    pass\n",
    "\n",
    "def booth_grad(x, y):\n",
    "    # TODO: 実装\n",
    "    pass\n",
    "\n",
    "# TODO: 勾配降下法を実行し、軌跡を可視化\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 8.2: 学習率のチューニング\n",
    "\n",
    "Rosenbrock関数に対して、様々な学習率を試し、最も効率的に収束する学習率を見つけてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.2: 解答欄\n",
    "\n",
    "# TODO: 学習率 0.0001, 0.0005, 0.001, 0.005 を比較\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 8.3: バッチサイズとノイズ\n",
    "\n",
    "異なるバッチサイズでの勾配分散を計算し、バッチサイズが大きくなるにつれて分散がどのように減少するかを確認してください。理論的には分散は $O(1/n)$ で減少します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.3: 解答欄\n",
    "\n",
    "# TODO: バッチサイズ 1, 2, 4, 8, 16, 32, 64 での勾配分散を計算\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **最適化問題の定式化**: $\\theta^* = \\arg\\min L(\\theta)$\n",
    "\n",
    "2. **勾配降下法**: $\\theta_{t+1} = \\theta_t - \\eta \\nabla L$\n",
    "\n",
    "3. **学習率の重要性**: 大きすぎると発散、小さすぎると収束が遅い\n",
    "\n",
    "4. **バッチサイズのトレードオフ**:\n",
    "   - 大きい → 安定だが遅い\n",
    "   - 小さい → 速いがノイジー\n",
    "\n",
    "5. **凸 vs 非凸**: ニューラルネットは非凸で複数の局所解が存在\n",
    "\n",
    "### 次のノートブック（111: Momentum と Nesterov）への橋渡し\n",
    "\n",
    "基本的なSGDには以下の問題があります：\n",
    "\n",
    "- **振動**: 急峻な谷では行ったり来たりする\n",
    "- **鞍点での停滞**: 勾配が小さい領域で動きが遅くなる\n",
    "\n",
    "次のノートブックでは、これらの問題を解決する **Momentum** と **Nesterov加速** を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. *COMPSTAT*.\n",
    "2. Ruder, S. (2016). An overview of gradient descent optimization algorithms. *arXiv:1609.04747*.\n",
    "3. Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
