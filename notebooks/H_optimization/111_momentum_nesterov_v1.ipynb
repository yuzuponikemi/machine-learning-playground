{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 111: Momentum と Nesterov 加速\n",
    "\n",
    "## Momentum SGD and Nesterov Accelerated Gradient\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Phase 10「最適化手法」** の第2章として、基本的なSGDの問題を解決する **Momentum** と **Nesterov加速** を学びます。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **Momentum** の直感と数式を理解する\n",
    "2. **Nesterov Accelerated Gradient (NAG)** の仕組みを理解する\n",
    "3. SGDとの比較を通じて改善効果を確認する\n",
    "4. 各手法の実装と可視化\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 110 の内容（勾配降下法の基礎）\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [SGDの問題点](#1-sgdの問題点)\n",
    "2. [Momentum SGD](#2-momentum-sgd)\n",
    "3. [Nesterov Accelerated Gradient](#3-nesterov-accelerated-gradient)\n",
    "4. [実装と比較](#4-実装と比較)\n",
    "5. [様々な損失曲面での挙動](#5-様々な損失曲面での挙動)\n",
    "6. [ハイパーパラメータの影響](#6-ハイパーパラメータの影響)\n",
    "7. [演習問題](#7-演習問題)\n",
    "8. [まとめと次のステップ](#8-まとめと次のステップ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. SGDの問題点\n",
    "\n",
    "### 1.1 振動問題\n",
    "\n",
    "急峻な谷（細長い楕円形の等高線）を持つ損失関数では、SGDは谷の壁に沿って行ったり来たりしながらゆっくりと進みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 病的な損失関数（急峻な谷）\n",
    "\n",
    "def pathological_loss(x, y, condition_number=50):\n",
    "    \"\"\"条件数の大きい楕円形損失関数\"\"\"\n",
    "    return x**2 + condition_number * y**2\n",
    "\n",
    "def pathological_grad(x, y, condition_number=50):\n",
    "    \"\"\"勾配\"\"\"\n",
    "    return np.array([2*x, 2*condition_number*y])\n",
    "\n",
    "\n",
    "def sgd_optimize(grad_fn, start, lr=0.01, n_steps=100, **kwargs):\n",
    "    \"\"\"基本的なSGD\"\"\"\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_fn(pos[0], pos[1], **kwargs)\n",
    "        pos = pos - lr * grad\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "# SGDの振動を可視化\n",
    "start = (3, 1)\n",
    "path_sgd = sgd_optimize(pathological_grad, start, lr=0.01, n_steps=100, condition_number=50)\n",
    "\n",
    "# 等高線\n",
    "x = np.linspace(-4, 4, 200)\n",
    "y = np.linspace(-2, 2, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = pathological_loss(X, Y, condition_number=50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 軌跡\n",
    "axes[0].contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.7)\n",
    "axes[0].plot(path_sgd[:, 0], path_sgd[:, 1], 'ro-', markersize=3, linewidth=1, alpha=0.7)\n",
    "axes[0].scatter([0], [0], color='green', s=100, marker='*', zorder=5, label='最適点')\n",
    "axes[0].scatter([start[0]], [start[1]], color='blue', s=100, zorder=5, label='開始点')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('SGDの振動問題（条件数 = 50）')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(-4, 4)\n",
    "axes[0].set_ylim(-2, 2)\n",
    "\n",
    "# パラメータの推移\n",
    "axes[1].plot(path_sgd[:, 0], label='x', linewidth=2)\n",
    "axes[1].plot(path_sgd[:, 1], label='y', linewidth=2)\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('パラメータ値')\n",
    "axes[1].set_title('パラメータの収束')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【問題点】\")\n",
    "print(\"- y方向: 勾配が大きいため激しく振動\")\n",
    "print(\"- x方向: 勾配が小さいためゆっくり進む\")\n",
    "print(\"- 全体として収束が遅い\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 条件数と収束速度\n",
    "\n",
    "**条件数** (Condition Number) は、ヘッセ行列の最大固有値と最小固有値の比です：\n",
    "\n",
    "$$\n",
    "\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\n",
    "$$\n",
    "\n",
    "条件数が大きいほど、SGDの収束は遅くなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条件数と収束速度の関係\n",
    "condition_numbers = [1, 10, 50, 100]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for cn in condition_numbers:\n",
    "    path = sgd_optimize(pathological_grad, (3, 1), lr=0.01, n_steps=200, condition_number=cn)\n",
    "    losses = [pathological_loss(p[0], p[1], cn) for p in path]\n",
    "    ax.semilogy(losses, linewidth=2, label=f'条件数 κ = {cn}')\n",
    "\n",
    "ax.set_xlabel('ステップ')\n",
    "ax.set_ylabel('損失（対数スケール）')\n",
    "ax.set_title('条件数と収束速度の関係')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(\"- 条件数が大きいほど収束が遅い\")\n",
    "print(\"- κ = 100 では、κ = 1 の約100倍のステップが必要\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Momentum SGD\n",
    "\n",
    "### 2.1 アイデア\n",
    "\n",
    "物理学のアナロジー：ボールが斜面を転がる様子を想像してください。\n",
    "\n",
    "- ボールは **速度（モーメンタム）** を持つ\n",
    "- 一度方向が決まると、すぐには方向転換しない\n",
    "- これにより振動が抑制され、一貫した方向への移動が促進される\n",
    "\n",
    "### 2.2 数式\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_t &= \\mu v_{t-1} + \\eta \\nabla L(\\theta_t) \\\\\n",
    "\\theta_{t+1} &= \\theta_t - v_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $v$: 速度（velocity）\n",
    "- $\\mu$: モーメンタム係数（通常 0.9）\n",
    "- 過去の勾配の指数移動平均を使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_optimize(grad_fn, start, lr=0.01, momentum=0.9, n_steps=100, **kwargs):\n",
    "    \"\"\"\n",
    "    Momentum SGD\n",
    "    \n",
    "    v = μ * v + η * ∇L\n",
    "    θ = θ - v\n",
    "    \"\"\"\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    velocity = np.zeros_like(pos)\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_fn(pos[0], pos[1], **kwargs)\n",
    "        velocity = momentum * velocity + lr * grad\n",
    "        pos = pos - velocity\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "# Momentumの直感的な理解\n",
    "print(\"【Momentumの効果】\")\n",
    "print(\"\")\n",
    "print(\"速度の更新: v = μ * v + η * ∇L\")\n",
    "print(\"\")\n",
    "print(\"過去の勾配の重み付け（μ = 0.9 の場合）:\")\n",
    "weights = [0.9**i for i in range(10)]\n",
    "for i, w in enumerate(weights):\n",
    "    bar = \"█\" * int(w * 20)\n",
    "    print(f\"  t-{i}: {w:.4f} {bar}\")\n",
    "print(f\"\")\n",
    "print(f\"  合計: {sum(weights):.2f}（μ/(1-μ) = {0.9/0.1:.1f}）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD vs Momentum の比較\n",
    "start = (3, 1)\n",
    "\n",
    "path_sgd = sgd_optimize(pathological_grad, start, lr=0.01, n_steps=100, condition_number=50)\n",
    "path_momentum = momentum_optimize(pathological_grad, start, lr=0.01, momentum=0.9, n_steps=100, condition_number=50)\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 軌跡\n",
    "axes[0].contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.7)\n",
    "axes[0].plot(path_sgd[:, 0], path_sgd[:, 1], 'r.-', markersize=3, linewidth=1, alpha=0.7, label='SGD')\n",
    "axes[0].plot(path_momentum[:, 0], path_momentum[:, 1], 'b.-', markersize=3, linewidth=1, alpha=0.7, label='Momentum')\n",
    "axes[0].scatter([0], [0], color='green', s=100, marker='*', zorder=5, label='最適点')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('SGD vs Momentum')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(-4, 4)\n",
    "axes[0].set_ylim(-2, 2)\n",
    "\n",
    "# 損失の推移\n",
    "losses_sgd = [pathological_loss(p[0], p[1], 50) for p in path_sgd]\n",
    "losses_momentum = [pathological_loss(p[0], p[1], 50) for p in path_momentum]\n",
    "\n",
    "axes[1].semilogy(losses_sgd, 'r-', linewidth=2, label='SGD')\n",
    "axes[1].semilogy(losses_momentum, 'b-', linewidth=2, label='Momentum')\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('損失（対数スケール）')\n",
    "axes[1].set_title('収束速度の比較')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【結果】\")\n",
    "print(f\"SGD      最終損失: {losses_sgd[-1]:.6f}\")\n",
    "print(f\"Momentum 最終損失: {losses_momentum[-1]:.6f}\")\n",
    "print(f\"\")\n",
    "print(\"Momentumは振動を抑制し、より効率的に収束\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Momentumが振動を抑制する理由\n",
    "\n",
    "1. **一貫した方向の勾配は加速される**\n",
    "   - 同じ方向の勾配が続くと、速度が積み重なる\n",
    "\n",
    "2. **振動方向の勾配は相殺される**\n",
    "   - 符号が交互に変わる勾配は、速度の更新で打ち消し合う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 振動の抑制メカニズムを可視化\n",
    "\n",
    "# シミュレーション: 一定方向 vs 交互方向\n",
    "n_steps = 20\n",
    "mu = 0.9\n",
    "\n",
    "# ケース1: 一貫した勾配（x方向）\n",
    "grads_consistent = np.ones(n_steps)\n",
    "velocity_consistent = np.zeros(n_steps + 1)\n",
    "for t in range(n_steps):\n",
    "    velocity_consistent[t + 1] = mu * velocity_consistent[t] + grads_consistent[t]\n",
    "\n",
    "# ケース2: 振動する勾配（y方向）\n",
    "grads_oscillating = np.array([1 if i % 2 == 0 else -1 for i in range(n_steps)])\n",
    "velocity_oscillating = np.zeros(n_steps + 1)\n",
    "for t in range(n_steps):\n",
    "    velocity_oscillating[t + 1] = mu * velocity_oscillating[t] + grads_oscillating[t]\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "steps = range(n_steps + 1)\n",
    "\n",
    "axes[0].bar(range(n_steps), grads_consistent, alpha=0.5, label='勾配')\n",
    "axes[0].plot(steps, velocity_consistent, 'b-', linewidth=2, marker='o', label='速度')\n",
    "axes[0].set_xlabel('ステップ')\n",
    "axes[0].set_ylabel('値')\n",
    "axes[0].set_title('一貫した勾配 → 速度が加速')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].bar(range(n_steps), grads_oscillating, alpha=0.5, label='勾配')\n",
    "axes[1].plot(steps, velocity_oscillating, 'r-', linewidth=2, marker='o', label='速度')\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('値')\n",
    "axes[1].set_title('振動する勾配 → 速度は相殺される')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(f\"一貫した勾配の最終速度: {velocity_consistent[-1]:.2f}\")\n",
    "print(f\"振動する勾配の最終速度: {velocity_oscillating[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Nesterov Accelerated Gradient\n",
    "\n",
    "### 3.1 アイデア\n",
    "\n",
    "Nesterov Accelerated Gradient (NAG) は、Momentumを改良したものです。\n",
    "\n",
    "**Momentumの問題点**:\n",
    "- 現在位置で勾配を計算してから移動する\n",
    "- 移動先で実際に必要な勾配とはずれている可能性がある\n",
    "\n",
    "**Nesterovの解決策**:\n",
    "- まず「移動予定先」を計算（lookahead）\n",
    "- その位置で勾配を計算\n",
    "- より正確な勾配情報が得られる\n",
    "\n",
    "### 3.2 数式\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_t &= \\mu v_{t-1} + \\eta \\nabla L(\\theta_t - \\mu v_{t-1}) \\\\\n",
    "\\theta_{t+1} &= \\theta_t - v_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "勾配を計算する位置が $\\theta_t$ ではなく $\\theta_t - \\mu v_{t-1}$（先読み位置）になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_optimize(grad_fn, start, lr=0.01, momentum=0.9, n_steps=100, **kwargs):\n",
    "    \"\"\"\n",
    "    Nesterov Accelerated Gradient (NAG)\n",
    "    \n",
    "    1. 先読み位置を計算: θ_lookahead = θ - μ * v\n",
    "    2. 先読み位置で勾配を計算: g = ∇L(θ_lookahead)\n",
    "    3. 速度を更新: v = μ * v + η * g\n",
    "    4. パラメータを更新: θ = θ - v\n",
    "    \"\"\"\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    velocity = np.zeros_like(pos)\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # 先読み位置\n",
    "        lookahead = pos - momentum * velocity\n",
    "        # 先読み位置で勾配を計算\n",
    "        grad = grad_fn(lookahead[0], lookahead[1], **kwargs)\n",
    "        # 速度更新\n",
    "        velocity = momentum * velocity + lr * grad\n",
    "        # パラメータ更新\n",
    "        pos = pos - velocity\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "# Nesterovの先読みを可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# 1次元の例\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = x**2  # 二次関数\n",
    "\n",
    "ax.plot(x, y, 'b-', linewidth=2)\n",
    "\n",
    "# 現在位置\n",
    "current_x = 2.0\n",
    "current_y = current_x**2\n",
    "velocity = 0.5  # 仮の速度\n",
    "mu = 0.9\n",
    "\n",
    "# 先読み位置\n",
    "lookahead_x = current_x - mu * velocity\n",
    "lookahead_y = lookahead_x**2\n",
    "\n",
    "# プロット\n",
    "ax.scatter([current_x], [current_y], color='blue', s=150, zorder=5, label='現在位置 θ')\n",
    "ax.scatter([lookahead_x], [lookahead_y], color='red', s=150, zorder=5, label='先読み位置 θ - μv')\n",
    "\n",
    "# 矢印\n",
    "ax.annotate('', xy=(lookahead_x, lookahead_y), xytext=(current_x, current_y),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "ax.text((current_x + lookahead_x)/2 + 0.1, (current_y + lookahead_y)/2 + 0.3, 'μv (モーメンタム)', fontsize=11)\n",
    "\n",
    "# 勾配を表示\n",
    "grad_current = 2 * current_x\n",
    "grad_lookahead = 2 * lookahead_x\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Nesterovの先読み（Lookahead）')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"現在位置での勾配: {grad_current:.2f}\")\n",
    "print(f\"先読み位置での勾配: {grad_lookahead:.2f}\")\n",
    "print(\"\")\n",
    "print(\"Nesterovは先読み位置の勾配を使用することで、\")\n",
    "print(\"より正確な更新方向を得ることができます。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 実装と比較\n",
    "\n",
    "### 4.1 3手法の比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD vs Momentum vs Nesterov\n",
    "start = (3, 1)\n",
    "n_steps = 100\n",
    "\n",
    "path_sgd = sgd_optimize(pathological_grad, start, lr=0.01, n_steps=n_steps, condition_number=50)\n",
    "path_momentum = momentum_optimize(pathological_grad, start, lr=0.01, momentum=0.9, n_steps=n_steps, condition_number=50)\n",
    "path_nesterov = nesterov_optimize(pathological_grad, start, lr=0.01, momentum=0.9, n_steps=n_steps, condition_number=50)\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 軌跡\n",
    "axes[0].contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.7)\n",
    "axes[0].plot(path_sgd[:, 0], path_sgd[:, 1], 'r.-', markersize=2, linewidth=1, alpha=0.7, label='SGD')\n",
    "axes[0].plot(path_momentum[:, 0], path_momentum[:, 1], 'b.-', markersize=2, linewidth=1, alpha=0.7, label='Momentum')\n",
    "axes[0].plot(path_nesterov[:, 0], path_nesterov[:, 1], 'g.-', markersize=2, linewidth=1, alpha=0.7, label='Nesterov')\n",
    "axes[0].scatter([0], [0], color='black', s=100, marker='*', zorder=5)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('最適化軌跡の比較')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(-1, 4)\n",
    "axes[0].set_ylim(-1.5, 1.5)\n",
    "\n",
    "# 損失の推移\n",
    "losses_sgd = [pathological_loss(p[0], p[1], 50) for p in path_sgd]\n",
    "losses_momentum = [pathological_loss(p[0], p[1], 50) for p in path_momentum]\n",
    "losses_nesterov = [pathological_loss(p[0], p[1], 50) for p in path_nesterov]\n",
    "\n",
    "axes[1].semilogy(losses_sgd, 'r-', linewidth=2, label='SGD')\n",
    "axes[1].semilogy(losses_momentum, 'b-', linewidth=2, label='Momentum')\n",
    "axes[1].semilogy(losses_nesterov, 'g-', linewidth=2, label='Nesterov')\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('損失（対数スケール）')\n",
    "axes[1].set_title('収束速度の比較')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【最終損失】\")\n",
    "print(f\"SGD:      {losses_sgd[-1]:.6f}\")\n",
    "print(f\"Momentum: {losses_momentum[-1]:.6f}\")\n",
    "print(f\"Nesterov: {losses_nesterov[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 PyTorchスタイルの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    \"\"\"PyTorchスタイルのSGDオプティマイザ\"\"\"\n",
    "    \n",
    "    def __init__(self, params, lr=0.01, momentum=0.0, nesterov=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            params: パラメータのリスト\n",
    "            lr: 学習率\n",
    "            momentum: モーメンタム係数\n",
    "            nesterov: Nesterovモーメンタムを使用するか\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.velocities = [np.zeros_like(p) for p in params]\n",
    "    \n",
    "    def step(self, grads):\n",
    "        \"\"\"パラメータを更新\"\"\"\n",
    "        for i, (param, grad, v) in enumerate(zip(self.params, grads, self.velocities)):\n",
    "            if self.momentum > 0:\n",
    "                v = self.momentum * v + grad\n",
    "                self.velocities[i] = v\n",
    "                \n",
    "                if self.nesterov:\n",
    "                    update = self.momentum * v + grad\n",
    "                else:\n",
    "                    update = v\n",
    "            else:\n",
    "                update = grad\n",
    "            \n",
    "            param -= self.lr * update\n",
    "\n",
    "\n",
    "# 使用例\n",
    "params = [np.array([3.0, 1.0])]\n",
    "\n",
    "# 各オプティマイザをテスト\n",
    "optimizers = {\n",
    "    'SGD': SGDOptimizer(params=[np.array([3.0, 1.0])], lr=0.01),\n",
    "    'Momentum': SGDOptimizer(params=[np.array([3.0, 1.0])], lr=0.01, momentum=0.9),\n",
    "    'Nesterov': SGDOptimizer(params=[np.array([3.0, 1.0])], lr=0.01, momentum=0.9, nesterov=True),\n",
    "}\n",
    "\n",
    "print(\"SGDOptimizer クラスを定義しました\")\n",
    "print(\"\")\n",
    "print(\"使用方法:\")\n",
    "print(\"  optimizer = SGDOptimizer(params, lr=0.01, momentum=0.9, nesterov=True)\")\n",
    "print(\"  optimizer.step(grads)  # 勾配でパラメータを更新\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 様々な損失曲面での挙動\n",
    "\n",
    "### 5.1 Rosenbrock関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x, y):\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_grad(x, y):\n",
    "    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dy = 200 * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "\n",
    "# 各手法での最適化\n",
    "start = (-1.5, 1.5)\n",
    "n_steps = 1000\n",
    "\n",
    "path_sgd = sgd_optimize(rosenbrock_grad, start, lr=0.001, n_steps=n_steps)\n",
    "path_momentum = momentum_optimize(rosenbrock_grad, start, lr=0.001, momentum=0.9, n_steps=n_steps)\n",
    "path_nesterov = nesterov_optimize(rosenbrock_grad, start, lr=0.001, momentum=0.9, n_steps=n_steps)\n",
    "\n",
    "# 等高線\n",
    "x = np.linspace(-2, 2, 200)\n",
    "y = np.linspace(-1, 3, 200)\n",
    "X_r, Y_r = np.meshgrid(x, y)\n",
    "Z_r = rosenbrock(X_r, Y_r)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 軌跡\n",
    "axes[0].contour(X_r, Y_r, np.log(Z_r + 1), levels=30, cmap='viridis', alpha=0.7)\n",
    "axes[0].plot(path_sgd[:, 0], path_sgd[:, 1], 'r-', linewidth=1, alpha=0.7, label='SGD')\n",
    "axes[0].plot(path_momentum[:, 0], path_momentum[:, 1], 'b-', linewidth=1, alpha=0.7, label='Momentum')\n",
    "axes[0].plot(path_nesterov[:, 0], path_nesterov[:, 1], 'g-', linewidth=1, alpha=0.7, label='Nesterov')\n",
    "axes[0].scatter([1], [1], color='black', s=100, marker='*', zorder=5, label='最適点')\n",
    "axes[0].scatter([start[0]], [start[1]], color='orange', s=100, zorder=5, label='開始点')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Rosenbrock関数での最適化')\n",
    "axes[0].legend()\n",
    "\n",
    "# 損失の推移\n",
    "losses_sgd = [rosenbrock(p[0], p[1]) for p in path_sgd]\n",
    "losses_momentum = [rosenbrock(p[0], p[1]) for p in path_momentum]\n",
    "losses_nesterov = [rosenbrock(p[0], p[1]) for p in path_nesterov]\n",
    "\n",
    "axes[1].semilogy(losses_sgd, 'r-', linewidth=2, label='SGD')\n",
    "axes[1].semilogy(losses_momentum, 'b-', linewidth=2, label='Momentum')\n",
    "axes[1].semilogy(losses_nesterov, 'g-', linewidth=2, label='Nesterov')\n",
    "axes[1].set_xlabel('ステップ')\n",
    "axes[1].set_ylabel('損失（対数スケール）')\n",
    "axes[1].set_title('収束速度の比較')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 鞍点（Saddle Point）での挙動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saddle(x, y):\n",
    "    return x**2 - y**2\n",
    "\n",
    "def saddle_grad(x, y):\n",
    "    return np.array([2*x, -2*y])\n",
    "\n",
    "\n",
    "# 鞍点付近からの最適化\n",
    "start = (0.1, 0.1)\n",
    "n_steps = 50\n",
    "\n",
    "path_sgd = sgd_optimize(saddle_grad, start, lr=0.1, n_steps=n_steps)\n",
    "path_momentum = momentum_optimize(saddle_grad, start, lr=0.1, momentum=0.9, n_steps=n_steps)\n",
    "path_nesterov = nesterov_optimize(saddle_grad, start, lr=0.1, momentum=0.9, n_steps=n_steps)\n",
    "\n",
    "# 等高線\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "X_s, Y_s = np.meshgrid(x, y)\n",
    "Z_s = saddle(X_s, Y_s)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "contour = ax.contour(X_s, Y_s, Z_s, levels=20, cmap='coolwarm')\n",
    "ax.plot(path_sgd[:, 0], path_sgd[:, 1], 'ro-', markersize=4, linewidth=2, label='SGD')\n",
    "ax.plot(path_momentum[:, 0], path_momentum[:, 1], 'bs-', markersize=4, linewidth=2, label='Momentum')\n",
    "ax.plot(path_nesterov[:, 0], path_nesterov[:, 1], 'g^-', markersize=4, linewidth=2, label='Nesterov')\n",
    "ax.scatter([0], [0], color='black', s=200, marker='X', zorder=5, label='鞍点')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('鞍点での挙動')\n",
    "ax.legend()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "\n",
    "plt.colorbar(contour)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(\"- SGD: 鞍点に吸い込まれやすい\")\n",
    "print(\"- Momentum/Nesterov: 慣性により鞍点を通過しやすい\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ハイパーパラメータの影響\n",
    "\n",
    "### 6.1 モーメンタム係数の影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モーメンタム係数の比較\n",
    "momentum_values = [0.0, 0.5, 0.9, 0.99]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for mu in momentum_values:\n",
    "    if mu == 0:\n",
    "        path = sgd_optimize(pathological_grad, (3, 1), lr=0.01, n_steps=100, condition_number=50)\n",
    "    else:\n",
    "        path = momentum_optimize(pathological_grad, (3, 1), lr=0.01, momentum=mu, n_steps=100, condition_number=50)\n",
    "    \n",
    "    losses = [pathological_loss(p[0], p[1], 50) for p in path]\n",
    "    axes[0].semilogy(losses, linewidth=2, label=f'μ = {mu}')\n",
    "\n",
    "axes[0].set_xlabel('ステップ')\n",
    "axes[0].set_ylabel('損失（対数スケール）')\n",
    "axes[0].set_title('モーメンタム係数と収束')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 効果的な学習率の計算\n",
    "mu_range = np.linspace(0, 0.99, 100)\n",
    "effective_lr = 1 / (1 - mu_range)  # 定常状態での実効学習率\n",
    "\n",
    "axes[1].plot(mu_range, effective_lr, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('モーメンタム係数 μ')\n",
    "axes[1].set_ylabel('実効学習率倍率 1/(1-μ)')\n",
    "axes[1].set_title('モーメンタムによる実効学習率の増加')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 20)\n",
    "\n",
    "# 典型的な値をマーク\n",
    "for mu in [0.9, 0.99]:\n",
    "    eff = 1 / (1 - mu)\n",
    "    axes[1].scatter([mu], [eff], s=100, zorder=5)\n",
    "    axes[1].annotate(f'μ={mu}: {eff:.0f}x', (mu, eff), textcoords=\"offset points\", xytext=(10, 10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【モーメンタム係数の指針】\")\n",
    "print(\"μ = 0.9:  一般的な選択（実効10倍）\")\n",
    "print(\"μ = 0.99: より強いモーメンタム（実効100倍）\")\n",
    "print(\"\")\n",
    "print(\"注意: μが大きいと発散のリスクも高まる\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 学習率とモーメンタムの組み合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率とモーメンタムの組み合わせ\n",
    "lrs = [0.001, 0.01, 0.05]\n",
    "mus = [0.0, 0.9]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for i, mu in enumerate(mus):\n",
    "    for j, lr in enumerate(lrs):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        if mu == 0:\n",
    "            path = sgd_optimize(pathological_grad, (3, 1), lr=lr, n_steps=100, condition_number=50)\n",
    "        else:\n",
    "            path = momentum_optimize(pathological_grad, (3, 1), lr=lr, momentum=mu, n_steps=100, condition_number=50)\n",
    "        \n",
    "        ax.contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.5)\n",
    "        ax.plot(path[:, 0], path[:, 1], 'r.-', markersize=2, linewidth=1)\n",
    "        ax.scatter([0], [0], color='black', s=50, marker='*')\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        \n",
    "        final_loss = pathological_loss(path[-1, 0], path[-1, 1], 50)\n",
    "        title = f'lr={lr}, μ={mu}\\n最終損失: {final_loss:.4f}'\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlim(-1, 4)\n",
    "        ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 演習問題\n",
    "\n",
    "### 演習 7.1: Beale関数での最適化\n",
    "\n",
    "Beale関数で SGD, Momentum, Nesterov を比較してください。\n",
    "\n",
    "$$\n",
    "f(x, y) = (1.5 - x + xy)^2 + (2.25 - x + xy^2)^2 + (2.625 - x + xy^3)^2\n",
    "$$\n",
    "\n",
    "最適点: $(3, 0.5)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.1: 解答欄\n",
    "\n",
    "def beale(x, y):\n",
    "    # TODO: 実装\n",
    "    pass\n",
    "\n",
    "def beale_grad(x, y):\n",
    "    # TODO: 実装\n",
    "    pass\n",
    "\n",
    "# TODO: 3手法を比較\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 7.2: モーメンタム係数のチューニング\n",
    "\n",
    "病的な損失関数（条件数=100）に対して、最も効率的に収束するモーメンタム係数を見つけてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.2: 解答欄\n",
    "\n",
    "# TODO: μ = 0.5, 0.7, 0.9, 0.95, 0.99 を比較\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 7.3: Nesterovの優位性\n",
    "\n",
    "Nesterovが Momentum より優れているケースを見つけてください。\n",
    "ヒント: 最適点をオーバーシュートしそうな状況で違いが顕著になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.3: 解答欄\n",
    "\n",
    "# TODO: Nesterovの先読みが効果的なケースを探す\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **SGDの問題点**: 急峻な谷での振動、収束の遅さ\n",
    "\n",
    "2. **Momentum SGD**:\n",
    "   - 速度（過去の勾配の指数移動平均）を導入\n",
    "   - 一貫した方向への移動を加速\n",
    "   - 振動を抑制\n",
    "\n",
    "3. **Nesterov Accelerated Gradient**:\n",
    "   - 先読み位置で勾配を計算\n",
    "   - オーバーシュートを軽減\n",
    "   - Momentumよりやや速い収束\n",
    "\n",
    "4. **ハイパーパラメータ**:\n",
    "   - モーメンタム係数 $\\mu$: 通常 0.9\n",
    "   - 学習率は実効的に $1/(1-\\mu)$ 倍される\n",
    "\n",
    "### 次のノートブック（112: 適応学習率手法）への橋渡し\n",
    "\n",
    "Momentum/Nesterovは以下の問題を解決しません：\n",
    "\n",
    "- **パラメータごとの学習率**: すべてのパラメータに同じ学習率を適用\n",
    "- **学習率のチューニング**: 適切な学習率の選択が困難\n",
    "\n",
    "次のノートブックでは、これらを解決する **適応学習率手法**（Adagrad, RMSprop, Adam）を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. *USSR Computational Mathematics and Mathematical Physics*.\n",
    "2. Nesterov, Y. (1983). A method for solving the convex programming problem with convergence rate O(1/k²). *Soviet Mathematics Doklady*.\n",
    "3. Sutskever, I., et al. (2013). On the importance of initialization and momentum in deep learning. *ICML*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
