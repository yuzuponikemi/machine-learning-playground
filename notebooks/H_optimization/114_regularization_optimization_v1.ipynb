{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 114: 正則化と最適化\n",
    "\n",
    "## Regularization and Optimization\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Phase 10「最適化手法」** の第5章として、**正則化** と最適化の関係を学びます。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **L1/L2正則化** の仕組みを理解する\n",
    "2. **Weight Decay** と L2正則化の違いを理解する\n",
    "3. **Dropout** の効果を理解する\n",
    "4. **Batch Normalization** の正則化効果を理解する\n",
    "5. 正則化と最適化の相互作用を学ぶ\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 110-113 の内容\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [正則化の目的](#1-正則化の目的)\n",
    "2. [L2正則化（Ridge）](#2-l2正則化ridge)\n",
    "3. [L1正則化（Lasso）](#3-l1正則化lasso)\n",
    "4. [Weight Decay vs L2正則化](#4-weight-decay-vs-l2正則化)\n",
    "5. [Elastic Net](#5-elastic-net)\n",
    "6. [Dropout](#6-dropout)\n",
    "7. [Batch Normalization](#7-batch-normalization)\n",
    "8. [演習問題](#8-演習問題)\n",
    "9. [まとめと次のステップ](#9-まとめと次のステップ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 正則化の目的\n",
    "\n",
    "### 1.1 過学習（Overfitting）\n",
    "\n",
    "モデルが訓練データに過度に適合し、新しいデータに対する性能が低下する現象。\n",
    "\n",
    "### 1.2 正則化の役割\n",
    "\n",
    "モデルの複雑さを制限することで、汎化性能を向上させます。\n",
    "\n",
    "$$\n",
    "L_{\\text{regularized}} = L_{\\text{data}} + \\lambda \\cdot R(\\theta)\n",
    "$$\n",
    "\n",
    "- $L_{\\text{data}}$: データに対する損失\n",
    "- $R(\\theta)$: 正則化項\n",
    "- $\\lambda$: 正則化の強さ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 過学習のデモンストレーション\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 真の関数: sin(x)\n",
    "def true_function(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "# データ生成\n",
    "n_samples = 15\n",
    "X_train = np.linspace(0, 2*np.pi, n_samples)\n",
    "y_train = true_function(X_train) + np.random.randn(n_samples) * 0.3\n",
    "\n",
    "# 多項式回帰\n",
    "def polynomial_features(x, degree):\n",
    "    return np.column_stack([x**i for i in range(degree + 1)])\n",
    "\n",
    "def fit_polynomial(X, y, degree):\n",
    "    X_poly = polynomial_features(X, degree)\n",
    "    # 正規方程式で解く\n",
    "    w = np.linalg.lstsq(X_poly, y, rcond=None)[0]\n",
    "    return w\n",
    "\n",
    "def predict_polynomial(x, w):\n",
    "    degree = len(w) - 1\n",
    "    X_poly = polynomial_features(x, degree)\n",
    "    return X_poly @ w\n",
    "\n",
    "\n",
    "# 異なる次数でフィット\n",
    "degrees = [1, 3, 12]\n",
    "X_test = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, degree in zip(axes, degrees):\n",
    "    w = fit_polynomial(X_train, y_train, degree)\n",
    "    y_pred = predict_polynomial(X_test, w)\n",
    "    \n",
    "    ax.scatter(X_train, y_train, color='blue', label='訓練データ', zorder=5)\n",
    "    ax.plot(X_test, true_function(X_test), 'g--', label='真の関数', linewidth=2)\n",
    "    ax.plot(X_test, y_pred, 'r-', label=f'予測 (次数={degree})', linewidth=2)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'多項式次数 = {degree}')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(\"- 次数1: アンダーフィット（モデルが単純すぎる）\")\n",
    "print(\"- 次数3: 適切なフィット\")\n",
    "print(\"- 次数12: オーバーフィット（訓練データに過度に適合）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. L2正則化（Ridge）\n",
    "\n",
    "### 2.1 定義\n",
    "\n",
    "$$\n",
    "L_{\\text{Ridge}} = L_{\\text{data}} + \\frac{\\lambda}{2} \\|\\theta\\|_2^2 = L_{\\text{data}} + \\frac{\\lambda}{2} \\sum_i \\theta_i^2\n",
    "$$\n",
    "\n",
    "### 2.2 効果\n",
    "\n",
    "- 重みを小さく保つ\n",
    "- 極端に大きな重みを防ぐ\n",
    "- 重みがゼロにはなりにくい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial_ridge(X, y, degree, lambda_reg):\n",
    "    \"\"\"L2正則化付き多項式回帰\"\"\"\n",
    "    X_poly = polynomial_features(X, degree)\n",
    "    n_features = X_poly.shape[1]\n",
    "    \n",
    "    # 正規方程式: (X^T X + λI)^-1 X^T y\n",
    "    identity = np.eye(n_features)\n",
    "    identity[0, 0] = 0  # バイアス項は正則化しない\n",
    "    \n",
    "    w = np.linalg.solve(X_poly.T @ X_poly + lambda_reg * identity, X_poly.T @ y)\n",
    "    return w\n",
    "\n",
    "\n",
    "# L2正則化の効果\n",
    "degree = 12\n",
    "lambdas = [0, 0.001, 0.1, 10]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, lambda_reg in zip(axes, lambdas):\n",
    "    w = fit_polynomial_ridge(X_train, y_train, degree, lambda_reg)\n",
    "    y_pred = predict_polynomial(X_test, w)\n",
    "    \n",
    "    ax.scatter(X_train, y_train, color='blue', zorder=5)\n",
    "    ax.plot(X_test, true_function(X_test), 'g--', linewidth=2)\n",
    "    ax.plot(X_test, y_pred, 'r-', linewidth=2)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'λ = {lambda_reg}')\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 重みの大きさを比較\n",
    "print(\"【重みの L2ノルム】\")\n",
    "for lambda_reg in lambdas:\n",
    "    w = fit_polynomial_ridge(X_train, y_train, degree, lambda_reg)\n",
    "    norm = np.linalg.norm(w)\n",
    "    print(f\"λ = {lambda_reg:5.3f}: ||w||₂ = {norm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 L2正則化の幾何学的理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2正則化の幾何学的解釈\n",
    "\n",
    "# データ損失（楕円）\n",
    "def data_loss(w1, w2):\n",
    "    return (w1 - 3)**2 + 0.5 * (w2 - 2)**2\n",
    "\n",
    "# L2正則化項（円）\n",
    "def l2_reg(w1, w2):\n",
    "    return w1**2 + w2**2\n",
    "\n",
    "w1 = np.linspace(-1, 5, 200)\n",
    "w2 = np.linspace(-1, 4, 200)\n",
    "W1, W2 = np.meshgrid(w1, w2)\n",
    "\n",
    "Z_data = data_loss(W1, W2)\n",
    "Z_l2 = l2_reg(W1, W2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# データ損失\n",
    "axes[0].contour(W1, W2, Z_data, levels=20, cmap='Blues')\n",
    "axes[0].scatter([3], [2], color='red', s=100, marker='*', label='最小点')\n",
    "axes[0].set_xlabel('w₁')\n",
    "axes[0].set_ylabel('w₂')\n",
    "axes[0].set_title('データ損失 L(w)')\n",
    "axes[0].legend()\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# L2正則化項\n",
    "axes[1].contour(W1, W2, Z_l2, levels=20, cmap='Greens')\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "for r in [1, 2, 3]:\n",
    "    axes[1].plot(r*np.cos(theta), r*np.sin(theta), 'g-', alpha=0.5)\n",
    "axes[1].scatter([0], [0], color='green', s=100, marker='*')\n",
    "axes[1].set_xlabel('w₁')\n",
    "axes[1].set_ylabel('w₂')\n",
    "axes[1].set_title('L2正則化 ||w||²')\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# 合計損失\n",
    "lambda_reg = 0.5\n",
    "Z_total = Z_data + lambda_reg * Z_l2\n",
    "\n",
    "# 最小点を数値的に求める\n",
    "min_idx = np.unravel_index(np.argmin(Z_total), Z_total.shape)\n",
    "w1_opt = w1[min_idx[1]]\n",
    "w2_opt = w2[min_idx[0]]\n",
    "\n",
    "axes[2].contour(W1, W2, Z_total, levels=20, cmap='Purples')\n",
    "axes[2].scatter([3], [2], color='blue', s=100, marker='*', label='正則化なし')\n",
    "axes[2].scatter([w1_opt], [w2_opt], color='red', s=100, marker='*', label='正則化あり')\n",
    "axes[2].set_xlabel('w₁')\n",
    "axes[2].set_ylabel('w₂')\n",
    "axes[2].set_title(f'合計損失 (λ={lambda_reg})')\n",
    "axes[2].legend()\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【幾何学的解釈】\")\n",
    "print(\"- L2正則化は原点に向かって重みを引き付ける\")\n",
    "print(f\"- 正則化なしの最適値: (3.0, 2.0)\")\n",
    "print(f\"- 正則化ありの最適値: ({w1_opt:.2f}, {w2_opt:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. L1正則化（Lasso）\n",
    "\n",
    "### 3.1 定義\n",
    "\n",
    "$$\n",
    "L_{\\text{Lasso}} = L_{\\text{data}} + \\lambda \\|\\theta\\|_1 = L_{\\text{data}} + \\lambda \\sum_i |\\theta_i|\n",
    "$$\n",
    "\n",
    "### 3.2 効果\n",
    "\n",
    "- **スパース性**: 多くの重みを正確にゼロにする\n",
    "- **特徴選択**: 重要でない特徴を自動的に削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1正則化の幾何学的解釈\n",
    "\n",
    "# L1正則化項（ダイヤモンド）\n",
    "def l1_reg(w1, w2):\n",
    "    return np.abs(w1) + np.abs(w2)\n",
    "\n",
    "Z_l1 = l1_reg(W1, W2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# L2 vs L1\n",
    "# L2\n",
    "axes[0].contour(W1, W2, Z_data, levels=10, cmap='Blues', alpha=0.5)\n",
    "for r in [0.5, 1, 1.5, 2, 2.5]:\n",
    "    axes[0].plot(r*np.cos(theta), r*np.sin(theta), 'g-', alpha=0.5)\n",
    "axes[0].scatter([3], [2], color='blue', s=100, marker='o', label='正則化なし')\n",
    "# 最適点（L2）\n",
    "axes[0].scatter([1.5], [1.3], color='red', s=100, marker='*', label='L2正則化')\n",
    "axes[0].set_xlabel('w₁')\n",
    "axes[0].set_ylabel('w₂')\n",
    "axes[0].set_title('L2正則化: 等高線は円')\n",
    "axes[0].set_xlim(-1, 4)\n",
    "axes[0].set_ylim(-1, 3)\n",
    "axes[0].legend()\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# L1\n",
    "axes[1].contour(W1, W2, Z_data, levels=10, cmap='Blues', alpha=0.5)\n",
    "# ダイヤモンドを描画\n",
    "for r in [0.5, 1, 1.5, 2, 2.5]:\n",
    "    diamond = np.array([[r, 0], [0, r], [-r, 0], [0, -r], [r, 0]])\n",
    "    axes[1].plot(diamond[:, 0], diamond[:, 1], 'g-', alpha=0.5)\n",
    "axes[1].scatter([3], [2], color='blue', s=100, marker='o', label='正則化なし')\n",
    "# L1の最適点（角に接触）\n",
    "axes[1].scatter([2], [0], color='red', s=100, marker='*', label='L1正則化')\n",
    "axes[1].set_xlabel('w₁')\n",
    "axes[1].set_ylabel('w₂')\n",
    "axes[1].set_title('L1正則化: 等高線はダイヤモンド')\n",
    "axes[1].set_xlim(-1, 4)\n",
    "axes[1].set_ylim(-1, 3)\n",
    "axes[1].legend()\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【L1 vs L2】\")\n",
    "print(\"- L2: 最適点は通常、軸上にない → 重みはゼロにならない\")\n",
    "print(\"- L1: 最適点は角（軸上）に接触しやすい → 重みがゼロになる（スパース）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1正則化による特徴選択のデモ\n",
    "\n",
    "def soft_threshold(x, threshold):\n",
    "    \"\"\"近接勾配法のL1に対する近接演算子\"\"\"\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "\n",
    "def lasso_coordinate_descent(X, y, lambda_reg, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"Lasso回帰（座標降下法）\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        w_old = w.copy()\n",
    "        \n",
    "        for j in range(n_features):\n",
    "            # 残差の計算（j番目の特徴を除く）\n",
    "            residual = y - X @ w + X[:, j] * w[j]\n",
    "            # j番目の重みを更新\n",
    "            rho = X[:, j] @ residual\n",
    "            w[j] = soft_threshold(rho, lambda_reg * n_samples) / (X[:, j] @ X[:, j])\n",
    "        \n",
    "        if np.linalg.norm(w - w_old) < tol:\n",
    "            break\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "# テスト: 高次元データでスパース性を確認\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 20\n",
    "n_informative = 5  # 実際に重要な特徴は5個だけ\n",
    "\n",
    "# データ生成\n",
    "X_sparse = np.random.randn(n_samples, n_features)\n",
    "true_w = np.zeros(n_features)\n",
    "true_w[:n_informative] = np.random.randn(n_informative) * 2  # 最初の5個だけ非ゼロ\n",
    "y_sparse = X_sparse @ true_w + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# L1正則化で回帰\n",
    "lambdas = [0.01, 0.1, 0.5]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, lambda_reg in zip(axes, lambdas):\n",
    "    w_lasso = lasso_coordinate_descent(X_sparse, y_sparse, lambda_reg)\n",
    "    \n",
    "    ax.bar(range(n_features), true_w, alpha=0.5, label='真の重み')\n",
    "    ax.bar(range(n_features), w_lasso, alpha=0.5, label='推定重み')\n",
    "    ax.set_xlabel('特徴インデックス')\n",
    "    ax.set_ylabel('重み')\n",
    "    ax.set_title(f'L1正則化 (λ={lambda_reg})\\n非ゼロ: {np.sum(np.abs(w_lasso) > 0.01)}/{n_features}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Weight Decay vs L2正則化\n",
    "\n",
    "### 4.1 SGDでは同等\n",
    "\n",
    "**L2正則化**:\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta (\\nabla L + \\lambda \\theta_t) = (1 - \\eta\\lambda)\\theta_t - \\eta \\nabla L\n",
    "$$\n",
    "\n",
    "**Weight Decay**:\n",
    "$$\n",
    "\\theta_{t+1} = (1 - \\eta\\lambda)\\theta_t - \\eta \\nabla L\n",
    "$$\n",
    "\n",
    "→ 同じ！\n",
    "\n",
    "### 4.2 Adamでは異なる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam + L2正則化 vs AdamW の違い\n",
    "\n",
    "print(\"【Adam + L2正則化】\")\n",
    "print(\"\")\n",
    "print(\"1. 勾配を計算: g = ∇L + λθ\")\n",
    "print(\"2. 1次モーメント: m = β₁m + (1-β₁)g\")\n",
    "print(\"3. 2次モーメント: v = β₂v + (1-β₂)g²\")\n",
    "print(\"4. 更新: θ = θ - η * m̂ / √v̂\")\n",
    "print(\"\")\n",
    "print(\"→ 正則化項 λθ も適応的にスケーリングされる\")\n",
    "print(\"→ 勾配の大きいパラメータは正則化効果が弱まる\")\n",
    "print(\"\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\")\n",
    "print(\"【AdamW (Decoupled Weight Decay)】\")\n",
    "print(\"\")\n",
    "print(\"1. 勾配を計算: g = ∇L（正則化なし）\")\n",
    "print(\"2. 1次モーメント: m = β₁m + (1-β₁)g\")\n",
    "print(\"3. 2次モーメント: v = β₂v + (1-β₂)g²\")\n",
    "print(\"4. 更新: θ = θ - η * m̂ / √v̂ - ηλθ\")\n",
    "print(\"\")\n",
    "print(\"→ Weight Decay は Adam の更新とは独立\")\n",
    "print(\"→ すべてのパラメータに均一に正則化が適用される\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡単なシミュレーションで違いを確認\n",
    "\n",
    "def adam_l2(grad_fn, start, lr=0.1, lambda_reg=0.1, beta1=0.9, beta2=0.999, n_steps=100):\n",
    "    \"\"\"Adam + L2正則化\"\"\"\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    m = np.zeros_like(pos)\n",
    "    v = np.zeros_like(pos)\n",
    "    eps = 1e-8\n",
    "    \n",
    "    for t in range(1, n_steps + 1):\n",
    "        grad = grad_fn(pos) + lambda_reg * pos  # L2正則化を勾配に含める\n",
    "        \n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * grad ** 2\n",
    "        \n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        pos = pos - lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "def adamw(grad_fn, start, lr=0.1, lambda_reg=0.1, beta1=0.9, beta2=0.999, n_steps=100):\n",
    "    \"\"\"AdamW (Decoupled Weight Decay)\"\"\"\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    m = np.zeros_like(pos)\n",
    "    v = np.zeros_like(pos)\n",
    "    eps = 1e-8\n",
    "    \n",
    "    for t in range(1, n_steps + 1):\n",
    "        grad = grad_fn(pos)  # 正則化なしの勾配\n",
    "        \n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * grad ** 2\n",
    "        \n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        pos = pos - lr * m_hat / (np.sqrt(v_hat) + eps) - lr * lambda_reg * pos  # Weight Decay を別途適用\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "# テスト関数\n",
    "def test_loss_grad(pos):\n",
    "    return np.array([2*pos[0], 20*pos[1]])  # 異方性のある勾配\n",
    "\n",
    "\n",
    "start = np.array([5.0, 5.0])\n",
    "path_adam_l2 = adam_l2(test_loss_grad, start, lr=0.1, lambda_reg=0.1, n_steps=100)\n",
    "path_adamw = adamw(test_loss_grad, start, lr=0.1, lambda_reg=0.1, n_steps=100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# パラメータの推移\n",
    "axes[0].plot(path_adam_l2[:, 0], label='Adam+L2 (w₁)')\n",
    "axes[0].plot(path_adam_l2[:, 1], label='Adam+L2 (w₂)')\n",
    "axes[0].plot(path_adamw[:, 0], '--', label='AdamW (w₁)')\n",
    "axes[0].plot(path_adamw[:, 1], '--', label='AdamW (w₂)')\n",
    "axes[0].set_xlabel('ステップ')\n",
    "axes[0].set_ylabel('パラメータ値')\n",
    "axes[0].set_title('Adam+L2 vs AdamW')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# パラメータ空間での軌跡\n",
    "axes[1].plot(path_adam_l2[:, 0], path_adam_l2[:, 1], 'b.-', label='Adam+L2')\n",
    "axes[1].plot(path_adamw[:, 0], path_adamw[:, 1], 'r.-', label='AdamW')\n",
    "axes[1].scatter([0], [0], color='green', s=100, marker='*', zorder=5)\n",
    "axes[1].set_xlabel('w₁')\n",
    "axes[1].set_ylabel('w₂')\n",
    "axes[1].set_title('パラメータ空間での軌跡')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"【最終パラメータ】\")\n",
    "print(f\"Adam+L2: ({path_adam_l2[-1, 0]:.4f}, {path_adam_l2[-1, 1]:.4f})\")\n",
    "print(f\"AdamW:   ({path_adamw[-1, 0]:.4f}, {path_adamw[-1, 1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Elastic Net\n",
    "\n",
    "### 5.1 定義\n",
    "\n",
    "L1とL2を組み合わせた正則化：\n",
    "\n",
    "$$\n",
    "L_{\\text{Elastic}} = L_{\\text{data}} + \\lambda_1 \\|\\theta\\|_1 + \\lambda_2 \\|\\theta\\|_2^2\n",
    "$$\n",
    "\n",
    "または、混合比率 $\\alpha$ を使用：\n",
    "\n",
    "$$\n",
    "L_{\\text{Elastic}} = L_{\\text{data}} + \\lambda \\left( \\alpha \\|\\theta\\|_1 + \\frac{1-\\alpha}{2} \\|\\theta\\|_2^2 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Netの等高線\n",
    "\n",
    "def elastic_net_penalty(w1, w2, alpha=0.5):\n",
    "    l1 = np.abs(w1) + np.abs(w2)\n",
    "    l2 = w1**2 + w2**2\n",
    "    return alpha * l1 + (1 - alpha) * l2 / 2\n",
    "\n",
    "alphas = [0, 0.5, 1]\n",
    "titles = ['L2のみ (α=0)', 'Elastic Net (α=0.5)', 'L1のみ (α=1)']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "w = np.linspace(-2, 2, 200)\n",
    "W1, W2 = np.meshgrid(w, w)\n",
    "\n",
    "for ax, alpha, title in zip(axes, alphas, titles):\n",
    "    Z = elastic_net_penalty(W1, W2, alpha)\n",
    "    ax.contour(W1, W2, Z, levels=20, cmap='viridis')\n",
    "    ax.set_xlabel('w₁')\n",
    "    ax.set_ylabel('w₂')\n",
    "    ax.set_title(title)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【Elastic Netの利点】\")\n",
    "print(\"- L1のスパース性を維持しつつ\")\n",
    "print(\"- L2により相関する特徴をグループとして選択\")\n",
    "print(\"- 特徴数 > サンプル数の場合でも安定\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Dropout\n",
    "\n",
    "### 6.1 アイデア\n",
    "\n",
    "訓練中にランダムにニューロンを無効化（出力を0に）します。\n",
    "\n",
    "- **訓練時**: 確率 $p$ でニューロンをドロップ\n",
    "- **推論時**: すべてのニューロンを使用（出力を $(1-p)$ でスケール、または訓練時に $1/(1-p)$ でスケール）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    Dropout Layer\n",
    "    \n",
    "    訓練時: p の確率でニューロンを無効化、残りを 1/(1-p) でスケール\n",
    "    推論時: 何もしない（Inverted Dropout）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.mask = (np.random.rand(*x.shape) > self.p).astype(float)\n",
    "            return x * self.mask / (1 - self.p)  # Inverted Dropout\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        if self.training:\n",
    "            return dout * self.mask / (1 - self.p)\n",
    "        else:\n",
    "            return dout\n",
    "\n",
    "\n",
    "# Dropoutの可視化\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(1, 10)  # 10個のニューロン\n",
    "\n",
    "dropout = Dropout(p=0.5)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 元の活性化\n",
    "axes[0].bar(range(10), x.flatten())\n",
    "axes[0].set_xlabel('ニューロン')\n",
    "axes[0].set_ylabel('活性化')\n",
    "axes[0].set_title('元の活性化')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dropout後（訓練時）\n",
    "np.random.seed(0)\n",
    "dropout.training = True\n",
    "x_dropped = dropout.forward(x)\n",
    "colors = ['red' if dropout.mask[0, i] == 0 else 'blue' for i in range(10)]\n",
    "axes[1].bar(range(10), x_dropped.flatten(), color=colors)\n",
    "axes[1].set_xlabel('ニューロン')\n",
    "axes[1].set_ylabel('活性化')\n",
    "axes[1].set_title('Dropout後（訓練時）\\n赤=ドロップ')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 推論時\n",
    "dropout.training = False\n",
    "x_inference = dropout.forward(x)\n",
    "axes[2].bar(range(10), x_inference.flatten())\n",
    "axes[2].set_xlabel('ニューロン')\n",
    "axes[2].set_ylabel('活性化')\n",
    "axes[2].set_title('推論時')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ドロップされたニューロン数: {np.sum(dropout.mask == 0)}\")\n",
    "print(f\"元の活性化の平均: {x.mean():.4f}\")\n",
    "print(f\"Dropout後の活性化の平均: {x_dropped[dropout.mask > 0].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Dropoutの正則化効果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"【Dropoutの効果】\")\n",
    "print(\"\")\n",
    "print(\"1. 共適応（Co-adaptation）の防止\")\n",
    "print(\"   - ニューロンが他の特定のニューロンに依存することを防ぐ\")\n",
    "print(\"   - より頑健な特徴を学習\")\n",
    "print(\"\")\n",
    "print(\"2. アンサンブル効果\")\n",
    "print(\"   - 各ミニバッチで異なるサブネットワークを訓練\")\n",
    "print(\"   - 推論時は全サブネットワークの平均化に相当\")\n",
    "print(\"\")\n",
    "print(\"3. ノイズ注入\")\n",
    "print(\"   - 訓練データへの過度な適合を防ぐ\")\n",
    "print(\"   - データ拡張と似た効果\")\n",
    "print(\"\")\n",
    "print(\"【典型的な使用法】\")\n",
    "print(\"- 全結合層: p = 0.5\")\n",
    "print(\"- 畳み込み層: p = 0.2-0.3（または使用しない）\")\n",
    "print(\"- 最終層の直前に配置することが多い\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Batch Normalization\n",
    "\n",
    "### 7.1 定義\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "\n",
    "- $\\mu_B, \\sigma_B^2$: ミニバッチの平均と分散\n",
    "- $\\gamma, \\beta$: 学習可能なパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "    \"\"\"\n",
    "    Batch Normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # 学習可能パラメータ\n",
    "        self.gamma = np.ones(n_features)\n",
    "        self.beta = np.zeros(n_features)\n",
    "        \n",
    "        # 推論用の移動平均\n",
    "        self.running_mean = np.zeros(n_features)\n",
    "        self.running_var = np.ones(n_features)\n",
    "        \n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mu = x.mean(axis=0)\n",
    "            var = x.var(axis=0)\n",
    "            \n",
    "            # 移動平均の更新\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mu\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "        else:\n",
    "            mu = self.running_mean\n",
    "            var = self.running_var\n",
    "        \n",
    "        # 正規化\n",
    "        self.x_norm = (x - mu) / np.sqrt(var + self.eps)\n",
    "        \n",
    "        # スケールとシフト\n",
    "        return self.gamma * self.x_norm + self.beta\n",
    "\n",
    "\n",
    "# Batch Normの効果を可視化\n",
    "np.random.seed(42)\n",
    "\n",
    "# 異なるスケールの特徴を持つデータ\n",
    "x = np.column_stack([\n",
    "    np.random.randn(100) * 10 + 50,  # 大きなスケール\n",
    "    np.random.randn(100) * 0.1 + 0.5,  # 小さなスケール\n",
    "])\n",
    "\n",
    "bn = BatchNorm(2)\n",
    "x_normalized = bn.forward(x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 正規化前\n",
    "axes[0].scatter(x[:, 0], x[:, 1], alpha=0.5)\n",
    "axes[0].set_xlabel('特徴1')\n",
    "axes[0].set_ylabel('特徴2')\n",
    "axes[0].set_title(f'正規化前\\n特徴1: μ={x[:, 0].mean():.1f}, σ={x[:, 0].std():.1f}\\n特徴2: μ={x[:, 1].mean():.2f}, σ={x[:, 1].std():.2f}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 正規化後\n",
    "axes[1].scatter(x_normalized[:, 0], x_normalized[:, 1], alpha=0.5)\n",
    "axes[1].set_xlabel('特徴1')\n",
    "axes[1].set_ylabel('特徴2')\n",
    "axes[1].set_title(f'正規化後\\n特徴1: μ={x_normalized[:, 0].mean():.4f}, σ={x_normalized[:, 0].std():.4f}\\n特徴2: μ={x_normalized[:, 1].mean():.4f}, σ={x_normalized[:, 1].std():.4f}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"【Batch Normalizationの効果】\")\n",
    "print(\"\")\n",
    "print(\"1. Internal Covariate Shift の軽減\")\n",
    "print(\"   - 各層の入力分布を安定化\")\n",
    "print(\"   - 学習の高速化\")\n",
    "print(\"\")\n",
    "print(\"2. 正則化効果\")\n",
    "print(\"   - ミニバッチごとに異なる正規化パラメータ\")\n",
    "print(\"   - ノイズ注入と同様の効果\")\n",
    "print(\"\")\n",
    "print(\"3. より大きな学習率の使用が可能\")\n",
    "print(\"   - 勾配が安定するため\")\n",
    "print(\"\")\n",
    "print(\"4. 初期化への依存が減少\")\n",
    "print(\"   - 入力が正規化されるため\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 演習問題\n",
    "\n",
    "### 演習 8.1: L1正則化の近接勾配法\n",
    "\n",
    "近接勾配法（Proximal Gradient Method）を使ってL1正則化付き回帰を実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.1: 解答欄\n",
    "\n",
    "def proximal_gradient_l1(X, y, lambda_reg, lr=0.01, max_iter=1000):\n",
    "    \"\"\"\n",
    "    近接勾配法によるLasso回帰\n",
    "    \n",
    "    1. 勾配ステップ: z = w - lr * ∇L(w)\n",
    "    2. 近接演算子: w = prox_{λ||·||₁}(z) = soft_threshold(z, lr*λ)\n",
    "    \"\"\"\n",
    "    # TODO: 実装\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 8.2: Dropout の勾配実装\n",
    "\n",
    "Dropout層の backward メソッドを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.2: 解答欄\n",
    "\n",
    "# TODO: Dropout.backward() をテスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **L2正則化**: 重みを小さく保つ、滑らかな制約\n",
    "\n",
    "2. **L1正則化**: スパースな解を生成、特徴選択\n",
    "\n",
    "3. **Weight Decay vs L2**:\n",
    "   - SGDでは同等\n",
    "   - Adamでは異なる（AdamWが推奨）\n",
    "\n",
    "4. **Dropout**: ランダムなニューロン無効化による正則化\n",
    "\n",
    "5. **Batch Normalization**: 分布の正規化と正則化効果\n",
    "\n",
    "### 次のノートブック（115: 高度な最適化テクニック）への橋渡し\n",
    "\n",
    "より高度な最適化テクニック：\n",
    "\n",
    "- Gradient Clipping\n",
    "- SAM（Sharpness-Aware Minimization）\n",
    "- LARS/LAMB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Srivastava, N., et al. (2014). Dropout: A simple way to prevent neural networks from overfitting. *JMLR*.\n",
    "2. Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training. *ICML*.\n",
    "3. Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. *ICLR*.\n",
    "4. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. *JRSS*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
