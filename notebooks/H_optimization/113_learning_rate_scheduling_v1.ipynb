{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 113: 学習率スケジューリング\n",
    "\n",
    "## Learning Rate Scheduling\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Phase 10「最適化手法」** の第4章として、学習の進行に応じて学習率を調整する **スケジューリング** 手法を学びます。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **Step Decay** の仕組みを理解する\n",
    "2. **Exponential Decay** を実装する\n",
    "3. **Cosine Annealing** を理解する\n",
    "4. **Warmup** の重要性を理解する\n",
    "5. **Cyclical Learning Rate** を学ぶ\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 110-112 の内容\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [学習率スケジューリングの必要性](#1-学習率スケジューリングの必要性)\n",
    "2. [Step Decay](#2-step-decay)\n",
    "3. [Exponential Decay](#3-exponential-decay)\n",
    "4. [Cosine Annealing](#4-cosine-annealing)\n",
    "5. [Warmup](#5-warmup)\n",
    "6. [Cyclical Learning Rate](#6-cyclical-learning-rate)\n",
    "7. [One Cycle Policy](#7-one-cycle-policy)\n",
    "8. [スケジューラの比較](#8-スケジューラの比較)\n",
    "9. [演習問題](#9-演習問題)\n",
    "10. [まとめと次のステップ](#10-まとめと次のステップ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 学習率スケジューリングの必要性\n",
    "\n",
    "### 1.1 固定学習率の問題\n",
    "\n",
    "- **学習初期**: 大きな学習率で素早く探索したい\n",
    "- **学習後期**: 小さな学習率で精密に収束したい\n",
    "\n",
    "固定学習率では両立が難しい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定学習率の問題を可視化\n",
    "\n",
    "def noisy_quadratic(x, noise_scale=0.5):\n",
    "    \"\"\"ノイズ付き二次関数\"\"\"\n",
    "    return (x - 2)**2 + noise_scale * np.random.randn()\n",
    "\n",
    "def noisy_quadratic_grad(x, noise_scale=0.5):\n",
    "    \"\"\"ノイズ付き勾配\"\"\"\n",
    "    return 2 * (x - 2) + noise_scale * np.random.randn()\n",
    "\n",
    "\n",
    "# 異なる固定学習率での学習\n",
    "lrs = [0.01, 0.1, 0.5]\n",
    "n_steps = 200\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, lr in zip(axes, lrs):\n",
    "    np.random.seed(42)\n",
    "    x = 0.0\n",
    "    history = [x]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = noisy_quadratic_grad(x)\n",
    "        x = x - lr * grad\n",
    "        history.append(x)\n",
    "    \n",
    "    ax.plot(history)\n",
    "    ax.axhline(y=2, color='red', linestyle='--', label='最適値')\n",
    "    ax.set_xlabel('ステップ')\n",
    "    ax.set_ylabel('x')\n",
    "    ax.set_title(f'学習率 = {lr}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(\"- 小さい学習率(0.01): 収束が遅い\")\n",
    "print(\"- 中間の学習率(0.1): バランスが良い\")\n",
    "print(\"- 大きい学習率(0.5): 収束後も振動\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Step Decay\n",
    "\n",
    "### 2.1 アイデア\n",
    "\n",
    "一定のエポック数ごとに学習率を減少させます。\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t / S \\rfloor}\n",
    "$$\n",
    "\n",
    "- $\\eta_0$: 初期学習率\n",
    "- $\\gamma$: 減衰率（例: 0.1）\n",
    "- $S$: ステップサイズ（例: 30 epochs）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepLR:\n",
    "    \"\"\"\n",
    "    Step Decay Learning Rate Scheduler\n",
    "    \n",
    "    η = η₀ * γ^(epoch // step_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr, step_size, gamma=0.1):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        return self.initial_lr * (self.gamma ** (epoch // self.step_size))\n",
    "\n",
    "\n",
    "# Step Decayの可視化\n",
    "scheduler = StepLR(initial_lr=0.1, step_size=30, gamma=0.1)\n",
    "epochs = np.arange(100)\n",
    "lrs = [scheduler.get_lr(e) for e in epochs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(epochs, lrs, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('学習率')\n",
    "ax.set_title('Step Decay (step_size=30, γ=0.1)')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ステップ位置をマーク\n",
    "for s in [30, 60, 90]:\n",
    "    ax.axvline(x=s, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【Step Decay】\")\n",
    "print(f\"Epoch 0-29:  lr = {scheduler.get_lr(0)}\")\n",
    "print(f\"Epoch 30-59: lr = {scheduler.get_lr(30)}\")\n",
    "print(f\"Epoch 60-89: lr = {scheduler.get_lr(60)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MultiStepLR\n",
    "\n",
    "指定したエポックで減衰させるバリエーション。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepLR:\n",
    "    \"\"\"\n",
    "    Multi-Step Learning Rate Scheduler\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr, milestones, gamma=0.1):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.milestones = sorted(milestones)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        lr = self.initial_lr\n",
    "        for milestone in self.milestones:\n",
    "            if epoch >= milestone:\n",
    "                lr *= self.gamma\n",
    "        return lr\n",
    "\n",
    "\n",
    "# MultiStepLRの可視化\n",
    "scheduler = MultiStepLR(initial_lr=0.1, milestones=[30, 60, 80], gamma=0.1)\n",
    "epochs = np.arange(100)\n",
    "lrs = [scheduler.get_lr(e) for e in epochs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(epochs, lrs, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('学習率')\n",
    "ax.set_title('MultiStepLR (milestones=[30, 60, 80])')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for s in [30, 60, 80]:\n",
    "    ax.axvline(x=s, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exponential Decay\n",
    "\n",
    "### 3.1 アイデア\n",
    "\n",
    "各エポックで学習率を指数的に減衰させます。\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_0 \\cdot \\gamma^t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialLR:\n",
    "    \"\"\"\n",
    "    Exponential Decay Learning Rate Scheduler\n",
    "    \n",
    "    η = η₀ * γ^epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr, gamma=0.95):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        return self.initial_lr * (self.gamma ** epoch)\n",
    "\n",
    "\n",
    "# 可視化\n",
    "gammas = [0.9, 0.95, 0.99]\n",
    "epochs = np.arange(100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for gamma in gammas:\n",
    "    scheduler = ExponentialLR(initial_lr=0.1, gamma=gamma)\n",
    "    lrs = [scheduler.get_lr(e) for e in epochs]\n",
    "    ax.plot(epochs, lrs, linewidth=2, label=f'γ = {gamma}')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('学習率')\n",
    "ax.set_title('Exponential Decay')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Cosine Annealing\n",
    "\n",
    "### 4.1 アイデア\n",
    "\n",
    "学習率をコサイン関数に従って滑らかに減衰させます。\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})\\left(1 + \\cos\\left(\\frac{t}{T_{\\max}}\\pi\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingLR:\n",
    "    \"\"\"\n",
    "    Cosine Annealing Learning Rate Scheduler\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr, T_max, eta_min=0):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        return self.eta_min + 0.5 * (self.initial_lr - self.eta_min) * \\\n",
    "               (1 + np.cos(np.pi * epoch / self.T_max))\n",
    "\n",
    "\n",
    "# Cosine Annealingの可視化\n",
    "scheduler = CosineAnnealingLR(initial_lr=0.1, T_max=100, eta_min=0.001)\n",
    "epochs = np.arange(100)\n",
    "lrs = [scheduler.get_lr(e) for e in epochs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(epochs, lrs, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('学習率')\n",
    "ax.set_title('Cosine Annealing (T_max=100)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【Cosine Annealingの利点】\")\n",
    "print(\"- 滑らかな減衰により学習が安定\")\n",
    "print(\"- Step Decayのような急激な変化がない\")\n",
    "print(\"- 学習終盤で精密な収束が可能\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Cosine Annealing with Warm Restarts\n",
    "\n",
    "周期的に学習率をリセットするバリエーション。局所解からの脱出に効果的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmRestarts:\n",
    "    \"\"\"\n",
    "    Cosine Annealing with Warm Restarts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr, T_0, T_mult=1, eta_min=0):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        # 現在のサイクルと、サイクル内での位置を計算\n",
    "        if self.T_mult == 1:\n",
    "            T_cur = epoch % self.T_0\n",
    "            T_i = self.T_0\n",
    "        else:\n",
    "            # 幾何級数の場合\n",
    "            n = 0\n",
    "            T_sum = 0\n",
    "            while T_sum + self.T_0 * (self.T_mult ** n) <= epoch:\n",
    "                T_sum += self.T_0 * (self.T_mult ** n)\n",
    "                n += 1\n",
    "            T_i = self.T_0 * (self.T_mult ** n)\n",
    "            T_cur = epoch - T_sum\n",
    "        \n",
    "        return self.eta_min + 0.5 * (self.initial_lr - self.eta_min) * \\\n",
    "               (1 + np.cos(np.pi * T_cur / T_i))\n",
    "\n",
    "\n",
    "# Warm Restartsの可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# T_mult = 1 (固定周期)\n",
    "scheduler1 = CosineAnnealingWarmRestarts(initial_lr=0.1, T_0=20, T_mult=1, eta_min=0.001)\n",
    "epochs = np.arange(100)\n",
    "lrs1 = [scheduler1.get_lr(e) for e in epochs]\n",
    "\n",
    "axes[0].plot(epochs, lrs1, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('学習率')\n",
    "axes[0].set_title('Warm Restarts (T_0=20, T_mult=1)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# T_mult = 2 (周期が倍増)\n",
    "scheduler2 = CosineAnnealingWarmRestarts(initial_lr=0.1, T_0=10, T_mult=2, eta_min=0.001)\n",
    "lrs2 = [scheduler2.get_lr(e) for e in epochs]\n",
    "\n",
    "axes[1].plot(epochs, lrs2, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('学習率')\n",
    "axes[1].set_title('Warm Restarts (T_0=10, T_mult=2)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Warmup\n",
    "\n",
    "### 5.1 アイデア\n",
    "\n",
    "学習初期は小さな学習率から始め、徐々に目標の学習率まで上げていきます。\n",
    "\n",
    "**なぜ必要か**:\n",
    "- 初期の勾配は不安定（重みがランダム初期化されている）\n",
    "- 大きな学習率で始めると発散するリスク\n",
    "- 特に大規模モデル（Transformer等）で重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWarmup:\n",
    "    \"\"\"\n",
    "    Linear Warmup Scheduler\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_lr, warmup_epochs):\n",
    "        self.target_lr = target_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return self.target_lr * (epoch + 1) / self.warmup_epochs\n",
    "        return self.target_lr\n",
    "\n",
    "\n",
    "class WarmupCosineAnnealing:\n",
    "    \"\"\"\n",
    "    Warmup + Cosine Annealing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_lr, warmup_epochs, total_epochs, eta_min=0):\n",
    "        self.target_lr = target_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.eta_min = eta_min\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            return self.target_lr * (epoch + 1) / self.warmup_epochs\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
    "            return self.eta_min + 0.5 * (self.target_lr - self.eta_min) * \\\n",
    "                   (1 + np.cos(np.pi * progress))\n",
    "\n",
    "\n",
    "# Warmupの可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear Warmup only\n",
    "scheduler1 = LinearWarmup(target_lr=0.1, warmup_epochs=10)\n",
    "epochs = np.arange(100)\n",
    "lrs1 = [scheduler1.get_lr(e) for e in epochs]\n",
    "\n",
    "axes[0].plot(epochs, lrs1, 'b-', linewidth=2)\n",
    "axes[0].axvline(x=10, color='red', linestyle='--', alpha=0.5, label='Warmup終了')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('学習率')\n",
    "axes[0].set_title('Linear Warmup (warmup=10)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Warmup + Cosine\n",
    "scheduler2 = WarmupCosineAnnealing(target_lr=0.1, warmup_epochs=10, total_epochs=100, eta_min=0.001)\n",
    "lrs2 = [scheduler2.get_lr(e) for e in epochs]\n",
    "\n",
    "axes[1].plot(epochs, lrs2, 'b-', linewidth=2)\n",
    "axes[1].axvline(x=10, color='red', linestyle='--', alpha=0.5, label='Warmup終了')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('学習率')\n",
    "axes[1].set_title('Warmup + Cosine Annealing')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【Warmupの効果】\")\n",
    "print(\"- 初期の不安定な勾配による発散を防ぐ\")\n",
    "print(\"- 大規模モデル（BERT, ViT等）では必須\")\n",
    "print(\"- 一般的に全体の5-10%程度のエポック\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Cyclical Learning Rate\n",
    "\n",
    "### 6.1 アイデア\n",
    "\n",
    "学習率を周期的に変動させることで、局所解からの脱出を促進します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicalLR:\n",
    "    \"\"\"\n",
    "    Cyclical Learning Rate (triangular policy)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_lr, max_lr, step_size):\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        cycle = np.floor(1 + epoch / (2 * self.step_size))\n",
    "        x = np.abs(epoch / self.step_size - 2 * cycle + 1)\n",
    "        return self.base_lr + (self.max_lr - self.base_lr) * max(0, 1 - x)\n",
    "\n",
    "\n",
    "class CyclicalLR2:\n",
    "    \"\"\"\n",
    "    Cyclical Learning Rate (triangular2 policy)\n",
    "    最大学習率が各サイクルで半減\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_lr, max_lr, step_size):\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        cycle = np.floor(1 + epoch / (2 * self.step_size))\n",
    "        x = np.abs(epoch / self.step_size - 2 * cycle + 1)\n",
    "        # 各サイクルで最大値を半減\n",
    "        amplitude = (self.max_lr - self.base_lr) / (2 ** (cycle - 1))\n",
    "        return self.base_lr + amplitude * max(0, 1 - x)\n",
    "\n",
    "\n",
    "# Cyclical LRの可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = np.arange(100)\n",
    "\n",
    "# Triangular\n",
    "scheduler1 = CyclicalLR(base_lr=0.001, max_lr=0.1, step_size=20)\n",
    "lrs1 = [scheduler1.get_lr(e) for e in epochs]\n",
    "\n",
    "axes[0].plot(epochs, lrs1, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('学習率')\n",
    "axes[0].set_title('Cyclical LR (triangular)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Triangular2\n",
    "scheduler2 = CyclicalLR2(base_lr=0.001, max_lr=0.1, step_size=20)\n",
    "lrs2 = [scheduler2.get_lr(e) for e in epochs]\n",
    "\n",
    "axes[1].plot(epochs, lrs2, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('学習率')\n",
    "axes[1].set_title('Cyclical LR (triangular2)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. One Cycle Policy\n",
    "\n",
    "### 7.1 アイデア\n",
    "\n",
    "Leslie Smith提案の手法。学習全体を通じて1サイクルだけ学習率を変動させます。\n",
    "\n",
    "1. 小さな学習率から始める\n",
    "2. 最大学習率まで増加\n",
    "3. 最小学習率まで減少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleLR:\n",
    "    \"\"\"\n",
    "    One Cycle Learning Rate Policy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_lr, total_epochs, pct_start=0.3, div_factor=25, final_div_factor=1e4):\n",
    "        self.max_lr = max_lr\n",
    "        self.total_epochs = total_epochs\n",
    "        self.pct_start = pct_start\n",
    "        self.div_factor = div_factor\n",
    "        self.final_div_factor = final_div_factor\n",
    "        \n",
    "        self.initial_lr = max_lr / div_factor\n",
    "        self.min_lr = max_lr / final_div_factor\n",
    "        self.up_epochs = int(total_epochs * pct_start)\n",
    "        self.down_epochs = total_epochs - self.up_epochs\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        if epoch < self.up_epochs:\n",
    "            # 上昇フェーズ\n",
    "            progress = epoch / self.up_epochs\n",
    "            return self.initial_lr + (self.max_lr - self.initial_lr) * progress\n",
    "        else:\n",
    "            # 下降フェーズ（コサイン減衰）\n",
    "            progress = (epoch - self.up_epochs) / self.down_epochs\n",
    "            return self.min_lr + 0.5 * (self.max_lr - self.min_lr) * \\\n",
    "                   (1 + np.cos(np.pi * progress))\n",
    "\n",
    "\n",
    "# One Cycle Policyの可視化\n",
    "scheduler = OneCycleLR(max_lr=0.1, total_epochs=100, pct_start=0.3)\n",
    "epochs = np.arange(100)\n",
    "lrs = [scheduler.get_lr(e) for e in epochs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(epochs, lrs, 'b-', linewidth=2)\n",
    "ax.axvline(x=30, color='red', linestyle='--', alpha=0.5, label='ピーク (30%)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('学習率')\n",
    "ax.set_title('One Cycle Policy (pct_start=0.3)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【One Cycle Policy】\")\n",
    "print(f\"初期学習率: {scheduler.initial_lr:.6f}\")\n",
    "print(f\"最大学習率: {scheduler.max_lr}\")\n",
    "print(f\"最小学習率: {scheduler.min_lr:.8f}\")\n",
    "print(\"\")\n",
    "print(\"Super-Convergence: 従来より少ないエポックで同等以上の精度を達成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. スケジューラの比較\n",
    "\n",
    "### 8.1 全スケジューラの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全スケジューラの比較\n",
    "\n",
    "schedulers = {\n",
    "    'Step Decay': StepLR(0.1, 30, 0.1),\n",
    "    'Exponential': ExponentialLR(0.1, 0.95),\n",
    "    'Cosine': CosineAnnealingLR(0.1, 100, 0.001),\n",
    "    'Warmup+Cosine': WarmupCosineAnnealing(0.1, 10, 100, 0.001),\n",
    "    'Cyclical': CyclicalLR(0.001, 0.1, 20),\n",
    "    'One Cycle': OneCycleLR(0.1, 100, 0.3),\n",
    "}\n",
    "\n",
    "epochs = np.arange(100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for name, scheduler in schedulers.items():\n",
    "    lrs = [scheduler.get_lr(e) for e in epochs]\n",
    "    ax.plot(epochs, lrs, linewidth=2, label=name)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('学習率')\n",
    "ax.set_title('学習率スケジューラの比較')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 スケジューラ選択ガイド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"学習率スケジューラ選択ガイド\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"【Step Decay】\")\n",
    "print(\"  用途: 画像分類（ResNet等の古典的手法）\")\n",
    "print(\"  特徴: シンプルで実績がある\")\n",
    "print(\"  典型: lr=0.1, milestones=[30,60,90], γ=0.1\")\n",
    "print(\"\")\n",
    "print(\"【Cosine Annealing】\")\n",
    "print(\"  用途: 汎用的、特にファインチューニング\")\n",
    "print(\"  特徴: 滑らかな減衰、チューニングが容易\")\n",
    "print(\"  典型: T_max=total_epochs, eta_min=0 or 1e-6\")\n",
    "print(\"\")\n",
    "print(\"【Warmup + Cosine】\")\n",
    "print(\"  用途: Transformer, ViT等の大規模モデル\")\n",
    "print(\"  特徴: 初期の不安定性を回避\")\n",
    "print(\"  典型: warmup=5-10%のエポック\")\n",
    "print(\"\")\n",
    "print(\"【One Cycle】\")\n",
    "print(\"  用途: 高速な学習（Super-Convergence）\")\n",
    "print(\"  特徴: 少ないエポックで高精度\")\n",
    "print(\"  典型: pct_start=0.3, div_factor=25\")\n",
    "print(\"\")\n",
    "print(\"【Cyclical LR】\")\n",
    "print(\"  用途: 局所解からの脱出が必要な場合\")\n",
    "print(\"  特徴: 最適な学習率範囲の発見にも使用\")\n",
    "print(\"  典型: LR Range Testで範囲を決定\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 演習問題\n",
    "\n",
    "### 演習 9.1: Polynomial Decay の実装\n",
    "\n",
    "多項式減衰スケジューラを実装してください。\n",
    "\n",
    "$$\n",
    "\\eta_t = (\\eta_0 - \\eta_{\\min}) \\left(1 - \\frac{t}{T}\\right)^p + \\eta_{\\min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 9.1: 解答欄\n",
    "\n",
    "class PolynomialLR:\n",
    "    def __init__(self, initial_lr, total_epochs, power=1.0, end_lr=0.0):\n",
    "        # TODO: 実装\n",
    "        pass\n",
    "    \n",
    "    def get_lr(self, epoch):\n",
    "        # TODO: 実装\n",
    "        pass\n",
    "\n",
    "# TODO: p=1, p=2, p=0.5 を比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 9.2: LR Range Test\n",
    "\n",
    "学習率を指数的に増加させながら損失を記録し、最適な学習率範囲を見つけるテストを実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 9.2: 解答欄\n",
    "\n",
    "def lr_range_test(model_fn, grad_fn, start_lr=1e-7, end_lr=10, num_steps=100):\n",
    "    \"\"\"\n",
    "    LR Range Test\n",
    "    \n",
    "    Args:\n",
    "        model_fn: 損失関数\n",
    "        grad_fn: 勾配関数\n",
    "        start_lr: 開始学習率\n",
    "        end_lr: 終了学習率\n",
    "        num_steps: ステップ数\n",
    "    \n",
    "    Returns:\n",
    "        lrs: 学習率のリスト\n",
    "        losses: 損失のリスト\n",
    "    \"\"\"\n",
    "    # TODO: 実装\n",
    "    pass\n",
    "\n",
    "# TODO: テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 9.3: カスタムスケジューラ\n",
    "\n",
    "2つのスケジューラを組み合わせたカスタムスケジューラを実装してください。\n",
    "例: 最初の50%はLinear Warmup、残りの50%はExponential Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 9.3: 解答欄\n",
    "\n",
    "class ChainedScheduler:\n",
    "    # TODO: 実装\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **Step Decay**: 一定間隔で学習率を減衰\n",
    "\n",
    "2. **Exponential Decay**: 毎エポック指数的に減衰\n",
    "\n",
    "3. **Cosine Annealing**: 滑らかなコサイン減衰\n",
    "\n",
    "4. **Warmup**: 学習初期に徐々に学習率を上げる\n",
    "\n",
    "5. **Cyclical LR**: 周期的に変動させる\n",
    "\n",
    "6. **One Cycle**: 1サイクルで上昇→下降\n",
    "\n",
    "### 次のノートブック（114: 正則化と最適化）への橋渡し\n",
    "\n",
    "学習率スケジューリングは過学習を防ぐ効果もありますが、より直接的な正則化手法があります：\n",
    "\n",
    "- **L1/L2正則化**\n",
    "- **Weight Decay**\n",
    "- **Dropout**\n",
    "\n",
    "次のノートブックでは、これらの正則化と最適化の関係を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Smith, L. N. (2017). Cyclical learning rates for training neural networks. *WACV*.\n",
    "2. Smith, L. N., & Topin, N. (2019). Super-convergence: Very fast training using large learning rates. *arXiv:1708.07120*.\n",
    "3. Loshchilov, I., & Hutter, F. (2017). SGDR: Stochastic gradient descent with warm restarts. *ICLR*.\n",
    "4. Gotmare, A., et al. (2019). A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. *ICLR*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
