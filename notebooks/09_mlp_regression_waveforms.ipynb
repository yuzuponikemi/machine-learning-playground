{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 09: MLP Regression for Waveform Prediction\n",
    "\n",
    "Train MLP models to predict complex waveforms and signals.\n",
    "\n",
    "## Learning Objectives\n",
    "- Generate complex waveforms with controlled parameters\n",
    "- Create features for time series prediction\n",
    "- Train MLPRegressor for waveform prediction\n",
    "- Visualize predicted vs actual waveforms\n",
    "- Explore parameter effects on signal reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Complex Waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complex_waveform(n_points=1000, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Generate a complex waveform with multiple harmonics.\n",
    "    y = sin(t) + 0.5*sin(3t) + 0.25*sin(5t) + noise\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, 4 * np.pi, n_points)\n",
    "    \n",
    "    # Base signal with harmonics\n",
    "    y = (np.sin(t) + \n",
    "         0.5 * np.sin(3 * t) + \n",
    "         0.25 * np.sin(5 * t))\n",
    "    \n",
    "    # Add noise\n",
    "    y += noise_level * np.random.randn(n_points)\n",
    "    \n",
    "    return t, y\n",
    "\n",
    "# Generate waveform\n",
    "t, y = generate_complex_waveform(n_points=1000, noise_level=0.1)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(t, y, 'b-', linewidth=0.5)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Complex Waveform (Sum of Harmonics + Noise)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Waveform length: {len(y)} points\")\n",
    "print(f\"Value range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Features for Time Series Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(y, n_lags=10):\n",
    "    \"\"\"\n",
    "    Create lagged features for time series prediction.\n",
    "    X[i] = [y[i-n_lags], y[i-n_lags+1], ..., y[i-1]]\n",
    "    y_target[i] = y[i]\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y_target = []\n",
    "    \n",
    "    for i in range(n_lags, len(y)):\n",
    "        X.append(y[i-n_lags:i])\n",
    "        y_target.append(y[i])\n",
    "    \n",
    "    return np.array(X), np.array(y_target)\n",
    "\n",
    "# Create features\n",
    "n_lags = 20\n",
    "X, y_target = create_lag_features(y, n_lags=n_lags)\n",
    "\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "print(f\"Target shape: {y_target.shape}\")\n",
    "print(f\"\\nEach sample uses {n_lags} previous time points to predict the next value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize lag features\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Show first few samples\n",
    "sample_idx = 0\n",
    "axes[0].plot(range(n_lags), X[sample_idx], 'bo-', label='Input (lag features)')\n",
    "axes[0].axvline(x=n_lags-0.5, color='r', linestyle='--')\n",
    "axes[0].scatter([n_lags], [y_target[sample_idx]], c='r', s=100, zorder=5, label='Target')\n",
    "axes[0].set_xlabel('Time Step')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].set_title(f'Sample {sample_idx}: Lag Features → Target')\n",
    "axes[0].legend()\n",
    "\n",
    "# Show multiple samples\n",
    "for i in range(0, min(5, len(X))):\n",
    "    axes[1].plot(range(i, i + n_lags), X[i], 'b-', alpha=0.3)\n",
    "    axes[1].scatter([i + n_lags], [y_target[i]], c='r', s=30, alpha=0.5)\n",
    "\n",
    "axes[1].set_xlabel('Time Step')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('First 5 Samples Overlaid')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_target, test_size=0.2, shuffle=False  # Keep temporal order\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP Regressor\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_train = mlp.predict(X_train_scaled)\n",
    "y_pred_test = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "print(\"Model Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training R²: {r2_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test R²: {r2_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(y_test, y_pred_test):.4f}\")\n",
    "print(f\"\\nIterations: {mlp.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualize Waveform Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs actual waveform\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Full waveform comparison\n",
    "axes[0].plot(y_test, 'b-', linewidth=0.5, label='Actual', alpha=0.7)\n",
    "axes[0].plot(y_pred_test, 'r-', linewidth=0.5, label='Predicted', alpha=0.7)\n",
    "axes[0].set_xlabel('Time Step')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title('Waveform Prediction: Full Test Set')\n",
    "axes[0].legend()\n",
    "\n",
    "# Zoomed view\n",
    "zoom_start, zoom_end = 0, 100\n",
    "axes[1].plot(range(zoom_start, zoom_end), y_test[zoom_start:zoom_end], 'b-', \n",
    "             linewidth=1, label='Actual', marker='o', markersize=3)\n",
    "axes[1].plot(range(zoom_start, zoom_end), y_pred_test[zoom_start:zoom_end], 'r--', \n",
    "             linewidth=1, label='Predicted', marker='x', markersize=3)\n",
    "axes[1].set_xlabel('Time Step')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].set_title('Waveform Prediction: Zoomed View')\n",
    "axes[1].legend()\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "axes[2].plot(residuals, 'g-', linewidth=0.5)\n",
    "axes[2].axhline(y=0, color='k', linestyle='--')\n",
    "axes[2].set_xlabel('Time Step')\n",
    "axes[2].set_ylabel('Residual')\n",
    "axes[2].set_title(f'Prediction Residuals (MAE = {mean_absolute_error(y_test, y_pred_test):.4f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.5, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual')\n",
    "axes[0].set_ylabel('Predicted')\n",
    "axes[0].set_title(f'Predicted vs Actual (R² = {r2_score(y_test, y_pred_test):.3f})')\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Residual')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residual Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Effect of Number of Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different number of lags\n",
    "lag_values = [5, 10, 20, 30, 50]\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_lags in lag_values:\n",
    "    # Create features\n",
    "    X_lag, y_lag = create_lag_features(y, n_lags=n_lags)\n",
    "    \n",
    "    # Split\n",
    "    X_train_lag, X_test_lag, y_train_lag, y_test_lag = train_test_split(\n",
    "        X_lag, y_lag, test_size=0.2, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Scale\n",
    "    scaler_lag = StandardScaler()\n",
    "    X_train_lag_scaled = scaler_lag.fit_transform(X_train_lag)\n",
    "    X_test_lag_scaled = scaler_lag.transform(X_test_lag)\n",
    "    \n",
    "    # Train\n",
    "    mlp_lag = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp_lag.fit(X_train_lag_scaled, y_train_lag)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_lag = mlp_lag.predict(X_test_lag_scaled)\n",
    "    \n",
    "    results.append({\n",
    "        'n_lags': n_lags,\n",
    "        'r2': r2_score(y_test_lag, y_pred_lag),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test_lag, y_pred_lag)),\n",
    "        'mae': mean_absolute_error(y_test_lag, y_pred_lag)\n",
    "    })\n",
    "\n",
    "df_lags = pd.DataFrame(results)\n",
    "print(\"Effect of Number of Lags:\")\n",
    "print(df_lags.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(df_lags['n_lags'], df_lags['r2'], 'bo-', markersize=10)\n",
    "axes[0].set_xlabel('Number of Lags')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('Effect of Lag Features on R²')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(df_lags['n_lags'], df_lags['rmse'], 'ro-', markersize=10)\n",
    "axes[1].set_xlabel('Number of Lags')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Effect of Lag Features on RMSE')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Architecture Search for Waveform Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimal number of lags\n",
    "best_n_lags = df_lags.loc[df_lags['r2'].idxmax(), 'n_lags']\n",
    "print(f\"Using optimal n_lags: {best_n_lags}\")\n",
    "\n",
    "# Create features\n",
    "X, y_target = create_lag_features(y, n_lags=int(best_n_lags))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_target, test_size=0.2, shuffle=False\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Test architectures\n",
    "architectures = [\n",
    "    (50,), (100,), (200,),\n",
    "    (100, 50), (200, 100),\n",
    "    (100, 50, 25), (200, 100, 50)\n",
    "]\n",
    "\n",
    "arch_results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=arch,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = mlp.predict(X_test_scaled)\n",
    "    \n",
    "    arch_results.append({\n",
    "        'architecture': str(arch),\n",
    "        'r2': r2_score(y_test, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'n_iter': mlp.n_iter_\n",
    "    })\n",
    "\n",
    "df_arch = pd.DataFrame(arch_results).sort_values('r2', ascending=False)\n",
    "print(\"\\nArchitecture Comparison:\")\n",
    "print(df_arch.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: GridSearchCV for Waveform Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (200,), (100, 50), (200, 100)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    mlp, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV R²: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Test with best model\n",
    "y_pred_best = grid_search.predict(X_test_scaled)\n",
    "print(f\"Test R²: {r2_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_best)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize best model predictions\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Full prediction\n",
    "axes[0].plot(y_test, 'b-', linewidth=0.5, label='Actual', alpha=0.7)\n",
    "axes[0].plot(y_pred_best, 'r-', linewidth=0.5, label='Predicted', alpha=0.7)\n",
    "axes[0].set_xlabel('Time Step')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title(f'Best Model Waveform Prediction (R² = {r2_score(y_test, y_pred_best):.4f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# Zoomed view\n",
    "zoom_range = slice(0, 150)\n",
    "axes[1].plot(y_test[zoom_range], 'b-', linewidth=1.5, label='Actual')\n",
    "axes[1].plot(y_pred_best[zoom_range], 'r--', linewidth=1.5, label='Predicted')\n",
    "axes[1].set_xlabel('Time Step')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].set_title('Zoomed View')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Different Waveform Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_damped_oscillation(n_points=1000, damping=0.3, noise=0.05):\n",
    "    t = np.linspace(0, 4 * np.pi, n_points)\n",
    "    y = np.exp(-damping * t) * np.sin(5 * t)\n",
    "    y += noise * np.random.randn(n_points)\n",
    "    return t, y\n",
    "\n",
    "def generate_am_signal(n_points=1000, carrier_freq=20, mod_freq=2, noise=0.05):\n",
    "    t = np.linspace(0, 4 * np.pi, n_points)\n",
    "    carrier = np.sin(carrier_freq * t)\n",
    "    modulator = 1 + 0.5 * np.sin(mod_freq * t)\n",
    "    y = carrier * modulator\n",
    "    y += noise * np.random.randn(n_points)\n",
    "    return t, y\n",
    "\n",
    "def generate_chirp(n_points=1000, noise=0.05):\n",
    "    t = np.linspace(0, 4 * np.pi, n_points)\n",
    "    # Frequency increases with time\n",
    "    y = np.sin(t * (1 + t/4))\n",
    "    y += noise * np.random.randn(n_points)\n",
    "    return t, y\n",
    "\n",
    "# Generate all waveforms\n",
    "waveforms = {\n",
    "    'Complex Harmonic': generate_complex_waveform(1000, 0.1),\n",
    "    'Damped Oscillation': generate_damped_oscillation(1000, 0.3, 0.05),\n",
    "    'AM Signal': generate_am_signal(1000, 20, 2, 0.05),\n",
    "    'Chirp': generate_chirp(1000, 0.05)\n",
    "}\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, (t, y)) in enumerate(waveforms.items()):\n",
    "    axes[idx].plot(t, y, 'b-', linewidth=0.5)\n",
    "    axes[idx].set_xlabel('Time')\n",
    "    axes[idx].set_ylabel('Amplitude')\n",
    "    axes[idx].set_title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each waveform type\n",
    "n_lags = 30\n",
    "\n",
    "waveform_results = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, (t, y_wave)) in enumerate(waveforms.items()):\n",
    "    # Create features\n",
    "    X, y_target = create_lag_features(y_wave, n_lags=n_lags)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_target, test_size=0.2, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(200, 100),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = mlp.predict(X_test_scaled)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    waveform_results.append({\n",
    "        'Waveform': name,\n",
    "        'R²': r2,\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].plot(y_test[:100], 'b-', linewidth=1, label='Actual')\n",
    "    axes[idx].plot(y_pred[:100], 'r--', linewidth=1, label='Predicted')\n",
    "    axes[idx].set_title(f'{name} (R² = {r2:.3f})')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWaveform Prediction Results:\")\n",
    "print(pd.DataFrame(waveform_results).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Multi-Step Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for recursive multi-step prediction\n",
    "t, y = generate_complex_waveform(1000, 0.1)\n",
    "n_lags = 30\n",
    "\n",
    "X, y_target = create_lag_features(y, n_lags=n_lags)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_target, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(200, 100),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Multi-step prediction\n",
    "def recursive_predict(model, scaler, initial_sequence, n_steps):\n",
    "    \"\"\"Predict multiple steps ahead recursively.\"\"\"\n",
    "    predictions = []\n",
    "    current_seq = initial_sequence.copy()\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Scale and predict\n",
    "        x_scaled = scaler.transform(current_seq.reshape(1, -1))\n",
    "        pred = model.predict(x_scaled)[0]\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        # Update sequence\n",
    "        current_seq = np.roll(current_seq, -1)\n",
    "        current_seq[-1] = pred\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Make multi-step predictions\n",
    "n_predict = 100\n",
    "initial_seq = X_test[0]\n",
    "y_multi = recursive_predict(mlp, scaler, initial_seq, n_predict)\n",
    "y_actual = y_test[:n_predict]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_actual, 'b-', linewidth=1.5, label='Actual')\n",
    "plt.plot(y_multi, 'r--', linewidth=1.5, label='Multi-step Prediction')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title(f'Multi-step Recursive Prediction ({n_predict} steps)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Multi-step R²: {r2_score(y_actual, y_multi):.4f}\")\n",
    "print(f\"Multi-step RMSE: {np.sqrt(mean_squared_error(y_actual, y_multi)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Summary and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waveform Prediction Best Practices:\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"1. FEATURE ENGINEERING\")\n",
    "print(\"   • Use lag features (past values as predictors)\")\n",
    "print(\"   • Optimal lag length depends on signal periodicity\")\n",
    "print(\"   • Consider adding derivative features\")\n",
    "print()\n",
    "print(\"2. ARCHITECTURE\")\n",
    "print(\"   • Start with (100, 50) or (200, 100)\")\n",
    "print(\"   • Deeper networks for more complex patterns\")\n",
    "print(\"   • Input size = number of lag features\")\n",
    "print()\n",
    "print(\"3. TRAINING\")\n",
    "print(\"   • Don't shuffle time series data\")\n",
    "print(\"   • Use early stopping\")\n",
    "print(\"   • Scale features with StandardScaler\")\n",
    "print()\n",
    "print(\"4. EVALUATION\")\n",
    "print(\"   • Use R², RMSE, MAE\")\n",
    "print(\"   • Visualize predictions vs actual\")\n",
    "print(\"   • Check residual distribution\")\n",
    "print()\n",
    "print(\"5. MULTI-STEP PREDICTION\")\n",
    "print(\"   • Recursive: predict one step, feed back\")\n",
    "print(\"   • Error accumulates with more steps\")\n",
    "print(\"   • Consider direct multi-output models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "### Waveform Generation\n",
    "- Complex harmonics, damped oscillations, AM signals, chirps\n",
    "- Controlling parameters (frequency, amplitude, noise)\n",
    "\n",
    "### Feature Engineering for Time Series\n",
    "- Lag features for prediction\n",
    "- Effect of number of lags on performance\n",
    "\n",
    "### MLP Regression\n",
    "- Architecture selection for waveform prediction\n",
    "- Hyperparameter tuning with GridSearchCV\n",
    "- Evaluation metrics (R², RMSE, MAE)\n",
    "\n",
    "### Visualization\n",
    "- Predicted vs actual waveforms\n",
    "- Residual analysis\n",
    "- Multi-step prediction\n",
    "\n",
    "### Key Takeaways\n",
    "- Number of lags significantly affects performance\n",
    "- Larger architectures work better for complex patterns\n",
    "- Multi-step prediction accumulates error\n",
    "- Always visualize your predictions!\n",
    "\n",
    "### Next Steps\n",
    "Continue to **Notebook 10** for automated hyperparameter tuning techniques!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
