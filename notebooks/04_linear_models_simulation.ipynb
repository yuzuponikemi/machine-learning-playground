{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Linear Models with Parameter Simulation\n",
    "\n",
    "Deep dive into linear models with comprehensive parameter exploration.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand linear regression fundamentals\n",
    "- Explore regularization (Ridge, Lasso, ElasticNet)\n",
    "- Simulate effects of noise, outliers, and multicollinearity\n",
    "- Visualize decision boundaries for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet,\n",
    "    LogisticRegression, RidgeClassifier\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear Regression Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple linear data\n",
    "n_samples = 200\n",
    "X = np.random.uniform(-3, 3, n_samples).reshape(-1, 1)\n",
    "y_true = 2 * X.ravel() + 1  # y = 2x + 1\n",
    "y = y_true + np.random.randn(n_samples) * 0.5  # Add noise\n",
    "\n",
    "# Fit linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_line = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y_pred = model.predict(X_line)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data')\n",
    "plt.plot(X_line, y_pred, 'r-', linewidth=2, label=f'y = {model.coef_[0]:.2f}x + {model.intercept_:.2f}')\n",
    "plt.plot(X_line, 2 * X_line + 1, 'g--', linewidth=1, label='True: y = 2x + 1')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Learned coefficient: {model.coef_[0]:.4f} (true: 2.0)\")\n",
    "print(f\"Learned intercept: {model.intercept_:.4f} (true: 1.0)\")\n",
    "print(f\"R² score: {model.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Effect of Noise Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate different noise levels\n",
    "noise_levels = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, noise in enumerate(noise_levels):\n",
    "    # Generate data\n",
    "    X = np.random.uniform(-3, 3, n_samples).reshape(-1, 1)\n",
    "    y = 2 * X.ravel() + 1 + np.random.randn(n_samples) * noise\n",
    "    \n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_line)\n",
    "    r2 = model.score(X, y)\n",
    "    \n",
    "    results.append({\n",
    "        'noise': noise,\n",
    "        'coef': model.coef_[0],\n",
    "        'intercept': model.intercept_,\n",
    "        'r2': r2\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].scatter(X, y, alpha=0.6)\n",
    "    axes[idx].plot(X_line, y_pred, 'r-', linewidth=2)\n",
    "    axes[idx].plot(X_line, 2 * X_line + 1, 'g--', linewidth=1)\n",
    "    axes[idx].set_title(f'Noise = {noise}, R² = {r2:.3f}')\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nEffect of Noise on Model Performance:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Regularization - Ridge, Lasso, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-dimensional data with some irrelevant features\n",
    "X, y, coef_true = make_regression(\n",
    "    n_samples=200, n_features=50, n_informative=10,\n",
    "    noise=10, coef=True, random_state=42\n",
    ")\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"True non-zero coefficients: {np.sum(coef_true != 0)}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularization methods\n",
    "models = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=1.0),\n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "results = []\n",
    "coefs = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    n_nonzero = np.sum(np.abs(model.coef_) > 0.01)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train R²': train_score,\n",
    "        'Test R²': test_score,\n",
    "        'Non-zero coefs': n_nonzero\n",
    "    })\n",
    "    coefs[name] = model.coef_\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"Model Comparison:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient magnitudes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, coef) in enumerate(coefs.items()):\n",
    "    axes[idx].bar(range(len(coef)), coef, alpha=0.7)\n",
    "    axes[idx].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    axes[idx].set_xlabel('Feature Index')\n",
    "    axes[idx].set_ylabel('Coefficient')\n",
    "    axes[idx].set_title(f'{name} Coefficients')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Alpha Parameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep alpha values for Ridge\n",
    "alphas = np.logspace(-4, 4, 50)\n",
    "\n",
    "ridge_train_scores = []\n",
    "ridge_test_scores = []\n",
    "ridge_coef_norms = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    ridge_train_scores.append(model.score(X_train_scaled, y_train))\n",
    "    ridge_test_scores.append(model.score(X_test_scaled, y_test))\n",
    "    ridge_coef_norms.append(np.linalg.norm(model.coef_))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scores vs alpha\n",
    "axes[0].semilogx(alphas, ridge_train_scores, 'b-', label='Train')\n",
    "axes[0].semilogx(alphas, ridge_test_scores, 'r-', label='Test')\n",
    "axes[0].set_xlabel('Alpha')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('Ridge: Score vs Alpha')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Coefficient norm vs alpha\n",
    "axes[1].loglog(alphas, ridge_coef_norms, 'g-')\n",
    "axes[1].set_xlabel('Alpha')\n",
    "axes[1].set_ylabel('Coefficient L2 Norm')\n",
    "axes[1].set_title('Ridge: Coefficient Shrinkage')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal alpha\n",
    "optimal_idx = np.argmax(ridge_test_scores)\n",
    "print(f\"Optimal alpha: {alphas[optimal_idx]:.4f}\")\n",
    "print(f\"Best test R²: {ridge_test_scores[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep alpha for Lasso - observe sparsity\n",
    "lasso_n_nonzero = []\n",
    "lasso_test_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = Lasso(alpha=alpha, max_iter=10000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    lasso_test_scores.append(model.score(X_test_scaled, y_test))\n",
    "    lasso_n_nonzero.append(np.sum(np.abs(model.coef_) > 1e-6))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].semilogx(alphas, lasso_test_scores, 'r-')\n",
    "axes[0].set_xlabel('Alpha')\n",
    "axes[0].set_ylabel('Test R² Score')\n",
    "axes[0].set_title('Lasso: Test Score vs Alpha')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].semilogx(alphas, lasso_n_nonzero, 'b-')\n",
    "axes[1].set_xlabel('Alpha')\n",
    "axes[1].set_ylabel('Number of Non-zero Coefficients')\n",
    "axes[1].set_title('Lasso: Feature Selection')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Effect of Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with correlated features\n",
    "n_samples = 200\n",
    "\n",
    "# Independent features\n",
    "X1 = np.random.randn(n_samples)\n",
    "X2 = np.random.randn(n_samples)\n",
    "\n",
    "# Correlated feature (X3 ≈ X1 + noise)\n",
    "X3 = X1 + np.random.randn(n_samples) * 0.1  # Highly correlated with X1\n",
    "\n",
    "# Create target\n",
    "y = 3 * X1 + 2 * X2 + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Compare models with/without correlated feature\n",
    "X_no_corr = np.column_stack([X1, X2])\n",
    "X_with_corr = np.column_stack([X1, X2, X3])\n",
    "\n",
    "print(f\"Correlation between X1 and X3: {np.corrcoef(X1, X3)[0, 1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Linear Regression vs Ridge with multicollinearity\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models_mc = {\n",
    "    'Linear (no corr)': (LinearRegression(), X_no_corr),\n",
    "    'Linear (with corr)': (LinearRegression(), X_with_corr),\n",
    "    'Ridge (with corr)': (Ridge(alpha=1.0), X_with_corr)\n",
    "}\n",
    "\n",
    "print(\"Effect of Multicollinearity:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, (model, X_data) in models_mc.items():\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_data)\n",
    "    \n",
    "    # Fit\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    # CV score\n",
    "    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Coefficients: {model.coef_}\")\n",
    "    print(f\"  CV R² Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Effect of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with outliers\n",
    "n_samples = 100\n",
    "X = np.random.uniform(-3, 3, n_samples).reshape(-1, 1)\n",
    "y = 2 * X.ravel() + 1 + np.random.randn(n_samples) * 0.3\n",
    "\n",
    "# Add outliers\n",
    "n_outliers = 5\n",
    "outlier_idx = np.random.choice(n_samples, n_outliers, replace=False)\n",
    "y_outliers = y.copy()\n",
    "y_outliers[outlier_idx] += np.random.uniform(5, 10, n_outliers) * np.sign(np.random.randn(n_outliers))\n",
    "\n",
    "# Compare models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "X_line = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "\n",
    "for idx, (y_data, title) in enumerate([(y, 'Without Outliers'), (y_outliers, 'With Outliers')]):\n",
    "    # Fit models\n",
    "    lr = LinearRegression().fit(X, y_data)\n",
    "    \n",
    "    # Use Huber loss for robust regression\n",
    "    from sklearn.linear_model import HuberRegressor\n",
    "    huber = HuberRegressor().fit(X, y_data)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].scatter(X, y_data, alpha=0.6)\n",
    "    if idx == 1:\n",
    "        axes[idx].scatter(X[outlier_idx], y_data[outlier_idx], c='red', s=100, marker='x', label='Outliers')\n",
    "    \n",
    "    axes[idx].plot(X_line, lr.predict(X_line), 'b-', lw=2, label=f'Linear (coef={lr.coef_[0]:.2f})')\n",
    "    axes[idx].plot(X_line, huber.predict(X_line), 'g--', lw=2, label=f'Huber (coef={huber.coef_[0]:.2f})')\n",
    "    axes[idx].plot(X_line, 2 * X_line + 1, 'k:', lw=1, label='True')\n",
    "    \n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].set_title(title)\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear data\n",
    "n_samples = 100\n",
    "X = np.sort(np.random.uniform(-3, 3, n_samples)).reshape(-1, 1)\n",
    "y = 0.5 * X.ravel()**3 - 2 * X.ravel()**2 + X.ravel() + np.random.randn(n_samples) * 2\n",
    "\n",
    "# Compare different polynomial degrees\n",
    "degrees = [1, 2, 3, 5, 10]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    \n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_plot_poly)\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_score = model.score(X_poly, y)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].scatter(X, y, alpha=0.6)\n",
    "    axes[idx].plot(X_plot, y_pred, 'r-', lw=2)\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].set_title(f'Degree {degree} (R² = {train_score:.3f})')\n",
    "    axes[idx].set_ylim(-30, 30)\n",
    "\n",
    "axes[-1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial with regularization to prevent overfitting\n",
    "degree = 10\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_plot_poly = poly.transform(X_plot)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "alphas_poly = [0, 0.01, 1.0]\n",
    "\n",
    "for idx, alpha in enumerate(alphas_poly):\n",
    "    if alpha == 0:\n",
    "        model = LinearRegression()\n",
    "    else:\n",
    "        model = Ridge(alpha=alpha)\n",
    "    \n",
    "    model.fit(X_poly, y)\n",
    "    y_pred = model.predict(X_plot_poly)\n",
    "    \n",
    "    axes[idx].scatter(X, y, alpha=0.6)\n",
    "    axes[idx].plot(X_plot, y_pred, 'r-', lw=2)\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].set_title(f'Degree {degree}, Alpha = {alpha}')\n",
    "    axes[idx].set_ylim(-30, 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Logistic Regression for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Visualize data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Classification Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision boundary function\n",
    "def plot_decision_boundary(model, X, y, ax, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black', s=30)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Compare regularization in logistic regression\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "C_values = [0.01, 1.0, 100.0]  # C = 1/alpha (inverse regularization)\n",
    "\n",
    "for idx, C in enumerate(C_values):\n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    acc = model.score(X_test_scaled, y_test)\n",
    "    plot_decision_boundary(model, X_train_scaled, y_train, axes[idx], \n",
    "                          f'C = {C} (Acc = {acc:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C parameter sweep\n",
    "C_range = np.logspace(-3, 3, 30)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for C in C_range:\n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_scores.append(model.score(X_train_scaled, y_train))\n",
    "    test_scores.append(model.score(X_test_scaled, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(C_range, train_scores, 'b-', label='Train')\n",
    "plt.semilogx(C_range, test_scores, 'r-', label='Test')\n",
    "plt.xlabel('C (Inverse Regularization)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Logistic Regression: Effect of Regularization')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "optimal_C = C_range[np.argmax(test_scores)]\n",
    "print(f\"Optimal C: {optimal_C:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "### Linear Regression\n",
    "- Basic linear regression fitting\n",
    "- Effect of noise on model performance\n",
    "\n",
    "### Regularization\n",
    "- **Ridge**: L2 penalty, shrinks coefficients\n",
    "- **Lasso**: L1 penalty, feature selection (sparse)\n",
    "- **ElasticNet**: Combination of L1 and L2\n",
    "\n",
    "### Parameter Effects\n",
    "- Higher alpha = more regularization = simpler model\n",
    "- Regularization helps with multicollinearity\n",
    "- Polynomial features + regularization prevents overfitting\n",
    "\n",
    "### Key Takeaways\n",
    "- Use Ridge when all features might be relevant\n",
    "- Use Lasso for automatic feature selection\n",
    "- Regularization is crucial for high-dimensional data\n",
    "- Always tune the regularization parameter (alpha/C)\n",
    "\n",
    "### Next Steps\n",
    "Continue to **Notebook 05** for tree and ensemble models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
