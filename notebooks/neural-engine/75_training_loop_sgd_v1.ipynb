{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 75: 学習ループの完成 ― SGDで重みを更新\n",
    "\n",
    "## Training Loop: Parameter Updates with SGD\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Unit 0.0「ニューラルエンジンの深部」** の第6章として、計算グラフ・逆伝播を統合し、**完全な学習ループ** を実装します。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **SGD（確率的勾配降下法）** とそのバリエーションを実装する\n",
    "2. **学習ループ** を構成する要素を理解する\n",
    "3. **XOR問題** を解いて動作を確認する\n",
    "4. **学習曲線** と **決定境界** を可視化する\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 70-74 の内容\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [学習アルゴリズムの概要](#1-学習アルゴリズムの概要)\n",
    "2. [オプティマイザの実装](#2-オプティマイザの実装)\n",
    "3. [学習ループの実装](#3-学習ループの実装)\n",
    "4. [XOR問題を解く](#4-xor問題を解く)\n",
    "5. [学習の可視化](#5-学習の可視化)\n",
    "6. [より複雑な問題への適用](#6-より複雑な問題への適用)\n",
    "7. [演習問題](#7-演習問題)\n",
    "8. [まとめと次のステップ](#8-まとめと次のステップ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. 前のノートブックのクラスを再定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 74 の Layer クラス群を再定義\n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        self.cache = {}\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x): pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, dout): pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        scale = np.sqrt(2.0 / (in_features + out_features))\n",
    "        self.params['W'] = np.random.randn(in_features, out_features) * scale\n",
    "        self.params['b'] = np.zeros(out_features)\n",
    "        self.grads['W'] = None\n",
    "        self.grads['b'] = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache['x'] = x\n",
    "        return x @ self.params['W'] + self.params['b']\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x = self.cache['x']\n",
    "        self.grads['W'] = x.T @ dout\n",
    "        self.grads['b'] = np.sum(dout, axis=0)\n",
    "        return dout @ self.params['W'].T\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, x):\n",
    "        y = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        self.cache['y'] = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        y = self.cache['y']\n",
    "        return dout * y * (1 - y)\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, x):\n",
    "        self.cache['mask'] = (x > 0).astype(float)\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.cache['mask']\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def forward(self, x):\n",
    "        y = np.tanh(x)\n",
    "        self.cache['y'] = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        y = self.cache['y']\n",
    "        return dout * (1 - y ** 2)\n",
    "\n",
    "\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, y, t):\n",
    "        self.cache['y'] = y\n",
    "        self.cache['t'] = t\n",
    "        self.cache['N'] = y.shape[0]\n",
    "        return np.mean((y - t) ** 2)\n",
    "    \n",
    "    def backward(self):\n",
    "        y, t, N = self.cache['y'], self.cache['t'], self.cache['N']\n",
    "        return (2 / N) * (y - t)\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_dims, activation='relu'):\n",
    "        self.layers = []\n",
    "        act_class = {'relu': ReLU, 'sigmoid': Sigmoid, 'tanh': Tanh}[activation]\n",
    "        \n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            self.layers.append(Linear(layer_dims[i], layer_dims[i+1]))\n",
    "            if i < len(layer_dims) - 2:\n",
    "                self.layers.append(act_class())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                for name, param in layer.params.items():\n",
    "                    params.append((layer, name, param))\n",
    "        return params\n",
    "\n",
    "\n",
    "print(\"Layer クラス群を定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 学習アルゴリズムの概要\n",
    "\n",
    "### 1.1 勾配降下法の基本\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)\n",
    "$$\n",
    "\n",
    "- $\\theta$: パラメータ（重み、バイアス）\n",
    "- $\\eta$: 学習率（Learning Rate）\n",
    "- $\\nabla L$: 損失の勾配\n",
    "\n",
    "### 1.2 学習ループの構成要素\n",
    "\n",
    "```\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data:\n",
    "        1. 順伝播: y = model(x)\n",
    "        2. 損失計算: L = loss(y, t)\n",
    "        3. 逆伝播: grads = backward(L)\n",
    "        4. パラメータ更新: params -= lr * grads\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. オプティマイザの実装\n",
    "\n",
    "### 2.1 SGD（確率的勾配降下法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法（Stochastic Gradient Descent）\n",
    "    \n",
    "    更新則: θ = θ - η * ∇L\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, model):\n",
    "        \"\"\"モデルのパラメータを更新\"\"\"\n",
    "        for layer in model.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                for name in layer.params:\n",
    "                    if layer.grads[name] is not None:\n",
    "                        layer.params[name] -= self.lr * layer.grads[name]\n",
    "\n",
    "\n",
    "print(\"SGDオプティマイザを定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Momentum SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumSGD:\n",
    "    \"\"\"\n",
    "    モーメンタム付きSGD\n",
    "    \n",
    "    更新則:\n",
    "        v = μ * v - η * ∇L\n",
    "        θ = θ + v\n",
    "    \n",
    "    μ: モーメンタム係数（通常0.9）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocity = {}  # 各パラメータの速度を保持\n",
    "    \n",
    "    def update(self, model):\n",
    "        for layer in model.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                for name in layer.params:\n",
    "                    if layer.grads[name] is not None:\n",
    "                        key = id(layer.params[name])\n",
    "                        \n",
    "                        # 速度の初期化\n",
    "                        if key not in self.velocity:\n",
    "                            self.velocity[key] = np.zeros_like(layer.params[name])\n",
    "                        \n",
    "                        # 速度の更新\n",
    "                        self.velocity[key] = self.momentum * self.velocity[key] - self.lr * layer.grads[name]\n",
    "                        \n",
    "                        # パラメータの更新\n",
    "                        layer.params[name] += self.velocity[key]\n",
    "\n",
    "\n",
    "print(\"MomentumSGDオプティマイザを定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam (Adaptive Moment Estimation)\n",
    "    \n",
    "    更新則:\n",
    "        m = β1 * m + (1 - β1) * g\n",
    "        v = β2 * v + (1 - β2) * g²\n",
    "        m_hat = m / (1 - β1^t)\n",
    "        v_hat = v / (1 - β2^t)\n",
    "        θ = θ - η * m_hat / (√v_hat + ε)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = {}  # 1次モーメント\n",
    "        self.v = {}  # 2次モーメント\n",
    "        self.t = 0   # タイムステップ\n",
    "    \n",
    "    def update(self, model):\n",
    "        self.t += 1\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                for name in layer.params:\n",
    "                    if layer.grads[name] is not None:\n",
    "                        key = id(layer.params[name])\n",
    "                        g = layer.grads[name]\n",
    "                        \n",
    "                        # 初期化\n",
    "                        if key not in self.m:\n",
    "                            self.m[key] = np.zeros_like(g)\n",
    "                            self.v[key] = np.zeros_like(g)\n",
    "                        \n",
    "                        # モーメントの更新\n",
    "                        self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * g\n",
    "                        self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (g ** 2)\n",
    "                        \n",
    "                        # バイアス補正\n",
    "                        m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "                        v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "                        \n",
    "                        # パラメータ更新\n",
    "                        layer.params[name] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "\n",
    "print(\"Adamオプティマイザを定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 学習ループの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, X, t, epochs=1000, verbose=True, log_interval=100):\n",
    "    \"\"\"\n",
    "    学習ループ\n",
    "    \n",
    "    Args:\n",
    "        model: MLPモデル\n",
    "        loss_fn: 損失関数\n",
    "        optimizer: オプティマイザ\n",
    "        X: 入力データ (N, D)\n",
    "        t: ターゲット (N, K)\n",
    "        epochs: エポック数\n",
    "        verbose: 進捗表示\n",
    "        log_interval: ログ出力間隔\n",
    "    \n",
    "    Returns:\n",
    "        学習履歴（損失のリスト）\n",
    "    \"\"\"\n",
    "    history = {'loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 1. 順伝播\n",
    "        y = model.forward(X)\n",
    "        \n",
    "        # 2. 損失計算\n",
    "        loss = loss_fn.forward(y, t)\n",
    "        history['loss'].append(loss)\n",
    "        \n",
    "        # 3. 逆伝播\n",
    "        dout = loss_fn.backward()\n",
    "        model.backward(dout)\n",
    "        \n",
    "        # 4. パラメータ更新\n",
    "        optimizer.update(model)\n",
    "        \n",
    "        # ログ出力\n",
    "        if verbose and (epoch + 1) % log_interval == 0:\n",
    "            print(f\"Epoch {epoch+1:5d}: Loss = {loss:.6f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"学習ループを定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. XOR問題を解く\n",
    "\n",
    "### 4.1 XOR問題とは\n",
    "\n",
    "XOR（排他的論理和）は線形分離不可能な問題で、ニューラルネットワークの能力を示す古典的なベンチマークです。\n",
    "\n",
    "```\n",
    "入力        出力\n",
    "(0, 0) →    0\n",
    "(0, 1) →    1\n",
    "(1, 0) →    1\n",
    "(1, 1) →    0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XORデータセット\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float64)\n",
    "t_xor = np.array([[0], [1], [1], [0]], dtype=np.float64)\n",
    "\n",
    "print(\"【XORデータセット】\")\n",
    "print(\"入力 → 出力\")\n",
    "for x, y in zip(X_xor, t_xor):\n",
    "    print(f\"  {x} → {y[0]:.0f}\")\n",
    "\n",
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "colors = ['red' if y[0] == 0 else 'blue' for y in t_xor]\n",
    "ax.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
    "for x, y in zip(X_xor, t_xor):\n",
    "    ax.annotate(f'{int(y[0])}', (x[0], x[1]), fontsize=14, ha='center', va='center', color='white', fontweight='bold')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('XOR問題: 線形分離不可能')\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR問題を解く\n",
    "np.random.seed(42)\n",
    "\n",
    "# モデル: 2 → 4 → 1\n",
    "model = MLP([2, 4, 1], activation='sigmoid')\n",
    "loss_fn = MSELoss()\n",
    "optimizer = SGD(lr=1.0)\n",
    "\n",
    "print(\"【XOR問題の学習】\")\n",
    "print(f\"モデル: 2 → 4 → 1 (Sigmoid)\")\n",
    "print(f\"オプティマイザ: SGD (lr=1.0)\")\n",
    "print()\n",
    "\n",
    "# 学習\n",
    "history = train(model, loss_fn, optimizer, X_xor, t_xor, epochs=5000, log_interval=1000)\n",
    "\n",
    "# 最終結果\n",
    "print(\"\\n【学習後の予測】\")\n",
    "y_pred = model.forward(X_xor)\n",
    "for x, t, y in zip(X_xor, t_xor, y_pred):\n",
    "    pred_class = 1 if y[0] > 0.5 else 0\n",
    "    correct = \"✓\" if pred_class == t[0] else \"✗\"\n",
    "    print(f\"  {x} → 予測: {y[0]:.4f} ({pred_class}) 正解: {int(t[0])} {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 学習の可視化\n",
    "\n",
    "### 5.1 学習曲線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線の可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(history['loss'], linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('学習曲線: XOR問題')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 決定境界の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, t, title=\"Decision Boundary\"):\n",
    "    \"\"\"決定境界を可視化\"\"\"\n",
    "    h = 0.01\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.forward(grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # 決定境界（確率のコンター）\n",
    "    contour = ax.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.8)\n",
    "    plt.colorbar(contour, ax=ax, label='出力値')\n",
    "    \n",
    "    # 決定境界線（0.5）\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # データ点\n",
    "    colors = ['red' if y[0] == 0 else 'blue' for y in t]\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=colors, s=200, edgecolors='black', linewidth=2, zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "plot_decision_boundary(model, X_xor, t_xor, \"XOR問題の決定境界\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 オプティマイザの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なるオプティマイザの比較\n",
    "np.random.seed(42)\n",
    "\n",
    "optimizers = {\n",
    "    'SGD (lr=1.0)': SGD(lr=1.0),\n",
    "    'Momentum (lr=0.5)': MomentumSGD(lr=0.5, momentum=0.9),\n",
    "    'Adam (lr=0.1)': Adam(lr=0.1),\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "\n",
    "for name, optimizer in optimizers.items():\n",
    "    np.random.seed(42)  # 同じ初期値\n",
    "    model = MLP([2, 4, 1], activation='sigmoid')\n",
    "    loss_fn = MSELoss()\n",
    "    \n",
    "    history = train(model, loss_fn, optimizer, X_xor, t_xor, epochs=2000, verbose=False)\n",
    "    histories[name] = history['loss']\n",
    "\n",
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for name, losses in histories.items():\n",
    "    ax.plot(losses, label=name, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('オプティマイザの比較: XOR問題')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. より複雑な問題への適用\n",
    "\n",
    "### 6.1 円形データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 円形に分布するデータを生成\n",
    "def make_circles(n_samples=200, noise=0.1):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 内側の円\n",
    "    n_inner = n_samples // 2\n",
    "    theta_inner = np.random.uniform(0, 2 * np.pi, n_inner)\n",
    "    r_inner = np.random.normal(0.3, noise, n_inner)\n",
    "    X_inner = np.column_stack([r_inner * np.cos(theta_inner), r_inner * np.sin(theta_inner)])\n",
    "    \n",
    "    # 外側の円\n",
    "    n_outer = n_samples - n_inner\n",
    "    theta_outer = np.random.uniform(0, 2 * np.pi, n_outer)\n",
    "    r_outer = np.random.normal(1.0, noise, n_outer)\n",
    "    X_outer = np.column_stack([r_outer * np.cos(theta_outer), r_outer * np.sin(theta_outer)])\n",
    "    \n",
    "    X = np.vstack([X_inner, X_outer])\n",
    "    t = np.vstack([np.zeros((n_inner, 1)), np.ones((n_outer, 1))])\n",
    "    \n",
    "    return X, t\n",
    "\n",
    "\n",
    "X_circle, t_circle = make_circles(200, noise=0.1)\n",
    "\n",
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "colors = ['red' if y[0] == 0 else 'blue' for y in t_circle]\n",
    "ax.scatter(X_circle[:, 0], X_circle[:, 1], c=colors, alpha=0.7)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('円形データセット')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 円形データセットの学習\n",
    "np.random.seed(42)\n",
    "\n",
    "model = MLP([2, 16, 8, 1], activation='relu')\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(lr=0.01)\n",
    "\n",
    "print(\"【円形データセットの学習】\")\n",
    "history = train(model, loss_fn, optimizer, X_circle, t_circle, epochs=1000, log_interval=200)\n",
    "\n",
    "# 決定境界の可視化\n",
    "plot_decision_boundary(model, X_circle, t_circle, \"円形データセットの決定境界\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 演習問題\n",
    "\n",
    "### 演習 7.1: 学習率の影響\n",
    "\n",
    "学習率を変えて学習曲線の違いを観察してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.1: 解答欄\n",
    "\n",
    "# TODO: 学習率 0.01, 0.1, 1.0, 5.0 で学習を比較\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 7.2: ネットワーク構造の影響\n",
    "\n",
    "隠れ層のニューロン数を変えて、学習結果を比較してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.2: 解答欄\n",
    "\n",
    "# TODO: [2, 2, 1], [2, 4, 1], [2, 8, 1], [2, 4, 4, 1] などを比較\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **オプティマイザ**: SGD, Momentum, Adam の実装\n",
    "\n",
    "2. **学習ループ**: 順伝播 → 損失計算 → 逆伝播 → パラメータ更新\n",
    "\n",
    "3. **XOR問題**: 線形分離不可能な問題を多層パーセプトロンで解く\n",
    "\n",
    "4. **可視化**: 学習曲線、決定境界\n",
    "\n",
    "### 次のノートブック（76: 勾配の病理学）への橋渡し\n",
    "\n",
    "学習がうまくいかない場合があります。次のノートブックでは：\n",
    "\n",
    "- **勾配消失・勾配爆発** の問題\n",
    "- **鞍点と局所解** の罠\n",
    "- **診断と対策** の方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
