{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 70: 微分の再発見 ― 変化率から勾配へ\n",
    "\n",
    "## Derivative Intuition: From Rate of Change to Gradient\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Unit 0.0「ニューラルエンジンの深部」** の第1章として、ニューラルネットワークの学習を支える最も基本的な数学的道具である **微分** を再発見します。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **数値微分** を実装し、「微分とは何か」を体感する\n",
    "2. **偏微分** と **勾配ベクトル** の概念を理解する\n",
    "3. 勾配が **「最も急な上り坂の方向」** を指すことを可視化で確認する\n",
    "4. ニューラルネットワークにおける微分の役割を直感的に理解する\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- 高校数学レベルの関数の概念\n",
    "- Python / NumPy の基本操作\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [変化率とは何か：数値微分の実装](#1-変化率とは何か数値微分の実装)\n",
    "2. [解析的微分 vs 数値微分](#2-解析的微分-vs-数値微分)\n",
    "3. [多変数への拡張：偏微分](#3-多変数への拡張偏微分)\n",
    "4. [勾配ベクトル：最急上昇方向](#4-勾配ベクトル最急上昇方向)\n",
    "5. [勾配降下法への橋渡し](#5-勾配降下法への橋渡し)\n",
    "6. [演習問題](#6-演習問題)\n",
    "7. [まとめと次のステップ](#7-まとめと次のステップ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 日本語フォント設定（環境に応じて調整）\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# 再現性のためのシード\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 変化率とは何か：数値微分の実装\n",
    "\n",
    "### 1.1 微分の直感的な意味\n",
    "\n",
    "微分とは、**「関数がある点でどれくらい急に変化しているか」** を測る操作です。\n",
    "\n",
    "身近な例で考えてみましょう：\n",
    "\n",
    "- **位置の微分 → 速度**：位置が時間に対してどれくらい変化しているか\n",
    "- **速度の微分 → 加速度**：速度が時間に対してどれくらい変化しているか\n",
    "\n",
    "数学的には、関数 $f(x)$ の点 $x$ における **微分係数（導関数の値）** は次のように定義されます：\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "この式は「$x$ を微小量 $h$ だけ動かしたとき、$f(x)$ がどれだけ変化するか」の比率を表しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 数値微分の実装\n",
    "\n",
    "極限を直接計算することはできませんが、**十分に小さな $h$** を使えば近似できます。\n",
    "\n",
    "#### 前進差分（Forward Difference）\n",
    "\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "#### 中心差分（Central Difference）― より精度が高い\n",
    "\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + h) - f(x - h)}{2h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff_forward(f, x, h=1e-5):\n",
    "    \"\"\"前進差分による数値微分\"\"\"\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "\n",
    "def numerical_diff_central(f, x, h=1e-5):\n",
    "    \"\"\"中心差分による数値微分（より高精度）\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "\n",
    "# テスト関数: f(x) = x^2\n",
    "# 解析的な導関数: f'(x) = 2x\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "\n",
    "def f_prime_analytical(x):\n",
    "    \"\"\"解析的な導関数\"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "# x = 3 での微分値を計算\n",
    "x = 3.0\n",
    "analytical = f_prime_analytical(x)\n",
    "forward = numerical_diff_forward(f, x)\n",
    "central = numerical_diff_central(f, x)\n",
    "\n",
    "print(f\"f(x) = x² の x = {x} における微分値\")\n",
    "print(f\"=\"*40)\n",
    "print(f\"解析的微分:     {analytical:.10f}\")\n",
    "print(f\"前進差分:       {forward:.10f}  (誤差: {abs(analytical - forward):.2e})\")\n",
    "print(f\"中心差分:       {central:.10f}  (誤差: {abs(analytical - central):.2e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 なぜ中心差分の方が精度が高いのか？\n",
    "\n",
    "テイラー展開を使うと、誤差の次数を分析できます：\n",
    "\n",
    "- **前進差分**: $O(h)$ の誤差（$h$ に比例）\n",
    "- **中心差分**: $O(h^2)$ の誤差（$h^2$ に比例、より小さい）\n",
    "\n",
    "$h$ を小さくしたときの挙動を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h を変化させたときの誤差を比較\n",
    "h_values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-10, 1e-12]\n",
    "x = 3.0\n",
    "analytical = f_prime_analytical(x)\n",
    "\n",
    "forward_errors = []\n",
    "central_errors = []\n",
    "\n",
    "for h in h_values:\n",
    "    forward_errors.append(abs(numerical_diff_forward(f, x, h) - analytical))\n",
    "    central_errors.append(abs(numerical_diff_central(f, x, h) - analytical))\n",
    "\n",
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.loglog(h_values, forward_errors, 'o-', label='前進差分', linewidth=2, markersize=8)\n",
    "ax.loglog(h_values, central_errors, 's-', label='中心差分', linewidth=2, markersize=8)\n",
    "\n",
    "# 理論的な誤差オーダーの参照線\n",
    "ax.loglog(h_values[:6], [h * 10 for h in h_values[:6]], '--', alpha=0.5, label='O(h) 参照')\n",
    "ax.loglog(h_values[:6], [h**2 * 100 for h in h_values[:6]], '--', alpha=0.5, label='O(h²) 参照')\n",
    "\n",
    "ax.set_xlabel('h（刻み幅）', fontsize=12)\n",
    "ax.set_ylabel('絶対誤差', fontsize=12)\n",
    "ax.set_title('数値微分の精度：h を小さくすると何が起きるか？', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(1e-13, 1)\n",
    "ax.set_ylim(1e-12, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n【重要な観察】\")\n",
    "print(\"1. 中心差分は前進差分より一貫して精度が高い\")\n",
    "print(\"2. h が小さすぎると（< 1e-8）、浮動小数点の丸め誤差で精度が悪化する\")\n",
    "print(\"3. 実用的には h = 1e-5 ~ 1e-7 程度が良いバランス\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 接線の可視化：微分の幾何学的意味\n",
    "\n",
    "微分係数 $f'(a)$ は、点 $(a, f(a))$ における接線の **傾き** です。\n",
    "\n",
    "接線の方程式：\n",
    "$$\n",
    "y = f'(a)(x - a) + f(a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tangent_line(f, f_prime, a, x_range=(-3, 3), title=\"関数と接線\"):\n",
    "    \"\"\"関数とその接線を可視化する\"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], 200)\n",
    "    y = f(x)\n",
    "    \n",
    "    # 接線の計算\n",
    "    slope = f_prime(a)\n",
    "    tangent = slope * (x - a) + f(a)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # 関数のプロット\n",
    "    ax.plot(x, y, 'b-', linewidth=2.5, label=f'$f(x)$')\n",
    "    \n",
    "    # 接線のプロット\n",
    "    ax.plot(x, tangent, 'r--', linewidth=2, label=f'接線（傾き = {slope:.2f}）')\n",
    "    \n",
    "    # 接点のマーク\n",
    "    ax.plot(a, f(a), 'go', markersize=12, label=f'接点 ({a}, {f(a):.2f})', zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel('y', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(min(y) - 1, max(y) + 1)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# f(x) = x^2 の接線を複数点で描画\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, a in enumerate([-2, 0, 1.5]):\n",
    "    x = np.linspace(-3, 3, 200)\n",
    "    y = x ** 2\n",
    "    slope = 2 * a  # f'(x) = 2x\n",
    "    tangent = slope * (x - a) + a**2\n",
    "    \n",
    "    axes[idx].plot(x, y, 'b-', linewidth=2.5, label='$f(x) = x^2$')\n",
    "    axes[idx].plot(x, tangent, 'r--', linewidth=2, label=f'接線 (傾き={slope:.1f})')\n",
    "    axes[idx].plot(a, a**2, 'go', markersize=12, zorder=5)\n",
    "    axes[idx].set_xlabel('x')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].set_title(f'x = {a} での接線', fontsize=12)\n",
    "    axes[idx].legend(fontsize=9)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim(-2, 10)\n",
    "    axes[idx].set_xlim(-3, 3)\n",
    "\n",
    "plt.suptitle('$f(x) = x^2$ の各点における接線', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察ポイント】\")\n",
    "print(\"・x = -2: 傾き -4（左下がり）→ x を増やすと f(x) は減少\")\n",
    "print(\"・x = 0:  傾き 0（水平）    → 極小点\")\n",
    "print(\"・x = 1.5: 傾き 3（右上がり）→ x を増やすと f(x) は増加\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 解析的微分 vs 数値微分\n",
    "\n",
    "### 2.1 解析的微分\n",
    "\n",
    "数学的な公式を使って導関数を求める方法です。\n",
    "\n",
    "**基本的な微分公式：**\n",
    "\n",
    "| 関数 $f(x)$ | 導関数 $f'(x)$ |\n",
    "|-------------|----------------|\n",
    "| $x^n$ | $nx^{n-1}$ |\n",
    "| $e^x$ | $e^x$ |\n",
    "| $\\ln(x)$ | $1/x$ |\n",
    "| $\\sin(x)$ | $\\cos(x)$ |\n",
    "| $\\cos(x)$ | $-\\sin(x)$ |\n",
    "\n",
    "### 2.2 ニューラルネットワークでよく使う関数の微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シグモイド関数とその導関数\n",
    "def sigmoid(x):\n",
    "    \"\"\"シグモイド関数: σ(x) = 1 / (1 + e^(-x))\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"シグモイドの導関数: σ'(x) = σ(x)(1 - σ(x))\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "# ReLU関数とその導関数\n",
    "def relu(x):\n",
    "    \"\"\"ReLU関数: max(0, x)\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"ReLUの導関数: 1 if x > 0 else 0\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "\n",
    "# tanh関数とその導関数\n",
    "def tanh(x):\n",
    "    \"\"\"tanh関数\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"tanhの導関数: 1 - tanh²(x)\"\"\"\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "\n",
    "# 可視化\n",
    "x = np.linspace(-5, 5, 500)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# シグモイド\n",
    "axes[0, 0].plot(x, sigmoid(x), 'b-', linewidth=2.5)\n",
    "axes[0, 0].set_title('Sigmoid: $\\\\sigma(x) = \\\\frac{1}{1+e^{-x}}$', fontsize=12)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "axes[1, 0].plot(x, sigmoid_derivative(x), 'r-', linewidth=2.5)\n",
    "axes[1, 0].set_title(\"$\\\\sigma'(x) = \\\\sigma(x)(1-\\\\sigma(x))$\", fontsize=12)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0.25, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].text(3, 0.26, '最大値 = 0.25', fontsize=10)\n",
    "\n",
    "# ReLU\n",
    "axes[0, 1].plot(x, relu(x), 'b-', linewidth=2.5)\n",
    "axes[0, 1].set_title('ReLU: $\\\\max(0, x)$', fontsize=12)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(x, relu_derivative(x), 'r-', linewidth=2.5)\n",
    "axes[1, 1].set_title(\"ReLU': 1 (x>0), 0 (x≤0)\", fontsize=12)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(-0.1, 1.5)\n",
    "\n",
    "# tanh\n",
    "axes[0, 2].plot(x, tanh(x), 'b-', linewidth=2.5)\n",
    "axes[0, 2].set_title('tanh: $\\\\frac{e^x - e^{-x}}{e^x + e^{-x}}$', fontsize=12)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "axes[0, 2].set_ylim(-1.2, 1.2)\n",
    "\n",
    "axes[1, 2].plot(x, tanh_derivative(x), 'r-', linewidth=2.5)\n",
    "axes[1, 2].set_title(\"$\\\\tanh'(x) = 1 - \\\\tanh^2(x)$\", fontsize=12)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('x')\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "axes[0, 0].set_ylabel('f(x)')\n",
    "axes[1, 0].set_ylabel(\"f'(x)\")\n",
    "\n",
    "plt.suptitle('活性化関数とその導関数', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【重要な観察】\")\n",
    "print(\"・Sigmoid の導関数は最大でも 0.25 → 深い層で勾配が消失しやすい\")\n",
    "print(\"・ReLU の導関数は 0 か 1 → 勾配がそのまま伝播（ただし負の入力では死ぬ）\")\n",
    "print(\"・tanh の導関数は最大 1 → Sigmoid より勾配消失しにくい\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 数値微分で検証する\n",
    "\n",
    "解析的に求めた導関数が正しいか、数値微分で検証してみましょう。これは **勾配チェック（Gradient Checking）** と呼ばれ、ニューラルネットワークの実装で非常に重要な検証手法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(f, f_prime, test_points, h=1e-5, tol=1e-6):\n",
    "    \"\"\"解析的な導関数を数値微分で検証する\"\"\"\n",
    "    print(f\"{'x':>8} | {'解析的':>12} | {'数値微分':>12} | {'相対誤差':>12} | 結果\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    all_passed = True\n",
    "    for x in test_points:\n",
    "        analytical = f_prime(x)\n",
    "        numerical = numerical_diff_central(f, x, h)\n",
    "        \n",
    "        # 相対誤差の計算（ゼロ除算を避ける）\n",
    "        if abs(analytical) > 1e-10:\n",
    "            relative_error = abs(analytical - numerical) / abs(analytical)\n",
    "        else:\n",
    "            relative_error = abs(analytical - numerical)\n",
    "        \n",
    "        passed = relative_error < tol\n",
    "        all_passed = all_passed and passed\n",
    "        status = \"✓ OK\" if passed else \"✗ NG\"\n",
    "        \n",
    "        print(f\"{x:>8.2f} | {analytical:>12.6f} | {numerical:>12.6f} | {relative_error:>12.2e} | {status}\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "\n",
    "# シグモイド関数の勾配チェック\n",
    "print(\"【Sigmoid関数の勾配チェック】\\n\")\n",
    "test_points = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "gradient_check(sigmoid, sigmoid_derivative, test_points)\n",
    "\n",
    "print(\"\\n\" + \"=\"*65 + \"\\n\")\n",
    "\n",
    "# tanh関数の勾配チェック\n",
    "print(\"【tanh関数の勾配チェック】\\n\")\n",
    "gradient_check(tanh, tanh_derivative, test_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 多変数への拡張：偏微分\n",
    "\n",
    "### 3.1 多変数関数とは\n",
    "\n",
    "ニューラルネットワークは **多数のパラメータ（重み）** を持ちます。損失関数はこれらすべてのパラメータの関数です：\n",
    "\n",
    "$$\n",
    "L = L(w_1, w_2, \\ldots, w_n)\n",
    "$$\n",
    "\n",
    "このような多変数関数を微分するには、**偏微分** を使います。\n",
    "\n",
    "### 3.2 偏微分の定義\n",
    "\n",
    "関数 $f(x, y)$ の $x$ に関する偏微分は、**$y$ を固定して** $x$ だけを変化させたときの変化率です：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x + h, y) - f(x, y)}{h}\n",
    "$$\n",
    "\n",
    "同様に、$y$ に関する偏微分は：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = \\lim_{h \\to 0} \\frac{f(x, y + h) - f(x, y)}{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2変数関数の例: f(x, y) = x² + y²\n",
    "def f_2d(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "\n",
    "# 解析的な偏微分\n",
    "def df_dx(x, y):\n",
    "    \"\"\"∂f/∂x = 2x\"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "def df_dy(x, y):\n",
    "    \"\"\"∂f/∂y = 2y\"\"\"\n",
    "    return 2 * y\n",
    "\n",
    "\n",
    "# 数値偏微分\n",
    "def numerical_partial_x(f, x, y, h=1e-5):\n",
    "    \"\"\"xに関する数値偏微分\"\"\"\n",
    "    return (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "\n",
    "\n",
    "def numerical_partial_y(f, x, y, h=1e-5):\n",
    "    \"\"\"yに関する数値偏微分\"\"\"\n",
    "    return (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "\n",
    "\n",
    "# テスト\n",
    "test_point = (3.0, 4.0)\n",
    "x, y = test_point\n",
    "\n",
    "print(f\"f(x, y) = x² + y² の点 ({x}, {y}) における偏微分\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n∂f/∂x:\")\n",
    "print(f\"  解析的: {df_dx(x, y):.6f}\")\n",
    "print(f\"  数値:   {numerical_partial_x(f_2d, x, y):.6f}\")\n",
    "print(f\"\\n∂f/∂y:\")\n",
    "print(f\"  解析的: {df_dy(x, y):.6f}\")\n",
    "print(f\"  数値:   {numerical_partial_y(f_2d, x, y):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 等高線図で偏微分を可視化\n",
    "\n",
    "等高線図は、3次元の関数を2次元で表現する方法です。偏微分の幾何学的意味を理解しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 等高線図と偏微分の方向を可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# メッシュグリッドの作成\n",
    "x_range = np.linspace(-4, 4, 100)\n",
    "y_range = np.linspace(-4, 4, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f_2d(X, Y)\n",
    "\n",
    "# 左: 等高線図\n",
    "contour = axes[0].contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "axes[0].clabel(contour, inline=True, fontsize=8)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('$f(x,y) = x^2 + y^2$ の等高線図', fontsize=12)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 特定の点での偏微分を矢印で表示\n",
    "point = (2.0, 1.5)\n",
    "axes[0].plot(*point, 'ro', markersize=12, label=f'点 {point}')\n",
    "\n",
    "# ∂f/∂x の方向（x方向のみの変化）\n",
    "scale = 0.3\n",
    "axes[0].annotate('', xy=(point[0] + scale * df_dx(*point), point[1]),\n",
    "                 xytext=point, arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "axes[0].text(point[0] + 0.3, point[1] - 0.5, f'∂f/∂x = {df_dx(*point):.1f}', \n",
    "             color='red', fontsize=11)\n",
    "\n",
    "# ∂f/∂y の方向（y方向のみの変化）\n",
    "axes[0].annotate('', xy=(point[0], point[1] + scale * df_dy(*point)),\n",
    "                 xytext=point, arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "axes[0].text(point[0] - 1.5, point[1] + 0.8, f'∂f/∂y = {df_dy(*point):.1f}', \n",
    "             color='blue', fontsize=11)\n",
    "\n",
    "axes[0].legend(loc='upper left')\n",
    "\n",
    "# 右: xを固定してyを変化させたときのスライス\n",
    "x_fixed = 2.0\n",
    "y_vals = np.linspace(-4, 4, 100)\n",
    "z_slice = f_2d(x_fixed, y_vals)\n",
    "\n",
    "axes[1].plot(y_vals, z_slice, 'b-', linewidth=2.5, label=f'$f({x_fixed}, y) = {x_fixed}^2 + y^2$')\n",
    "\n",
    "# 接線を描画\n",
    "y_point = 1.5\n",
    "slope = df_dy(x_fixed, y_point)\n",
    "tangent = slope * (y_vals - y_point) + f_2d(x_fixed, y_point)\n",
    "axes[1].plot(y_vals, tangent, 'r--', linewidth=2, label=f'接線（傾き = {slope:.1f}）')\n",
    "axes[1].plot(y_point, f_2d(x_fixed, y_point), 'go', markersize=12)\n",
    "\n",
    "axes[1].set_xlabel('y')\n",
    "axes[1].set_ylabel('f')\n",
    "axes[1].set_title(f'x = {x_fixed} で固定したときのスライス', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(-2, 25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【偏微分の意味】\")\n",
    "print(\"・∂f/∂x: 『y を固定して x だけ動かしたとき、f がどれだけ変化するか』\")\n",
    "print(\"・∂f/∂y: 『x を固定して y だけ動かしたとき、f がどれだけ変化するか』\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 勾配ベクトル：最急上昇方向\n",
    "\n",
    "### 4.1 勾配の定義\n",
    "\n",
    "偏微分を並べてベクトルにしたものを **勾配（Gradient）** と呼びます：\n",
    "\n",
    "$$\n",
    "\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\n",
    "$$\n",
    "\n",
    "### 4.2 勾配の重要な性質\n",
    "\n",
    "**勾配ベクトルは、関数が最も急に増加する方向を指す**\n",
    "\n",
    "これがニューラルネットワークの学習で本質的に重要な性質です：\n",
    "\n",
    "- 損失関数 $L$ を **減少** させたい\n",
    "- 勾配 $\\nabla L$ は $L$ が **増加** する方向を指す\n",
    "- したがって、**勾配の逆方向（$-\\nabla L$）** に進めば $L$ は減少する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(f, x, y, h=1e-5):\n",
    "    \"\"\"数値微分で勾配を計算\"\"\"\n",
    "    grad_x = (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "    grad_y = (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "    return np.array([grad_x, grad_y])\n",
    "\n",
    "\n",
    "# 勾配場（Gradient Field）の可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 左: 勾配ベクトル場\n",
    "x_range = np.linspace(-3, 3, 15)\n",
    "y_range = np.linspace(-3, 3, 15)\n",
    "X_grid, Y_grid = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# 各点での勾配を計算\n",
    "U = 2 * X_grid  # ∂f/∂x = 2x\n",
    "V = 2 * Y_grid  # ∂f/∂y = 2y\n",
    "\n",
    "# 等高線\n",
    "X_fine, Y_fine = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "Z_fine = f_2d(X_fine, Y_fine)\n",
    "contour = axes[0].contour(X_fine, Y_fine, Z_fine, levels=15, cmap='coolwarm', alpha=0.6)\n",
    "\n",
    "# 勾配ベクトル場（矢印）\n",
    "axes[0].quiver(X_grid, Y_grid, U, V, color='blue', alpha=0.7, scale=50)\n",
    "\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('勾配ベクトル場：矢印は f が最も急に増加する方向', fontsize=12)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 右: 負の勾配（勾配降下の方向）\n",
    "contour2 = axes[1].contour(X_fine, Y_fine, Z_fine, levels=15, cmap='coolwarm', alpha=0.6)\n",
    "\n",
    "# 負の勾配ベクトル場\n",
    "axes[1].quiver(X_grid, Y_grid, -U, -V, color='green', alpha=0.7, scale=50)\n",
    "\n",
    "# 最小点をマーク\n",
    "axes[1].plot(0, 0, 'r*', markersize=20, label='最小点 (0, 0)')\n",
    "\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('負の勾配：f を減少させる方向（最小点へ向かう）', fontsize=12)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【核心的な洞察】\")\n",
    "print(\"・勾配ベクトルはすべて外向き（原点から離れる方向）= f が増加する方向\")\n",
    "print(\"・負の勾配はすべて内向き（原点に向かう方向）= f が減少する方向\")\n",
    "print(\"・これが『勾配降下法』の基本原理\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 3D曲面上での勾配の意味\n",
    "\n",
    "勾配の大きさは「傾斜の急さ」を表し、方向は「最も急な上り坂」を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D曲面と勾配の可視化\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3D曲面\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "X = np.linspace(-3, 3, 50)\n",
    "Y = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = f_2d(X, Y)\n",
    "\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')\n",
    "\n",
    "# 特定の点とその勾配方向を表示\n",
    "point = (2.0, 1.0)\n",
    "z_point = f_2d(*point)\n",
    "ax1.scatter(*point, z_point, color='red', s=100, label=f'点 {point}')\n",
    "\n",
    "# 勾配の方向を曲面上に投影\n",
    "grad = np.array([2 * point[0], 2 * point[1]])  # 解析的な勾配\n",
    "grad_norm = np.linalg.norm(grad)\n",
    "grad_unit = grad / grad_norm * 0.5  # 単位ベクトルにスケーリング\n",
    "\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('$f(x,y) = x^2 + y^2$ の3D曲面', fontsize=12)\n",
    "ax1.view_init(elev=30, azim=45)\n",
    "\n",
    "# 2D等高線図と勾配\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "X_fine = np.linspace(-3, 3, 100)\n",
    "Y_fine = np.linspace(-3, 3, 100)\n",
    "X_fine, Y_fine = np.meshgrid(X_fine, Y_fine)\n",
    "Z_fine = f_2d(X_fine, Y_fine)\n",
    "\n",
    "contour = ax2.contourf(X_fine, Y_fine, Z_fine, levels=20, cmap='viridis', alpha=0.8)\n",
    "plt.colorbar(contour, ax=ax2, label='f(x,y)')\n",
    "\n",
    "# 勾配ベクトルを描画\n",
    "ax2.arrow(point[0], point[1], grad_unit[0], grad_unit[1],\n",
    "          head_width=0.15, head_length=0.1, fc='red', ec='red', linewidth=2,\n",
    "          label=f'勾配 ∇f = ({grad[0]:.1f}, {grad[1]:.1f})')\n",
    "\n",
    "# 負の勾配（降下方向）\n",
    "ax2.arrow(point[0], point[1], -grad_unit[0], -grad_unit[1],\n",
    "          head_width=0.15, head_length=0.1, fc='lime', ec='lime', linewidth=2,\n",
    "          label='降下方向 -∇f')\n",
    "\n",
    "ax2.plot(*point, 'ko', markersize=10)\n",
    "ax2.plot(0, 0, 'w*', markersize=15, markeredgecolor='black')\n",
    "ax2.text(0.1, 0.3, '最小点', fontsize=10, color='white')\n",
    "\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('等高線図と勾配ベクトル', fontsize=12)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.legend(loc='upper left', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"点 {point} での勾配: ∇f = ({grad[0]:.1f}, {grad[1]:.1f})\")\n",
    "print(f\"勾配の大きさ |∇f| = {grad_norm:.2f}（傾斜の急さ）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 勾配降下法への橋渡し\n",
    "\n",
    "### 5.1 最適化問題としてのニューラルネットワーク学習\n",
    "\n",
    "ニューラルネットワークの学習は、次の最適化問題を解くことに帰着します：\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}} L(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "ここで：\n",
    "- $\\mathbf{w}$：ネットワークの全パラメータ（重みとバイアス）\n",
    "- $L$：損失関数（予測と正解のズレを測る）\n",
    "\n",
    "### 5.2 勾配降下法のアルゴリズム\n",
    "\n",
    "1. 現在のパラメータ $\\mathbf{w}$ で勾配 $\\nabla L(\\mathbf{w})$ を計算\n",
    "2. 勾配の逆方向に少しだけ移動：\n",
    "   $$\\mathbf{w}_{\\text{new}} = \\mathbf{w} - \\eta \\nabla L(\\mathbf{w})$$\n",
    "3. 収束するまで繰り返す\n",
    "\n",
    "$\\eta$ は **学習率（Learning Rate）** と呼ばれるハイパーパラメータです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_2d(f, grad_f, start, learning_rate=0.1, n_iterations=50):\n",
    "    \"\"\"2変数関数の勾配降下法\"\"\"\n",
    "    path = [np.array(start)]\n",
    "    current = np.array(start, dtype=float)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        grad = grad_f(current[0], current[1])\n",
    "        current = current - learning_rate * grad\n",
    "        path.append(current.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "def analytical_gradient(x, y):\n",
    "    \"\"\"f(x,y) = x² + y² の勾配\"\"\"\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "\n",
    "# 異なる学習率での軌跡を比較\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "start_point = (2.5, 2.0)\n",
    "\n",
    "# 等高線の準備\n",
    "X = np.linspace(-3, 3, 100)\n",
    "Y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = f_2d(X, Y)\n",
    "\n",
    "for ax, lr in zip(axes, learning_rates):\n",
    "    # 等高線\n",
    "    contour = ax.contour(X, Y, Z, levels=15, cmap='viridis', alpha=0.6)\n",
    "    \n",
    "    # 勾配降下の軌跡\n",
    "    path = gradient_descent_2d(f_2d, analytical_gradient, start_point, \n",
    "                               learning_rate=lr, n_iterations=30)\n",
    "    \n",
    "    # 軌跡をプロット\n",
    "    ax.plot(path[:, 0], path[:, 1], 'ro-', markersize=4, linewidth=1.5, alpha=0.8)\n",
    "    ax.plot(path[0, 0], path[0, 1], 'g^', markersize=12, label='開始点')\n",
    "    ax.plot(path[-1, 0], path[-1, 1], 'bs', markersize=10, label='終了点')\n",
    "    ax.plot(0, 0, 'k*', markersize=15, label='最小点')\n",
    "    \n",
    "    # 各ステップでの損失値を計算\n",
    "    final_loss = f_2d(path[-1, 0], path[-1, 1])\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'学習率 η = {lr}\\n(最終損失: {final_loss:.4f})', fontsize=12)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.suptitle('勾配降下法：学習率による収束速度の違い', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【学習率の影響】\")\n",
    "print(\"・η = 0.01: ゆっくり収束（安定だが遅い）\")\n",
    "print(\"・η = 0.1:  バランスが良い\")\n",
    "print(\"・η = 0.5:  速いが、大きすぎると発散の危険性\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線（損失の推移）を可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for lr in [0.01, 0.05, 0.1, 0.3, 0.5]:\n",
    "    path = gradient_descent_2d(f_2d, analytical_gradient, start_point,\n",
    "                               learning_rate=lr, n_iterations=50)\n",
    "    losses = [f_2d(p[0], p[1]) for p in path]\n",
    "    ax.plot(losses, label=f'η = {lr}', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('イテレーション', fontsize=12)\n",
    "ax.set_ylabel('損失 L(w)', fontsize=12)\n",
    "ax.set_title('学習曲線：損失関数の減少', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【対数スケールで見る収束】\")\n",
    "print(\"・適切な学習率では、損失が指数関数的に減少\")\n",
    "print(\"・学習率が大きいほど最初の減少は速いが、振動のリスクも\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 演習問題\n",
    "\n",
    "### 演習 6.1: 数値微分の実装確認\n",
    "\n",
    "以下の関数について、数値微分と解析的微分を比較してください。\n",
    "\n",
    "$$\n",
    "f(x) = \\sin(x) \\cdot e^{-x^2/2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 6.1: 解答欄\n",
    "\n",
    "def exercise_f(x):\n",
    "    \"\"\"f(x) = sin(x) * exp(-x²/2)\"\"\"\n",
    "    # TODO: 実装してください\n",
    "    pass\n",
    "\n",
    "\n",
    "def exercise_f_prime(x):\n",
    "    \"\"\"f'(x) の解析的な導関数（積の微分法則を使用）\"\"\"\n",
    "    # ヒント: (fg)' = f'g + fg'\n",
    "    # f = sin(x), f' = cos(x)\n",
    "    # g = exp(-x²/2), g' = -x * exp(-x²/2)\n",
    "    # TODO: 実装してください\n",
    "    pass\n",
    "\n",
    "\n",
    "# テストコード（実装後にコメントを外して実行）\n",
    "# test_points = np.array([-2.0, -1.0, 0.0, 0.5, 1.0, 2.0])\n",
    "# gradient_check(exercise_f, exercise_f_prime, test_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 6.2: 2変数関数の勾配降下\n",
    "\n",
    "以下の **ローゼンブロック関数**（最適化のベンチマークとして有名）に対して勾配降下法を適用してください。\n",
    "\n",
    "$$\n",
    "f(x, y) = (1 - x)^2 + 100(y - x^2)^2\n",
    "$$\n",
    "\n",
    "最小値は $(x, y) = (1, 1)$ で $f = 0$ です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 6.2: 解答欄\n",
    "\n",
    "def rosenbrock(x, y):\n",
    "    \"\"\"ローゼンブロック関数\"\"\"\n",
    "    # TODO: 実装してください\n",
    "    pass\n",
    "\n",
    "\n",
    "def rosenbrock_gradient(x, y):\n",
    "    \"\"\"ローゼンブロック関数の勾配\"\"\"\n",
    "    # ∂f/∂x = -2(1-x) - 400x(y - x²)\n",
    "    # ∂f/∂y = 200(y - x²)\n",
    "    # TODO: 実装してください\n",
    "    pass\n",
    "\n",
    "\n",
    "# 勾配降下法を試す\n",
    "# start_point = (-1.0, 1.0)\n",
    "# path = gradient_descent_2d(rosenbrock, rosenbrock_gradient, start_point,\n",
    "#                            learning_rate=0.001, n_iterations=5000)\n",
    "\n",
    "# 可視化コード（実装後にコメントを外して実行）\n",
    "# X = np.linspace(-2, 2, 100)\n",
    "# Y = np.linspace(-1, 3, 100)\n",
    "# X, Y = np.meshgrid(X, Y)\n",
    "# Z = rosenbrock(X, Y)\n",
    "# plt.contour(X, Y, np.log(Z + 1), levels=30)\n",
    "# plt.plot(path[:, 0], path[:, 1], 'r.-', alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 6.3: シグモイドの飽和問題\n",
    "\n",
    "シグモイド関数の導関数が、入力 $x$ が大きい（または小さい）ときにほぼ 0 になることを確認してください。これが「勾配消失」の原因の一つです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 6.3: 解答欄\n",
    "\n",
    "# シグモイドの導関数値を様々な入力で計算\n",
    "x_values = [-10, -5, -2, -1, 0, 1, 2, 5, 10]\n",
    "\n",
    "print(\"x の値とシグモイドの導関数 σ'(x) = σ(x)(1-σ(x))\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# TODO: 各 x_values に対して sigmoid_derivative を計算し、\n",
    "# 値が極端に小さくなる様子を確認してください\n",
    "\n",
    "# for x in x_values:\n",
    "#     deriv = sigmoid_derivative(x)\n",
    "#     print(f\"x = {x:>4}: σ'(x) = {deriv:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **数値微分**：$f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}$ で任意の関数の微分を近似できる\n",
    "\n",
    "2. **偏微分**：多変数関数の各変数に対する変化率\n",
    "\n",
    "3. **勾配ベクトル**：偏微分を並べたベクトル $\\nabla f = (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})$\n",
    "\n",
    "4. **勾配の方向**：関数が最も急に増加する方向を指す\n",
    "\n",
    "5. **勾配降下法**：$\\mathbf{w}_{\\text{new}} = \\mathbf{w} - \\eta \\nabla L$ で損失を最小化\n",
    "\n",
    "### ニューラルネットワークとの接続\n",
    "\n",
    "| 概念 | ニューラルネットワークでの役割 |\n",
    "|------|-------------------------------|\n",
    "| 関数 $f$ | モデルの出力（予測値） |\n",
    "| 変数 $x, y, \\ldots$ | 重み $w_1, w_2, \\ldots$ とバイアス |\n",
    "| 損失関数 $L$ | 予測と正解のズレ（MSE, Cross Entropyなど） |\n",
    "| 勾配 $\\nabla L$ | 各パラメータの「責任度」 |\n",
    "| 勾配降下 | パラメータの更新（学習） |\n",
    "\n",
    "### 次のノートブック（71: 連鎖律の解剖）への橋渡し\n",
    "\n",
    "実際のニューラルネットワークでは、関数が **何層にも重なった合成関数** になっています：\n",
    "\n",
    "$$\n",
    "y = f_3(f_2(f_1(x)))\n",
    "$$\n",
    "\n",
    "このような合成関数の微分には **連鎖律（Chain Rule）** が必要です。次のノートブックでは：\n",
    "\n",
    "- 連鎖律の数学的導出\n",
    "- 複雑な合成関数の手計算練習\n",
    "- 計算グラフ表現への準備\n",
    "\n",
    "を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Chapter 4: Numerical Computation\n",
    "2. Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer. Appendix D: Calculus of Variations\n",
    "3. 斎藤康毅 (2016). 『ゼロから作るDeep Learning』. O'Reilly Japan. 第4章: ニューラルネットワークの学習"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
