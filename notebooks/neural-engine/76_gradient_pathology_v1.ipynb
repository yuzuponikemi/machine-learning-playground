{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 76: 勾配の病理学 ― 消失・爆発・鞍点\n",
    "\n",
    "## Gradient Pathology: Vanishing, Exploding, and Saddle Points\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Unit 0.0「ニューラルエンジンの深部」** の最終章として、学習が失敗するメカニズムを理解し、診断・対策の方法を学びます。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **勾配消失問題** のメカニズムと対策を理解する\n",
    "2. **勾配爆発問題** のメカニズムと対策を理解する\n",
    "3. **鞍点と局所解** の概念を理解する\n",
    "4. **勾配のモニタリング** ツールを実装する\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 70-75 の内容\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [勾配消失問題](#1-勾配消失問題)\n",
    "2. [勾配爆発問題](#2-勾配爆発問題)\n",
    "3. [活性化関数の選択](#3-活性化関数の選択)\n",
    "4. [重みの初期化](#4-重みの初期化)\n",
    "5. [鞍点と局所解](#5-鞍点と局所解)\n",
    "6. [勾配モニタリングツール](#6-勾配モニタリングツール)\n",
    "7. [最適化手法の比較](#7-最適化手法の比較)\n",
    "8. [演習問題](#8-演習問題)\n",
    "9. [Unit 0.0 総まとめ](#9-unit-00-総まとめ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前のノートブックのクラスを再定義\n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        self.cache = {}\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x): pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, dout): pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_features, out_features, init='xavier'):\n",
    "        super().__init__()\n",
    "        if init == 'xavier':\n",
    "            scale = np.sqrt(2.0 / (in_features + out_features))\n",
    "        elif init == 'he':\n",
    "            scale = np.sqrt(2.0 / in_features)\n",
    "        elif init == 'normal':\n",
    "            scale = 0.01\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        \n",
    "        self.params['W'] = np.random.randn(in_features, out_features) * scale\n",
    "        self.params['b'] = np.zeros(out_features)\n",
    "        self.grads['W'] = None\n",
    "        self.grads['b'] = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache['x'] = x\n",
    "        return x @ self.params['W'] + self.params['b']\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x = self.cache['x']\n",
    "        self.grads['W'] = x.T @ dout\n",
    "        self.grads['b'] = np.sum(dout, axis=0)\n",
    "        return dout @ self.params['W'].T\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, x):\n",
    "        y = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        self.cache['y'] = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        y = self.cache['y']\n",
    "        return dout * y * (1 - y)\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, x):\n",
    "        self.cache['mask'] = (x > 0).astype(float)\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.cache['mask']\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def forward(self, x):\n",
    "        y = np.tanh(x)\n",
    "        self.cache['y'] = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        y = self.cache['y']\n",
    "        return dout * (1 - y ** 2)\n",
    "\n",
    "\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, y, t):\n",
    "        self.cache['y'] = y\n",
    "        self.cache['t'] = t\n",
    "        self.cache['N'] = y.shape[0]\n",
    "        return np.mean((y - t) ** 2)\n",
    "    \n",
    "    def backward(self):\n",
    "        y, t, N = self.cache['y'], self.cache['t'], self.cache['N']\n",
    "        return (2 / N) * (y - t)\n",
    "\n",
    "\n",
    "print(\"Layer クラス群を定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 勾配消失問題\n",
    "\n",
    "### 1.1 問題の概要\n",
    "\n",
    "深いネットワークでは、逆伝播中に勾配が層を通過するたびに **小さくなっていく** 問題が発生します。\n",
    "\n",
    "**原因**：シグモイドやtanhの導関数は最大でも1未満\n",
    "- $\\sigma'(x) \\leq 0.25$（シグモイド）\n",
    "- $\\tanh'(x) \\leq 1$（tanh）\n",
    "\n",
    "n層のネットワークでは、勾配が $0.25^n$ 倍に減衰する可能性があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シグモイドの導関数の最大値を確認\n",
    "x = np.linspace(-6, 6, 1000)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "sigmoid_deriv = sigmoid * (1 - sigmoid)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# シグモイドと導関数\n",
    "axes[0].plot(x, sigmoid, label=r'$\\sigma(x)$', linewidth=2)\n",
    "axes[0].plot(x, sigmoid_deriv, label=r\"$\\sigma'(x)$\", linewidth=2)\n",
    "axes[0].axhline(y=0.25, color='red', linestyle='--', alpha=0.7, label='最大値 0.25')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('シグモイド関数とその導関数')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 勾配の減衰\n",
    "n_layers = np.arange(1, 21)\n",
    "gradient_decay_025 = 0.25 ** n_layers  # 最悪ケース\n",
    "gradient_decay_05 = 0.5 ** n_layers    # 中間ケース\n",
    "\n",
    "axes[1].semilogy(n_layers, gradient_decay_025, 'o-', label=r'$0.25^n$ (最悪)', linewidth=2)\n",
    "axes[1].semilogy(n_layers, gradient_decay_05, 's-', label=r'$0.5^n$', linewidth=2)\n",
    "axes[1].set_xlabel('層の数')\n",
    "axes[1].set_ylabel('勾配の大きさ（対数スケール）')\n",
    "axes[1].set_title('勾配消失: 層が深くなるほど勾配が減衰')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"10層の場合: 勾配は最大 {0.25**10:.2e} 倍に減衰\")\n",
    "print(f\"20層の場合: 勾配は最大 {0.25**20:.2e} 倍に減衰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 深いネットワークでの実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_network(n_layers, activation='sigmoid', init='normal'):\n",
    "    \"\"\"深いネットワークを構築\"\"\"\n",
    "    layers = []\n",
    "    act_class = {'sigmoid': Sigmoid, 'relu': ReLU, 'tanh': Tanh}[activation]\n",
    "    \n",
    "    # 入力層\n",
    "    layers.append(Linear(2, 10, init=init))\n",
    "    layers.append(act_class())\n",
    "    \n",
    "    # 隠れ層\n",
    "    for _ in range(n_layers - 2):\n",
    "        layers.append(Linear(10, 10, init=init))\n",
    "        layers.append(act_class())\n",
    "    \n",
    "    # 出力層\n",
    "    layers.append(Linear(10, 1, init=init))\n",
    "    \n",
    "    return layers\n",
    "\n",
    "\n",
    "def forward_backward(layers, X, t):\n",
    "    \"\"\"順伝播・逆伝播を実行し、各層の勾配ノルムを記録\"\"\"\n",
    "    loss_fn = MSELoss()\n",
    "    \n",
    "    # 順伝播\n",
    "    h = X\n",
    "    for layer in layers:\n",
    "        h = layer.forward(h)\n",
    "    \n",
    "    loss = loss_fn.forward(h, t)\n",
    "    \n",
    "    # 逆伝播\n",
    "    dout = loss_fn.backward()\n",
    "    gradient_norms = []\n",
    "    \n",
    "    for layer in reversed(layers):\n",
    "        dout = layer.backward(dout)\n",
    "        if hasattr(layer, 'grads') and layer.grads.get('W') is not None:\n",
    "            grad_norm = np.linalg.norm(layer.grads['W'])\n",
    "            gradient_norms.append(grad_norm)\n",
    "    \n",
    "    return loss, gradient_norms[::-1]  # 入力側から順に\n",
    "\n",
    "\n",
    "# 勾配消失の実験\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(32, 2)\n",
    "t = np.random.randn(32, 1)\n",
    "\n",
    "# シグモイドで10層\n",
    "layers_sigmoid = build_deep_network(10, activation='sigmoid', init='normal')\n",
    "_, grad_norms_sigmoid = forward_backward(layers_sigmoid, X, t)\n",
    "\n",
    "# ReLUで10層\n",
    "np.random.seed(42)\n",
    "layers_relu = build_deep_network(10, activation='relu', init='he')\n",
    "_, grad_norms_relu = forward_backward(layers_relu, X, t)\n",
    "\n",
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "layer_indices = np.arange(1, len(grad_norms_sigmoid) + 1)\n",
    "\n",
    "ax.bar(layer_indices - 0.2, grad_norms_sigmoid, width=0.4, label='Sigmoid', alpha=0.8)\n",
    "ax.bar(layer_indices + 0.2, grad_norms_relu, width=0.4, label='ReLU', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('層番号（入力側から）')\n",
    "ax.set_ylabel('勾配のノルム')\n",
    "ax.set_title('各層の勾配ノルム: Sigmoid vs ReLU')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(f\"Sigmoid: 入力側の勾配 = {grad_norms_sigmoid[0]:.2e}（消失）\")\n",
    "print(f\"ReLU:    入力側の勾配 = {grad_norms_relu[0]:.2e}（維持）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 勾配爆発問題\n",
    "\n",
    "### 2.1 問題の概要\n",
    "\n",
    "勾配消失とは逆に、勾配が **大きくなりすぎる** 問題もあります。\n",
    "\n",
    "**原因**：\n",
    "- 重みの初期値が大きすぎる\n",
    "- 特定のネットワーク構造（RNNなど）\n",
    "\n",
    "**症状**：\n",
    "- 損失が NaN になる\n",
    "- パラメータが発散する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配爆発の実験\n",
    "np.random.seed(42)\n",
    "\n",
    "# 大きな初期値\n",
    "layers_explode = build_deep_network(10, activation='relu', init='large')\n",
    "# 手動で大きな重みを設定\n",
    "for layer in layers_explode:\n",
    "    if hasattr(layer, 'params'):\n",
    "        layer.params['W'] = np.random.randn(*layer.params['W'].shape) * 2.0\n",
    "\n",
    "loss, grad_norms_explode = forward_backward(layers_explode, X, t)\n",
    "\n",
    "# 適切な初期値\n",
    "np.random.seed(42)\n",
    "layers_normal = build_deep_network(10, activation='relu', init='he')\n",
    "_, grad_norms_normal = forward_backward(layers_normal, X, t)\n",
    "\n",
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.bar(layer_indices - 0.2, grad_norms_explode, width=0.4, label='大きい初期値', alpha=0.8, color='red')\n",
    "ax.bar(layer_indices + 0.2, grad_norms_normal, width=0.4, label='He初期化', alpha=0.8, color='green')\n",
    "\n",
    "ax.set_xlabel('層番号（入力側から）')\n",
    "ax.set_ylabel('勾配のノルム（対数スケール）')\n",
    "ax.set_title('勾配爆発: 大きな初期値の危険性')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"大きい初期値: 最大勾配ノルム = {max(grad_norms_explode):.2e}\")\n",
    "print(f\"He初期化:     最大勾配ノルム = {max(grad_norms_normal):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 勾配クリッピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(layers, max_norm=1.0):\n",
    "    \"\"\"\n",
    "    勾配クリッピング: 勾配のノルムが閾値を超えたらスケーリング\n",
    "    \n",
    "    Args:\n",
    "        layers: レイヤーのリスト\n",
    "        max_norm: 最大ノルム\n",
    "    \"\"\"\n",
    "    # 全勾配のノルムを計算\n",
    "    total_norm = 0\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'grads'):\n",
    "            for name, grad in layer.grads.items():\n",
    "                if grad is not None:\n",
    "                    total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # クリッピング\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'grads'):\n",
    "                for name in layer.grads:\n",
    "                    if layer.grads[name] is not None:\n",
    "                        layer.grads[name] *= clip_coef\n",
    "    \n",
    "    return total_norm, min(clip_coef, 1.0)\n",
    "\n",
    "\n",
    "# クリッピングの効果を確認\n",
    "np.random.seed(42)\n",
    "layers_test = build_deep_network(10, activation='relu', init='large')\n",
    "for layer in layers_test:\n",
    "    if hasattr(layer, 'params'):\n",
    "        layer.params['W'] = np.random.randn(*layer.params['W'].shape) * 2.0\n",
    "\n",
    "_, _ = forward_backward(layers_test, X, t)\n",
    "\n",
    "# クリッピング前\n",
    "grad_norms_before = []\n",
    "for layer in layers_test:\n",
    "    if hasattr(layer, 'grads') and layer.grads.get('W') is not None:\n",
    "        grad_norms_before.append(np.linalg.norm(layer.grads['W']))\n",
    "\n",
    "# クリッピング実行\n",
    "total_norm, clip_coef = clip_gradients(layers_test, max_norm=1.0)\n",
    "\n",
    "# クリッピング後\n",
    "grad_norms_after = []\n",
    "for layer in layers_test:\n",
    "    if hasattr(layer, 'grads') and layer.grads.get('W') is not None:\n",
    "        grad_norms_after.append(np.linalg.norm(layer.grads['W']))\n",
    "\n",
    "print(f\"【勾配クリッピング】\")\n",
    "print(f\"クリッピング前の総ノルム: {total_norm:.2f}\")\n",
    "print(f\"クリッピング係数: {clip_coef:.4f}\")\n",
    "print(f\"クリッピング後の最大勾配: {max(grad_norms_after):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 活性化関数の選択\n",
    "\n",
    "### 3.1 各活性化関数の特性比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数の比較\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "\n",
    "activations = {\n",
    "    'Sigmoid': (1 / (1 + np.exp(-x)), lambda y: y * (1 - y)),\n",
    "    'Tanh': (np.tanh(x), lambda y: 1 - y**2),\n",
    "    'ReLU': (np.maximum(0, x), lambda _: (x > 0).astype(float)),\n",
    "    'Leaky ReLU': (np.where(x > 0, x, 0.01 * x), lambda _: np.where(x > 0, 1, 0.01)),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, (name, (y, deriv_fn)) in zip(axes.flat, activations.items()):\n",
    "    deriv = deriv_fn(y)\n",
    "    \n",
    "    ax.plot(x, y, label='f(x)', linewidth=2)\n",
    "    ax.plot(x, deriv, label=\"f'(x)\", linewidth=2, linestyle='--')\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(name)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-2, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【活性化関数の特性】\")\n",
    "print(\"\")\n",
    "print(\"Sigmoid:\")\n",
    "print(\"  - 出力範囲: (0, 1)\")\n",
    "print(\"  - 問題: 勾配消失、計算コスト\")\n",
    "print(\"\")\n",
    "print(\"Tanh:\")\n",
    "print(\"  - 出力範囲: (-1, 1)\")\n",
    "print(\"  - Sigmoidより勾配消失しにくい\")\n",
    "print(\"\")\n",
    "print(\"ReLU:\")\n",
    "print(\"  - 出力範囲: [0, ∞)\")\n",
    "print(\"  - 計算が速い、勾配消失しにくい\")\n",
    "print(\"  - 問題: 死んだニューロン (x < 0 で勾配が0)\")\n",
    "print(\"\")\n",
    "print(\"Leaky ReLU:\")\n",
    "print(\"  - x < 0 でも小さな勾配を保持\")\n",
    "print(\"  - 死んだニューロン問題を軽減\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 重みの初期化\n",
    "\n",
    "### 4.1 初期化手法の比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_activations(layers, X, title):\n",
    "    \"\"\"各層の活性化の分布を分析\"\"\"\n",
    "    activations = []\n",
    "    h = X\n",
    "    \n",
    "    for layer in layers:\n",
    "        h = layer.forward(h)\n",
    "        if isinstance(layer, (Sigmoid, ReLU, Tanh)):\n",
    "            activations.append(h.copy())\n",
    "    \n",
    "    return activations\n",
    "\n",
    "\n",
    "# 異なる初期化での活性化分布\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(1000, 2)\n",
    "\n",
    "init_methods = {\n",
    "    '標準正規 (σ=1)': 'large',\n",
    "    '小さい (σ=0.01)': 'normal',\n",
    "    'Xavier': 'xavier',\n",
    "    'He': 'he',\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, (name, init) in zip(axes.flat, init_methods.items()):\n",
    "    np.random.seed(42)\n",
    "    layers = build_deep_network(5, activation='sigmoid', init=init if init != 'large' else 'normal')\n",
    "    \n",
    "    # 大きな初期値の場合は手動設定\n",
    "    if init == 'large':\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                layer.params['W'] = np.random.randn(*layer.params['W'].shape) * 1.0\n",
    "    \n",
    "    activations = analyze_activations(layers, X_test, name)\n",
    "    \n",
    "    for i, act in enumerate(activations):\n",
    "        ax.hist(act.flatten(), bins=50, alpha=0.5, label=f'層 {i+1}')\n",
    "    \n",
    "    ax.set_xlabel('活性化値')\n",
    "    ax.set_ylabel('頻度')\n",
    "    ax.set_title(f'{name}\\n(std={np.std(activations[-1]):.4f})')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【初期化の指針】\")\n",
    "print(\"- Sigmoid/Tanh: Xavier初期化 (σ = √(2/(n_in + n_out)))\")\n",
    "print(\"- ReLU: He初期化 (σ = √(2/n_in))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 鞍点と局所解\n",
    "\n",
    "### 5.1 損失曲面の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 鞍点を持つ関数の可視化\n",
    "def saddle_function(x, y):\n",
    "    \"\"\"鞍点を持つ関数: f(x,y) = x² - y²\"\"\"\n",
    "    return x**2 - y**2\n",
    "\n",
    "\n",
    "# メッシュグリッド\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "X_mesh, Y_mesh = np.meshgrid(x, y)\n",
    "Z = saddle_function(X_mesh, Y_mesh)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3Dプロット\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.plot_surface(X_mesh, Y_mesh, Z, cmap='coolwarm', alpha=0.8)\n",
    "ax1.scatter([0], [0], [0], color='black', s=100, label='鞍点')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title(r'鞍点: $f(x,y) = x^2 - y^2$')\n",
    "ax1.view_init(elev=25, azim=45)\n",
    "\n",
    "# 等高線\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "contour = ax2.contour(X_mesh, Y_mesh, Z, levels=20, cmap='coolwarm')\n",
    "ax2.scatter([0], [0], color='black', s=100, zorder=5, label='鞍点')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('等高線図')\n",
    "ax2.set_aspect('equal')\n",
    "ax2.legend()\n",
    "plt.colorbar(contour, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【鞍点の特徴】\")\n",
    "print(\"- 勾配がゼロになる点\")\n",
    "print(\"- ある方向では極小、別の方向では極大\")\n",
    "print(\"- 高次元空間では局所解より鞍点の方が多い\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 最適化の軌跡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複雑な損失曲面での最適化\n",
    "def complex_loss(x, y):\n",
    "    \"\"\"複数の極小を持つ関数\"\"\"\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2  # Himmelblau関数\n",
    "\n",
    "\n",
    "def gradient_complex(x, y):\n",
    "    dx = 2 * (x**2 + y - 11) * 2 * x + 2 * (x + y**2 - 7)\n",
    "    dy = 2 * (x**2 + y - 11) + 2 * (x + y**2 - 7) * 2 * y\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "\n",
    "def optimize_trajectory(start, lr=0.01, n_steps=100):\n",
    "    \"\"\"勾配降下の軌跡を記録\"\"\"\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = gradient_complex(pos[0], pos[1])\n",
    "        pos = pos - lr * grad\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "# 等高線\n",
    "x = np.linspace(-5, 5, 200)\n",
    "y = np.linspace(-5, 5, 200)\n",
    "X_mesh, Y_mesh = np.meshgrid(x, y)\n",
    "Z = complex_loss(X_mesh, Y_mesh)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "contour = ax.contour(X_mesh, Y_mesh, np.log(Z + 1), levels=30, cmap='viridis', alpha=0.7)\n",
    "\n",
    "# 異なる開始点からの軌跡\n",
    "start_points = [(-4, -4), (4, 4), (-4, 4), (4, -4), (0, 0)]\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "\n",
    "for start, color in zip(start_points, colors):\n",
    "    path = optimize_trajectory(start, lr=0.01, n_steps=200)\n",
    "    ax.plot(path[:, 0], path[:, 1], '-', color=color, linewidth=2, alpha=0.8)\n",
    "    ax.scatter(path[0, 0], path[0, 1], color=color, s=100, marker='o', edgecolors='black')\n",
    "    ax.scatter(path[-1, 0], path[-1, 1], color=color, s=100, marker='*', edgecolors='black')\n",
    "\n",
    "# 真の極小点をマーク\n",
    "minima = [(3, 2), (-2.805, 3.131), (-3.779, -3.283), (3.584, -1.848)]\n",
    "for m in minima:\n",
    "    ax.scatter(*m, color='black', s=200, marker='X', zorder=10)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Himmelblau関数: 複数の極小点への収束\\n(○: 開始点, ★: 終了点, X: 真の極小点)')\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-5, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(\"- 開始点によって収束先が異なる\")\n",
    "print(\"- 必ずしもグローバル最小点に到達しない\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 勾配モニタリングツール\n",
    "\n",
    "### 6.1 GradientMonitorクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientMonitor:\n",
    "    \"\"\"\n",
    "    学習中の勾配をモニタリングするツール\n",
    "    \n",
    "    機能:\n",
    "    - 各層の勾配ノルムの記録\n",
    "    - 勾配消失・爆発の検出\n",
    "    - 可視化\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = []  # [(epoch, {layer_name: grad_norm}), ...]\n",
    "    \n",
    "    def record(self, epoch, layers):\n",
    "        \"\"\"各層の勾配ノルムを記録\"\"\"\n",
    "        grad_norms = {}\n",
    "        layer_idx = 0\n",
    "        \n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'grads') and layer.grads.get('W') is not None:\n",
    "                name = f'Linear_{layer_idx}'\n",
    "                grad_norms[name] = np.linalg.norm(layer.grads['W'])\n",
    "                layer_idx += 1\n",
    "        \n",
    "        self.history.append((epoch, grad_norms))\n",
    "    \n",
    "    def detect_problems(self, vanish_threshold=1e-7, explode_threshold=1e3):\n",
    "        \"\"\"勾配の問題を検出\"\"\"\n",
    "        if not self.history:\n",
    "            return []\n",
    "        \n",
    "        problems = []\n",
    "        epoch, grad_norms = self.history[-1]\n",
    "        \n",
    "        for name, norm in grad_norms.items():\n",
    "            if norm < vanish_threshold:\n",
    "                problems.append(f\"警告: {name} で勾配消失の可能性 (norm={norm:.2e})\")\n",
    "            elif norm > explode_threshold:\n",
    "                problems.append(f\"警告: {name} で勾配爆発の可能性 (norm={norm:.2e})\")\n",
    "        \n",
    "        return problems\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"勾配ノルムの推移を可視化\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"記録がありません\")\n",
    "            return\n",
    "        \n",
    "        epochs = [h[0] for h in self.history]\n",
    "        layer_names = list(self.history[0][1].keys())\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        for name in layer_names:\n",
    "            norms = [h[1][name] for h in self.history]\n",
    "            ax.plot(epochs, norms, label=name, linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('勾配ノルム')\n",
    "        ax.set_title('各層の勾配ノルムの推移')\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig, ax\n",
    "\n",
    "\n",
    "print(\"GradientMonitor クラスを定義しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientMonitorの使用例\n",
    "np.random.seed(42)\n",
    "\n",
    "# データ\n",
    "X = np.random.randn(100, 2)\n",
    "t = np.random.randn(100, 1)\n",
    "\n",
    "# モデル\n",
    "layers = build_deep_network(8, activation='sigmoid', init='xavier')\n",
    "loss_fn = MSELoss()\n",
    "monitor = GradientMonitor()\n",
    "\n",
    "# 学習ループ（簡易版）\n",
    "lr = 0.1\n",
    "for epoch in range(100):\n",
    "    # 順伝播\n",
    "    h = X\n",
    "    for layer in layers:\n",
    "        h = layer.forward(h)\n",
    "    loss = loss_fn.forward(h, t)\n",
    "    \n",
    "    # 逆伝播\n",
    "    dout = loss_fn.backward()\n",
    "    for layer in reversed(layers):\n",
    "        dout = layer.backward(dout)\n",
    "    \n",
    "    # 勾配をモニタリング\n",
    "    monitor.record(epoch, layers)\n",
    "    \n",
    "    # パラメータ更新\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'params'):\n",
    "            for name in layer.params:\n",
    "                if layer.grads.get(name) is not None:\n",
    "                    layer.params[name] -= lr * layer.grads[name]\n",
    "\n",
    "# 可視化\n",
    "monitor.plot()\n",
    "plt.show()\n",
    "\n",
    "# 問題の検出\n",
    "problems = monitor.detect_problems()\n",
    "if problems:\n",
    "    print(\"\\n検出された問題:\")\n",
    "    for p in problems:\n",
    "        print(f\"  {p}\")\n",
    "else:\n",
    "    print(\"\\n問題は検出されませんでした\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 最適化手法の比較\n",
    "\n",
    "### 7.1 鞍点での挙動比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD vs Momentum vs Adam の鞍点での挙動\n",
    "\n",
    "def sgd_step(pos, grad, lr=0.1, state=None):\n",
    "    return pos - lr * grad, state\n",
    "\n",
    "\n",
    "def momentum_step(pos, grad, lr=0.1, state=None, momentum=0.9):\n",
    "    if state is None:\n",
    "        state = {'v': np.zeros_like(pos)}\n",
    "    state['v'] = momentum * state['v'] - lr * grad\n",
    "    return pos + state['v'], state\n",
    "\n",
    "\n",
    "def adam_step(pos, grad, lr=0.01, state=None, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    if state is None:\n",
    "        state = {'m': np.zeros_like(pos), 'v': np.zeros_like(pos), 't': 0}\n",
    "    state['t'] += 1\n",
    "    state['m'] = beta1 * state['m'] + (1 - beta1) * grad\n",
    "    state['v'] = beta2 * state['v'] + (1 - beta2) * (grad ** 2)\n",
    "    m_hat = state['m'] / (1 - beta1 ** state['t'])\n",
    "    v_hat = state['v'] / (1 - beta2 ** state['t'])\n",
    "    return pos - lr * m_hat / (np.sqrt(v_hat) + eps), state\n",
    "\n",
    "\n",
    "def run_optimizer(step_fn, start, loss_fn, grad_fn, n_steps=100, **kwargs):\n",
    "    path = [np.array(start)]\n",
    "    pos = np.array(start, dtype=float)\n",
    "    state = None\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_fn(pos[0], pos[1])\n",
    "        pos, state = step_fn(pos, grad, state=state, **kwargs)\n",
    "        path.append(pos.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "# 鞍点関数\n",
    "def saddle_grad(x, y):\n",
    "    return np.array([2*x, -2*y])\n",
    "\n",
    "\n",
    "# 各最適化手法の軌跡\n",
    "start = (0.1, 0.1)\n",
    "\n",
    "path_sgd = run_optimizer(sgd_step, start, saddle_function, saddle_grad, n_steps=50, lr=0.1)\n",
    "path_momentum = run_optimizer(momentum_step, start, saddle_function, saddle_grad, n_steps=50, lr=0.1)\n",
    "path_adam = run_optimizer(adam_step, start, saddle_function, saddle_grad, n_steps=50, lr=0.5)\n",
    "\n",
    "# 可視化\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "X_mesh, Y_mesh = np.meshgrid(x, y)\n",
    "Z = saddle_function(X_mesh, Y_mesh)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "contour = ax.contour(X_mesh, Y_mesh, Z, levels=20, cmap='coolwarm', alpha=0.7)\n",
    "\n",
    "ax.plot(path_sgd[:, 0], path_sgd[:, 1], 'o-', label='SGD', linewidth=2, markersize=4)\n",
    "ax.plot(path_momentum[:, 0], path_momentum[:, 1], 's-', label='Momentum', linewidth=2, markersize=4)\n",
    "ax.plot(path_adam[:, 0], path_adam[:, 1], '^-', label='Adam', linewidth=2, markersize=4)\n",
    "\n",
    "ax.scatter([0], [0], color='black', s=200, marker='X', zorder=10, label='鞍点')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('鞍点周辺での最適化手法の挙動')\n",
    "ax.legend()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【観察】\")\n",
    "print(\"- SGD: 鞍点に吸い込まれやすい\")\n",
    "print(\"- Momentum: 慣性で鞍点を通過しやすい\")\n",
    "print(\"- Adam: 適応的な学習率で効率的に脱出\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 演習問題\n",
    "\n",
    "### 演習 8.1: Batch Normalizationの実装\n",
    "\n",
    "勾配消失を軽減する Batch Normalization を実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.1: 解答欄\n",
    "\n",
    "class BatchNorm(Layer):\n",
    "    \"\"\"\n",
    "    Batch Normalization\n",
    "    \n",
    "    y = γ * (x - μ) / √(σ² + ε) + β\n",
    "    \n",
    "    TODO: forward() と backward() を実装\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.params['gamma'] = np.ones(n_features)\n",
    "        self.params['beta'] = np.zeros(n_features)\n",
    "        self.grads['gamma'] = None\n",
    "        self.grads['beta'] = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: 実装\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # TODO: 実装\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 8.2: 異なる深さでの学習比較\n",
    "\n",
    "5層、10層、20層のネットワークで学習を比較し、勾配消失の影響を観察してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.2: 解答欄\n",
    "\n",
    "# TODO: 異なる深さのネットワークを構築し、学習曲線を比較\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Unit 0.0 総まとめ\n",
    "\n",
    "### 学習したこと\n",
    "\n",
    "| ノートブック | テーマ | キーポイント |\n",
    "|-------------|--------|-------------|\n",
    "| **70** | 微分の再発見 | 数値微分、偏微分、勾配ベクトル |\n",
    "| **71** | 連鎖律の解剖 | 合成関数の微分、分岐と合流 |\n",
    "| **72** | 計算グラフ | ノードとエッジ、トポロジカルソート |\n",
    "| **73** | 逆伝播（スカラー） | backward()の実装、勾配チェック |\n",
    "| **74** | 逆伝播（行列） | バッチ処理、行列微分、MLP |\n",
    "| **75** | 学習ループ | SGD、Momentum、Adam、XOR問題 |\n",
    "| **76** | 勾配の病理学 | 消失・爆発、初期化、鞍点 |\n",
    "\n",
    "### 習得したスキル\n",
    "\n",
    "1. **数学的理解**: 微分、連鎖律、勾配の意味\n",
    "2. **実装力**: NumPyのみでのニューラルネットワーク構築\n",
    "3. **デバッグ力**: 勾配チェック、モニタリング\n",
    "4. **診断力**: 学習の問題を特定し対策する能力\n",
    "\n",
    "### 次のステップへ\n",
    "\n",
    "このユニットで学んだ「誤差逆伝播の物理」は、以下の発展的トピックの基礎となります：\n",
    "\n",
    "- **CNN**: 畳み込み層の逆伝播\n",
    "- **RNN/LSTM**: 時間方向の逆伝播（BPTT）\n",
    "- **Transformer**: Attentionの勾配\n",
    "- **自動微分ライブラリ**: PyTorch/JAXの内部理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. *AISTATS*.\n",
    "2. He, K., et al. (2015). Delving deep into rectifiers. *ICCV*.\n",
    "3. Ioffe, S., & Szegedy, C. (2015). Batch normalization. *ICML*.\n",
    "4. Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. *ICLR*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
