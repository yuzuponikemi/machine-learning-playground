{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 73: 逆伝播の誕生 ― 勾配を逆流させる\n",
    "\n",
    "## Backpropagation: Flowing Gradients Backward\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Unit 0.0「ニューラルエンジンの深部」** の第4章として、計算グラフ上で **勾配を逆流させる** 逆伝播（Backpropagation）をスカラー版で完全に実装します。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. 各ノードの `backward()` メソッドを完全に実装する\n",
    "2. **逆伝播の自動実行** を実現する\n",
    "3. 損失関数を追加し、**勾配チェック** で正しさを検証する\n",
    "4. $y = \\sigma(Wx + b)$ の全パラメータの勾配を自動計算する\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 70-72 の内容\n",
    "- 特に Notebook 72 で構築した計算グラフのクラス設計\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [逆伝播の原理：復習](#1-逆伝播の原理復習)\n",
    "2. [ノードクラスの再定義（backward付き）](#2-ノードクラスの再定義backward付き)\n",
    "3. [自動逆伝播の実装](#3-自動逆伝播の実装)\n",
    "4. [損失関数ノードの追加](#4-損失関数ノードの追加)\n",
    "5. [勾配チェックによる検証](#5-勾配チェックによる検証)\n",
    "6. [完全な順伝播・逆伝播の実行](#6-完全な順伝播逆伝播の実行)\n",
    "7. [勾配の可視化](#7-勾配の可視化)\n",
    "8. [演習問題](#8-演習問題)\n",
    "9. [まとめと次のステップ](#9-まとめと次のステップ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, FancyArrowPatch\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 日本語フォント設定\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 逆伝播の原理：復習\n",
    "\n",
    "### 1.1 順伝播と逆伝播の対比\n",
    "\n",
    "| 方向 | 流れるもの | 目的 |\n",
    "|------|-----------|------|\n",
    "| **順伝播** | 値（データ） | 入力から出力を計算 |\n",
    "| **逆伝播** | 勾配（誤差信号） | 各パラメータの影響度を計算 |\n",
    "\n",
    "### 1.2 逆伝播のルール\n",
    "\n",
    "各ノードは、**上流から受け取った勾配** と **局所的な勾配** を掛けて、**下流へ伝播** します。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\text{入力}} = \\frac{\\partial L}{\\partial \\text{出力}} \\times \\frac{\\partial \\text{出力}}{\\partial \\text{入力}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 順伝播と逆伝播の対比図\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# 共通設定\n",
    "node_positions = [(1, 1.5), (3, 1.5), (5, 1.5), (7, 1.5)]\n",
    "node_labels = ['x', 'f', 'g', 'y']\n",
    "\n",
    "# 左: 順伝播\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 8)\n",
    "ax.set_ylim(0, 3)\n",
    "ax.axis('off')\n",
    "\n",
    "for (nx, ny), label in zip(node_positions, node_labels):\n",
    "    circle = Circle((nx, ny), 0.4, facecolor='lightgreen', edgecolor='darkgreen', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(nx, ny, label, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(3):\n",
    "    ax.annotate('', xy=(node_positions[i+1][0] - 0.5, 1.5),\n",
    "                xytext=(node_positions[i][0] + 0.5, 1.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "ax.text(4, 2.5, '順伝播: 値が前へ流れる', ha='center', fontsize=12, color='darkgreen')\n",
    "ax.text(4, 0.5, r'$x \\rightarrow f(x) \\rightarrow g(f(x)) \\rightarrow y$', ha='center', fontsize=12)\n",
    "\n",
    "# 右: 逆伝播\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 8)\n",
    "ax.set_ylim(0, 3)\n",
    "ax.axis('off')\n",
    "\n",
    "for (nx, ny), label in zip(node_positions, node_labels):\n",
    "    circle = Circle((nx, ny), 0.4, facecolor='lightyellow', edgecolor='orange', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(nx, ny, label, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(3):\n",
    "    ax.annotate('', xy=(node_positions[i][0] + 0.5, 1.5),\n",
    "                xytext=(node_positions[i+1][0] - 0.5, 1.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "ax.text(4, 2.5, '逆伝播: 勾配が後ろへ流れる', ha='center', fontsize=12, color='darkred')\n",
    "ax.text(4, 0.5, r'$\\frac{\\partial L}{\\partial x} \\leftarrow \\frac{\\partial L}{\\partial f} \\leftarrow \\frac{\\partial L}{\\partial g} \\leftarrow \\frac{\\partial L}{\\partial y}$', \n",
    "        ha='center', fontsize=12)\n",
    "\n",
    "plt.suptitle('順伝播 vs 逆伝播', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ノードクラスの再定義（backward付き）\n",
    "\n",
    "Notebook 72 のクラスを拡張し、`backward()` メソッドを完全に実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    計算グラフのノード基底クラス（backward対応版）\n",
    "    \"\"\"\n",
    "    \n",
    "    _id_counter = 0\n",
    "    \n",
    "    def __init__(self, name=None):\n",
    "        self.value = None      # 順伝播の結果\n",
    "        self.grad = None       # 逆伝播の勾配（∂L/∂self）\n",
    "        self.inputs = []       # 入力ノード\n",
    "        self.outputs = []      # 出力ノード\n",
    "        \n",
    "        self.id = Node._id_counter\n",
    "        Node._id_counter += 1\n",
    "        self.name = name if name else f\"node_{self.id}\"\n",
    "    \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"勾配をゼロにリセット\"\"\"\n",
    "        if self.grad is not None:\n",
    "            self.grad = np.zeros_like(self.grad)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        val_str = f\"{self.value.item():.4f}\" if self.value is not None and self.value.size == 1 else str(self.value)\n",
    "        grad_str = f\"{self.grad.item():.4f}\" if self.grad is not None and np.array(self.grad).size == 1 else str(self.grad)\n",
    "        return f\"{self.__class__.__name__}('{self.name}', val={val_str}, grad={grad_str})\"\n",
    "\n",
    "\n",
    "class Variable(Node):\n",
    "    \"\"\"\n",
    "    入力変数ノード（学習パラメータまたは入力データ）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, value, name=None, requires_grad=True):\n",
    "        super().__init__(name)\n",
    "        self.value = np.atleast_1d(np.array(value, dtype=np.float64))\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = np.zeros_like(self.value) if requires_grad else None\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        # 葉ノードは勾配を受け取るだけ（伝播先がない）\n",
    "        pass\n",
    "    \n",
    "    def set_value(self, value):\n",
    "        self.value = np.atleast_1d(np.array(value, dtype=np.float64))\n",
    "        if self.requires_grad:\n",
    "            self.grad = np.zeros_like(self.value)\n",
    "\n",
    "\n",
    "class Operation(Node):\n",
    "    \"\"\"\n",
    "    演算ノードの基底クラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *inputs, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs = list(inputs)\n",
    "        \n",
    "        for inp in self.inputs:\n",
    "            inp.outputs.append(self)\n",
    "    \n",
    "    def _get_input_values(self):\n",
    "        return [inp.value for inp in self.inputs]\n",
    "\n",
    "\n",
    "print(\"基底クラスを定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 加算ノード（Add）の逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Operation):\n",
    "    \"\"\"\n",
    "    加算: z = x + y\n",
    "    \n",
    "    順伝播: z = x + y\n",
    "    逆伝播: \n",
    "        ∂L/∂x = ∂L/∂z × ∂z/∂x = ∂L/∂z × 1 = ∂L/∂z\n",
    "        ∂L/∂y = ∂L/∂z × ∂z/∂y = ∂L/∂z × 1 = ∂L/∂z\n",
    "    \n",
    "    → 勾配をそのまま両方の入力に分配\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, y, name=None):\n",
    "        super().__init__(x, y, name=name or 'add')\n",
    "    \n",
    "    def forward(self):\n",
    "        x_val, y_val = self._get_input_values()\n",
    "        self.value = x_val + y_val\n",
    "        return self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"勾配をそのまま入力に分配\"\"\"\n",
    "        x, y = self.inputs\n",
    "        \n",
    "        # 入力の勾配に加算（分岐がある場合に対応）\n",
    "        if x.grad is None:\n",
    "            x.grad = np.zeros_like(x.value)\n",
    "        if y.grad is None:\n",
    "            y.grad = np.zeros_like(y.value)\n",
    "        \n",
    "        x.grad = x.grad + self.grad * 1.0  # ∂z/∂x = 1\n",
    "        y.grad = y.grad + self.grad * 1.0  # ∂z/∂y = 1\n",
    "\n",
    "\n",
    "# 加算ノードの逆伝播を図解\n",
    "print(\"【加算ノード z = x + y の逆伝播】\")\n",
    "print(\"\")\n",
    "print(\"      ∂L/∂x = ∂L/∂z × 1\")\n",
    "print(\"     ↗\")\n",
    "print(\"∂L/∂z\")\n",
    "print(\"     ↘\")\n",
    "print(\"      ∂L/∂y = ∂L/∂z × 1\")\n",
    "print(\"\")\n",
    "print(\"→ 上流からの勾配をそのまま両方の入力へ分配\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 乗算ノード（Multiply）の逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiply(Operation):\n",
    "    \"\"\"\n",
    "    乗算: z = x × y\n",
    "    \n",
    "    順伝播: z = x × y\n",
    "    逆伝播:\n",
    "        ∂L/∂x = ∂L/∂z × ∂z/∂x = ∂L/∂z × y\n",
    "        ∂L/∂y = ∂L/∂z × ∂z/∂y = ∂L/∂z × x\n",
    "    \n",
    "    → 勾配に「相手の値」を掛けて分配\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, y, name=None):\n",
    "        super().__init__(x, y, name=name or 'mul')\n",
    "    \n",
    "    def forward(self):\n",
    "        x_val, y_val = self._get_input_values()\n",
    "        self.value = x_val * y_val\n",
    "        return self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"勾配に相手の値を掛けて分配\"\"\"\n",
    "        x, y = self.inputs\n",
    "        x_val, y_val = self._get_input_values()\n",
    "        \n",
    "        if x.grad is None:\n",
    "            x.grad = np.zeros_like(x.value)\n",
    "        if y.grad is None:\n",
    "            y.grad = np.zeros_like(y.value)\n",
    "        \n",
    "        x.grad = x.grad + self.grad * y_val  # ∂z/∂x = y\n",
    "        y.grad = y.grad + self.grad * x_val  # ∂z/∂y = x\n",
    "\n",
    "\n",
    "# 乗算ノードの逆伝播を図解\n",
    "print(\"【乗算ノード z = x × y の逆伝播】\")\n",
    "print(\"\")\n",
    "print(\"      ∂L/∂x = ∂L/∂z × y  ← 相手の値(y)を掛ける\")\n",
    "print(\"     ↗\")\n",
    "print(\"∂L/∂z\")\n",
    "print(\"     ↘\")\n",
    "print(\"      ∂L/∂y = ∂L/∂z × x  ← 相手の値(x)を掛ける\")\n",
    "print(\"\")\n",
    "print(\"→ 順伝播で使った相手の値を『記憶』しておく必要がある\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 シグモイドノード（Sigmoid）の逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    \"\"\"\n",
    "    シグモイド: y = σ(x) = 1 / (1 + exp(-x))\n",
    "    \n",
    "    順伝播: y = σ(x)\n",
    "    逆伝播:\n",
    "        ∂L/∂x = ∂L/∂y × ∂y/∂x = ∂L/∂y × σ(x)(1 - σ(x))\n",
    "    \n",
    "    → 順伝播の出力値を使って効率的に計算\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, name=None):\n",
    "        super().__init__(x, name=name or 'sigmoid')\n",
    "    \n",
    "    def forward(self):\n",
    "        x_val = self.inputs[0].value\n",
    "        # 数値安定性のための実装\n",
    "        self.value = np.where(\n",
    "            x_val >= 0,\n",
    "            1 / (1 + np.exp(-x_val)),\n",
    "            np.exp(x_val) / (1 + np.exp(x_val))\n",
    "        )\n",
    "        return self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"シグモイドの微分: σ'(x) = σ(x)(1 - σ(x))\"\"\"\n",
    "        x = self.inputs[0]\n",
    "        \n",
    "        if x.grad is None:\n",
    "            x.grad = np.zeros_like(x.value)\n",
    "        \n",
    "        # σ'(x) = σ(x)(1 - σ(x)) を使う（self.valueはσ(x)）\n",
    "        sigmoid_derivative = self.value * (1 - self.value)\n",
    "        x.grad = x.grad + self.grad * sigmoid_derivative\n",
    "\n",
    "\n",
    "# シグモイドの微分を図解\n",
    "print(\"【シグモイドノード y = σ(x) の逆伝播】\")\n",
    "print(\"\")\n",
    "print(\"∂L/∂y  →  ∂L/∂x = ∂L/∂y × σ(x)(1 - σ(x))\")\n",
    "print(\"\")\n",
    "print(\"→ σ(x)(1-σ(x)) は最大でも 0.25\")\n",
    "print(\"→ 層が深くなると勾配が消失しやすい\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 その他の演算ノード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Operation):\n",
    "    \"\"\"ReLU: y = max(0, x)\"\"\"\n",
    "    \n",
    "    def __init__(self, x, name=None):\n",
    "        super().__init__(x, name=name or 'relu')\n",
    "        self._mask = None\n",
    "    \n",
    "    def forward(self):\n",
    "        x_val = self.inputs[0].value\n",
    "        self._mask = (x_val > 0).astype(float)\n",
    "        self.value = np.maximum(0, x_val)\n",
    "        return self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        x = self.inputs[0]\n",
    "        if x.grad is None:\n",
    "            x.grad = np.zeros_like(x.value)\n",
    "        x.grad = x.grad + self.grad * self._mask\n",
    "\n",
    "\n",
    "class Square(Operation):\n",
    "    \"\"\"二乗: y = x²\"\"\"\n",
    "    \n",
    "    def __init__(self, x, name=None):\n",
    "        super().__init__(x, name=name or 'square')\n",
    "    \n",
    "    def forward(self):\n",
    "        x_val = self.inputs[0].value\n",
    "        self.value = x_val ** 2\n",
    "        return self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        x = self.inputs[0]\n",
    "        x_val = x.value\n",
    "        if x.grad is None:\n",
    "            x.grad = np.zeros_like(x.value)\n",
    "        x.grad = x.grad + self.grad * 2 * x_val  # ∂(x²)/∂x = 2x\n",
    "\n",
    "\n",
    "class ScalarMultiply(Operation):\n",
    "    \"\"\"スカラー倍: y = c × x\"\"\"\n",
    "    \n",
    "    def __init__(self, x, scalar, name=None):\n",
    "        super().__init__(x, name=name or f'scale({scalar})')\n",
    "        self.scalar = scalar\n",
    "    \n",
    "    def forward(self):\n",
    "        x_val = self.inputs[0].value\n",
    "        self.value = self.scalar * x_val\n",
    "        return self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        x = self.inputs[0]\n",
    "        if x.grad is None:\n",
    "            x.grad = np.zeros_like(x.value)\n",
    "        x.grad = x.grad + self.grad * self.scalar  # ∂(cx)/∂x = c\n",
    "\n",
    "\n",
    "class Subtract(Operation):\n",
    "    \"\"\"減算: z = x - y\"\"\"\n",
    "    \n",
    "    def __init__(self, x, y, name=None):\n",
    "        super().__init__(x, y, name=name or 'sub')\n",
    "    \n",
    "    def forward(self):\n",
    "        x_val, y_val = self._get_input_values()\n",
    "        self.value = x_val - y_val\n",
    "        return self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        x, y = self.inputs\n",
    "        if x.grad is None:\n",
    "            x.grad = np.zeros_like(x.value)\n",
    "        if y.grad is None:\n",
    "            y.grad = np.zeros_like(y.value)\n",
    "        x.grad = x.grad + self.grad * 1.0   # ∂z/∂x = 1\n",
    "        y.grad = y.grad + self.grad * (-1.0)  # ∂z/∂y = -1\n",
    "\n",
    "\n",
    "print(\"演算ノードを定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 自動逆伝播の実装\n",
    "\n",
    "### 3.1 トポロジカルソートの逆順\n",
    "\n",
    "逆伝播は、順伝播の **逆順** で実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(output_node):\n",
    "    \"\"\"計算グラフをトポロジカルソート\"\"\"\n",
    "    visited = set()\n",
    "    order = []\n",
    "    \n",
    "    def dfs(node):\n",
    "        if id(node) in visited:\n",
    "            return\n",
    "        visited.add(id(node))\n",
    "        \n",
    "        if hasattr(node, 'inputs'):\n",
    "            for inp in node.inputs:\n",
    "                dfs(inp)\n",
    "        \n",
    "        order.append(node)\n",
    "    \n",
    "    dfs(output_node)\n",
    "    return order\n",
    "\n",
    "\n",
    "def forward_pass(output_node, verbose=False):\n",
    "    \"\"\"順伝播を自動実行\"\"\"\n",
    "    sorted_nodes = topological_sort(output_node)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"【順伝播】\")\n",
    "    \n",
    "    for node in sorted_nodes:\n",
    "        result = node.forward()\n",
    "        if verbose:\n",
    "            val_str = f\"{result.item():.6f}\" if result.size == 1 else str(result)\n",
    "            print(f\"  {node.name}: {val_str}\")\n",
    "    \n",
    "    return output_node.value\n",
    "\n",
    "\n",
    "def backward_pass(output_node, verbose=False):\n",
    "    \"\"\"\n",
    "    逆伝播を自動実行\n",
    "    \n",
    "    1. トポロジカルソートの逆順でノードを処理\n",
    "    2. 出力ノードの勾配を1に初期化\n",
    "    3. 各ノードの backward() を呼び出す\n",
    "    \"\"\"\n",
    "    sorted_nodes = topological_sort(output_node)\n",
    "    reversed_nodes = sorted_nodes[::-1]  # 逆順\n",
    "    \n",
    "    # 出力ノードの勾配を 1 に初期化（∂L/∂L = 1）\n",
    "    output_node.grad = np.ones_like(output_node.value)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n【逆伝播】\")\n",
    "        print(f\"  {output_node.name}: grad = 1（出発点）\")\n",
    "    \n",
    "    # 逆順で逆伝播を実行\n",
    "    for node in reversed_nodes:\n",
    "        if isinstance(node, Operation):\n",
    "            node.backward()\n",
    "            if verbose:\n",
    "                for inp in node.inputs:\n",
    "                    grad_str = f\"{inp.grad.item():.6f}\" if inp.grad.size == 1 else str(inp.grad)\n",
    "                    print(f\"  {inp.name}: grad = {grad_str}\")\n",
    "\n",
    "\n",
    "print(\"自動順伝播・逆伝播を定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 簡単な例でテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト: y = (a + b) × c\n",
    "print(\"=\"*60)\n",
    "print(\"テスト: y = (a + b) × c\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# グラフ構築\n",
    "a = Variable(2.0, name='a')\n",
    "b = Variable(3.0, name='b')\n",
    "c = Variable(4.0, name='c')\n",
    "\n",
    "t = Add(a, b, name='t=a+b')\n",
    "y = Multiply(t, c, name='y=t×c')\n",
    "\n",
    "# 順伝播\n",
    "forward_pass(y, verbose=True)\n",
    "\n",
    "# 逆伝播\n",
    "backward_pass(y, verbose=True)\n",
    "\n",
    "print(\"\\n【勾配の解釈】\")\n",
    "print(f\"  ∂y/∂a = {a.grad.item():.1f}: a を1増やすと y は {a.grad.item():.1f} 増える\")\n",
    "print(f\"  ∂y/∂b = {b.grad.item():.1f}: b を1増やすと y は {b.grad.item():.1f} 増える\")\n",
    "print(f\"  ∂y/∂c = {c.grad.item():.1f}: c を1増やすと y は {c.grad.item():.1f} 増える\")\n",
    "\n",
    "# 手計算での検証\n",
    "# y = (a + b) × c = ac + bc\n",
    "# ∂y/∂a = c = 4\n",
    "# ∂y/∂b = c = 4\n",
    "# ∂y/∂c = a + b = 5\n",
    "print(\"\\n【検算】\")\n",
    "print(f\"  ∂y/∂a = c = 4 ✓\")\n",
    "print(f\"  ∂y/∂b = c = 4 ✓\")\n",
    "print(f\"  ∂y/∂c = a + b = 5 ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 損失関数ノードの追加\n",
    "\n",
    "### 4.1 平均二乗誤差（MSE）損失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Operation):\n",
    "    \"\"\"\n",
    "    平均二乗誤差損失: L = (y - t)² / 2\n",
    "    \n",
    "    ここで t は教師信号（定数として扱う）\n",
    "    \n",
    "    順伝播: L = (y - t)² / 2\n",
    "    逆伝播: ∂L/∂y = y - t\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, y, target, name=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y: 予測値ノード\n",
    "            target: 教師信号（スカラーまたは配列）\n",
    "        \"\"\"\n",
    "        super().__init__(y, name=name or 'mse_loss')\n",
    "        self.target = np.atleast_1d(np.array(target, dtype=np.float64))\n",
    "    \n",
    "    def forward(self):\n",
    "        y_val = self.inputs[0].value\n",
    "        self.value = 0.5 * np.sum((y_val - self.target) ** 2)\n",
    "        return self.value\n",
    "    \n",
    "    def backward(self):\n",
    "        y = self.inputs[0]\n",
    "        y_val = y.value\n",
    "        \n",
    "        if y.grad is None:\n",
    "            y.grad = np.zeros_like(y.value)\n",
    "        \n",
    "        # ∂L/∂y = y - t\n",
    "        y.grad = y.grad + self.grad * (y_val - self.target)\n",
    "\n",
    "\n",
    "# MSE損失の微分を確認\n",
    "print(\"【MSE損失 L = (y - t)² / 2 の逆伝播】\")\n",
    "print(\"\")\n",
    "print(\"∂L/∂y = y - t\")\n",
    "print(\"\")\n",
    "print(\"→ 予測が正解より大きい(y > t): 正の勾配 → y を減らす方向\")\n",
    "print(\"→ 予測が正解より小さい(y < t): 負の勾配 → y を増やす方向\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 勾配チェックによる検証\n",
    "\n",
    "### 5.1 数値微分との比較\n",
    "\n",
    "逆伝播で計算した勾配が正しいか、数値微分と比較して検証します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(param_node, loss_node, h=1e-5):\n",
    "    \"\"\"\n",
    "    パラメータの数値勾配を計算\n",
    "    \n",
    "    Args:\n",
    "        param_node: 勾配を計算したいパラメータノード\n",
    "        loss_node: 損失関数ノード\n",
    "        h: 微小変化量\n",
    "    \n",
    "    Returns:\n",
    "        数値勾配\n",
    "    \"\"\"\n",
    "    original_value = param_node.value.copy()\n",
    "    grad = np.zeros_like(original_value)\n",
    "    \n",
    "    for i in range(original_value.size):\n",
    "        # +h\n",
    "        param_node.value = original_value.copy()\n",
    "        param_node.value.flat[i] += h\n",
    "        forward_pass(loss_node)\n",
    "        loss_plus = loss_node.value.item()\n",
    "        \n",
    "        # -h\n",
    "        param_node.value = original_value.copy()\n",
    "        param_node.value.flat[i] -= h\n",
    "        forward_pass(loss_node)\n",
    "        loss_minus = loss_node.value.item()\n",
    "        \n",
    "        # 中心差分\n",
    "        grad.flat[i] = (loss_plus - loss_minus) / (2 * h)\n",
    "    \n",
    "    # 元の値に戻す\n",
    "    param_node.value = original_value\n",
    "    forward_pass(loss_node)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def gradient_check(params, loss_node, tol=1e-5):\n",
    "    \"\"\"\n",
    "    勾配チェック: 解析的勾配と数値勾配を比較\n",
    "    \n",
    "    Args:\n",
    "        params: パラメータノードのリスト\n",
    "        loss_node: 損失関数ノード\n",
    "        tol: 許容誤差\n",
    "    \n",
    "    Returns:\n",
    "        全てのパラメータでチェックが通ったか\n",
    "    \"\"\"\n",
    "    print(\"\\n【勾配チェック】\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'パラメータ':>12} | {'解析的勾配':>15} | {'数値勾配':>15} | {'相対誤差':>12} | 結果\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    for param in params:\n",
    "        analytical = param.grad\n",
    "        numerical = numerical_gradient(param, loss_node)\n",
    "        \n",
    "        # 相対誤差の計算\n",
    "        diff = np.abs(analytical - numerical)\n",
    "        denom = np.maximum(np.abs(analytical), np.abs(numerical)) + 1e-10\n",
    "        relative_error = np.max(diff / denom)\n",
    "        \n",
    "        passed = relative_error < tol\n",
    "        all_passed = all_passed and passed\n",
    "        status = \"✓ OK\" if passed else \"✗ NG\"\n",
    "        \n",
    "        anal_str = f\"{analytical.item():.8f}\" if analytical.size == 1 else str(analytical)\n",
    "        num_str = f\"{numerical.item():.8f}\" if numerical.size == 1 else str(numerical)\n",
    "        \n",
    "        print(f\"{param.name:>12} | {anal_str:>15} | {num_str:>15} | {relative_error:>12.2e} | {status}\")\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    if all_passed:\n",
    "        print(\"全てのパラメータで勾配チェックに合格しました！\")\n",
    "    else:\n",
    "        print(\"警告: 一部のパラメータで勾配が一致しません\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "\n",
    "print(\"勾配チェック関数を定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 完全な順伝播・逆伝播の実行\n",
    "\n",
    "### 6.1 y = σ(Wx + b) の完全な学習グラフ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neuron_with_loss(x_val, W_val, b_val, target):\n",
    "    \"\"\"\n",
    "    y = σ(Wx + b) + MSE損失の計算グラフを構築\n",
    "    \n",
    "    計算グラフ:\n",
    "        x ─┬─ (×) ─┬─ (+) ─── (σ) ─── y ─── (MSE) ─── L\n",
    "           │       │                        │\n",
    "        W ─┘       b                        t(教師)\n",
    "    \"\"\"\n",
    "    # 入力データ（勾配不要）\n",
    "    x = Variable(x_val, name='x', requires_grad=False)\n",
    "    \n",
    "    # 学習パラメータ（勾配必要）\n",
    "    W = Variable(W_val, name='W', requires_grad=True)\n",
    "    b = Variable(b_val, name='b', requires_grad=True)\n",
    "    \n",
    "    # 順伝播グラフ\n",
    "    z1 = Multiply(W, x, name='z1=Wx')\n",
    "    z2 = Add(z1, b, name='z2=z1+b')\n",
    "    y = Sigmoid(z2, name='y=σ(z2)')\n",
    "    \n",
    "    # 損失\n",
    "    loss = MSELoss(y, target, name='L')\n",
    "    \n",
    "    return {\n",
    "        'x': x, 'W': W, 'b': b,\n",
    "        'z1': z1, 'z2': z2, 'y': y,\n",
    "        'loss': loss,\n",
    "        'target': target\n",
    "    }\n",
    "\n",
    "\n",
    "# グラフの構築\n",
    "graph = build_neuron_with_loss(\n",
    "    x_val=2.0,\n",
    "    W_val=0.5,\n",
    "    b_val=-0.3,\n",
    "    target=1.0\n",
    ")\n",
    "\n",
    "print(\"【計算グラフ: y = σ(Wx + b) + MSE損失】\\n\")\n",
    "print(f\"入力: x = {graph['x'].value.item()}\")\n",
    "print(f\"パラメータ: W = {graph['W'].value.item()}, b = {graph['b'].value.item()}\")\n",
    "print(f\"教師信号: t = {graph['target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 順伝播と逆伝播の実行\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"順伝播・逆伝播の完全実行\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 順伝播\n",
    "forward_pass(graph['loss'], verbose=True)\n",
    "\n",
    "# 逆伝播\n",
    "backward_pass(graph['loss'], verbose=True)\n",
    "\n",
    "# 結果のまとめ\n",
    "print(\"\\n【結果のまとめ】\")\n",
    "print(f\"  予測値 y = {graph['y'].value.item():.6f}\")\n",
    "print(f\"  正解値 t = {graph['target']}\")\n",
    "print(f\"  損失 L = {graph['loss'].value.item():.6f}\")\n",
    "print(f\"\")\n",
    "print(f\"  ∂L/∂W = {graph['W'].grad.item():.6f}\")\n",
    "print(f\"  ∂L/∂b = {graph['b'].grad.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配チェック\n",
    "params = [graph['W'], graph['b']]\n",
    "gradient_check(params, graph['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 逆伝播の詳細追跡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_backward_trace(graph):\n",
    "    \"\"\"\n",
    "    逆伝播の各ステップを詳細に追跡\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"逆伝播の詳細追跡: y = σ(Wx + b), L = (y - t)²/2\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 値の取得\n",
    "    x = graph['x'].value.item()\n",
    "    W = graph['W'].value.item()\n",
    "    b = graph['b'].value.item()\n",
    "    z1 = graph['z1'].value.item()\n",
    "    z2 = graph['z2'].value.item()\n",
    "    y = graph['y'].value.item()\n",
    "    t = graph['target']\n",
    "    L = graph['loss'].value.item()\n",
    "    \n",
    "    print(f\"\\n【順伝播の値】\")\n",
    "    print(f\"  x = {x}\")\n",
    "    print(f\"  W = {W}, b = {b}\")\n",
    "    print(f\"  z1 = Wx = {z1}\")\n",
    "    print(f\"  z2 = z1 + b = {z2}\")\n",
    "    print(f\"  y = σ(z2) = {y:.6f}\")\n",
    "    print(f\"  t = {t}\")\n",
    "    print(f\"  L = (y-t)²/2 = {L:.6f}\")\n",
    "    \n",
    "    print(f\"\\n【逆伝播のステップ】\")\n",
    "    \n",
    "    # Step 1: dL/dL = 1\n",
    "    dL_dL = 1\n",
    "    print(f\"\\nStep 1: 出発点\")\n",
    "    print(f\"  dL/dL = {dL_dL}\")\n",
    "    \n",
    "    # Step 2: dL/dy = y - t\n",
    "    dL_dy = y - t\n",
    "    print(f\"\\nStep 2: MSE損失ノード\")\n",
    "    print(f\"  dL/dy = y - t = {y:.6f} - {t} = {dL_dy:.6f}\")\n",
    "    \n",
    "    # Step 3: dL/dz2 = dL/dy × dy/dz2 = dL/dy × σ(z2)(1-σ(z2))\n",
    "    sigmoid_deriv = y * (1 - y)\n",
    "    dL_dz2 = dL_dy * sigmoid_deriv\n",
    "    print(f\"\\nStep 3: シグモイドノード\")\n",
    "    print(f\"  dy/dz2 = σ(z2)(1-σ(z2)) = {y:.6f} × {1-y:.6f} = {sigmoid_deriv:.6f}\")\n",
    "    print(f\"  dL/dz2 = dL/dy × dy/dz2 = {dL_dy:.6f} × {sigmoid_deriv:.6f} = {dL_dz2:.6f}\")\n",
    "    \n",
    "    # Step 4: 加算ノード (z2 = z1 + b)\n",
    "    dL_dz1 = dL_dz2 * 1  # dz2/dz1 = 1\n",
    "    dL_db = dL_dz2 * 1   # dz2/db = 1\n",
    "    print(f\"\\nStep 4: 加算ノード (z2 = z1 + b)\")\n",
    "    print(f\"  dz2/dz1 = 1, dz2/db = 1\")\n",
    "    print(f\"  dL/dz1 = dL/dz2 × 1 = {dL_dz1:.6f}\")\n",
    "    print(f\"  dL/db = dL/dz2 × 1 = {dL_db:.6f}\")\n",
    "    \n",
    "    # Step 5: 乗算ノード (z1 = W × x)\n",
    "    dL_dW = dL_dz1 * x   # dz1/dW = x\n",
    "    dL_dx = dL_dz1 * W   # dz1/dx = W\n",
    "    print(f\"\\nStep 5: 乗算ノード (z1 = W × x)\")\n",
    "    print(f\"  dz1/dW = x = {x}, dz1/dx = W = {W}\")\n",
    "    print(f\"  dL/dW = dL/dz1 × x = {dL_dz1:.6f} × {x} = {dL_dW:.6f}\")\n",
    "    print(f\"  dL/dx = dL/dz1 × W = {dL_dz1:.6f} × {W} = {dL_dx:.6f}\")\n",
    "    \n",
    "    print(f\"\\n【最終結果】\")\n",
    "    print(f\"  ∂L/∂W = {dL_dW:.6f}\")\n",
    "    print(f\"  ∂L/∂b = {dL_db:.6f}\")\n",
    "    \n",
    "    # 自動計算との比較\n",
    "    print(f\"\\n【自動計算との比較】\")\n",
    "    print(f\"  ∂L/∂W: 手計算 = {dL_dW:.6f}, 自動 = {graph['W'].grad.item():.6f}\")\n",
    "    print(f\"  ∂L/∂b: 手計算 = {dL_db:.6f}, 自動 = {graph['b'].grad.item():.6f}\")\n",
    "\n",
    "\n",
    "# 詳細追跡の実行\n",
    "detailed_backward_trace(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 勾配の可視化\n",
    "\n",
    "### 7.1 計算グラフ上での勾配フロー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient_flow(graph):\n",
    "    \"\"\"\n",
    "    計算グラフ上で順伝播の値と逆伝播の勾配を可視化\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # ノード位置\n",
    "    positions = {\n",
    "        'x': (1, 3),\n",
    "        'W': (1, 1),\n",
    "        'b': (5, 1),\n",
    "        'z1': (3, 2),\n",
    "        'z2': (6, 2),\n",
    "        'y': (9, 2),\n",
    "        'loss': (12, 2),\n",
    "    }\n",
    "    \n",
    "    # ========== 上: 順伝播 ==========\n",
    "    ax = axes[0]\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('順伝播: 値が前へ流れる', fontsize=12, color='darkgreen')\n",
    "    \n",
    "    for name, (nx, ny) in positions.items():\n",
    "        node = graph.get(name)\n",
    "        if node is None:\n",
    "            continue\n",
    "        \n",
    "        # 色の決定\n",
    "        if name in ['W', 'b']:\n",
    "            color = 'lightyellow'\n",
    "        elif name == 'loss':\n",
    "            color = 'lightcoral'\n",
    "        elif name == 'x':\n",
    "            color = 'lightblue'\n",
    "        else:\n",
    "            color = 'lightgreen'\n",
    "        \n",
    "        circle = Circle((nx, ny), 0.5, facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        # ラベルと値\n",
    "        if hasattr(node, 'value') and node.value is not None:\n",
    "            val = node.value.item() if node.value.size == 1 else '...'\n",
    "            ax.text(nx, ny + 0.1, name, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "            ax.text(nx, ny - 0.25, f'{val:.3f}', ha='center', va='center', fontsize=8, color='blue')\n",
    "        else:\n",
    "            ax.text(nx, ny, name, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # エッジ（順方向）\n",
    "    edges = [\n",
    "        ('x', 'z1'), ('W', 'z1'), ('z1', 'z2'), ('b', 'z2'),\n",
    "        ('z2', 'y'), ('y', 'loss')\n",
    "    ]\n",
    "    for start, end in edges:\n",
    "        sx, sy = positions[start]\n",
    "        ex, ey = positions[end]\n",
    "        ax.annotate('', xy=(ex - 0.55, ey), xytext=(sx + 0.55, sy),\n",
    "                    arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
    "    \n",
    "    # 演算ラベル\n",
    "    ax.text(2, 2.7, '×', fontsize=14, ha='center')\n",
    "    ax.text(4.5, 2.7, '+', fontsize=14, ha='center')\n",
    "    ax.text(7.5, 2.7, 'σ', fontsize=14, ha='center')\n",
    "    ax.text(10.5, 2.7, 'MSE', fontsize=10, ha='center')\n",
    "    \n",
    "    # ========== 下: 逆伝播 ==========\n",
    "    ax = axes[1]\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('逆伝播: 勾配が後ろへ流れる', fontsize=12, color='darkred')\n",
    "    \n",
    "    for name, (nx, ny) in positions.items():\n",
    "        node = graph.get(name)\n",
    "        if node is None:\n",
    "            continue\n",
    "        \n",
    "        # 色の決定（パラメータを強調）\n",
    "        if name in ['W', 'b']:\n",
    "            color = 'orange'\n",
    "        elif name == 'loss':\n",
    "            color = 'lightcoral'\n",
    "        else:\n",
    "            color = 'lightyellow'\n",
    "        \n",
    "        circle = Circle((nx, ny), 0.5, facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        # ラベルと勾配\n",
    "        ax.text(nx, ny + 0.1, name, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "        if hasattr(node, 'grad') and node.grad is not None:\n",
    "            grad = node.grad.item() if np.array(node.grad).size == 1 else '...'\n",
    "            ax.text(nx, ny - 0.25, f'∇={grad:.3f}', ha='center', va='center', fontsize=8, color='red')\n",
    "    \n",
    "    # エッジ（逆方向）\n",
    "    for start, end in edges:\n",
    "        sx, sy = positions[start]\n",
    "        ex, ey = positions[end]\n",
    "        ax.annotate('', xy=(sx + 0.55, sy), xytext=(ex - 0.55, ey),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=1.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 可視化\n",
    "visualize_gradient_flow(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 勾配の意味を解釈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配の影響を確認\n",
    "print(\"【勾配の解釈】\\n\")\n",
    "\n",
    "W_grad = graph['W'].grad.item()\n",
    "b_grad = graph['b'].grad.item()\n",
    "\n",
    "print(f\"現在の状態:\")\n",
    "print(f\"  予測 y = {graph['y'].value.item():.4f}\")\n",
    "print(f\"  正解 t = {graph['target']}\")\n",
    "print(f\"  損失 L = {graph['loss'].value.item():.6f}\")\n",
    "print()\n",
    "\n",
    "print(f\"勾配:\")\n",
    "print(f\"  ∂L/∂W = {W_grad:.6f}\")\n",
    "if W_grad > 0:\n",
    "    print(f\"    → W を減らすと損失が下がる（W は大きすぎる）\")\n",
    "else:\n",
    "    print(f\"    → W を増やすと損失が下がる（W は小さすぎる）\")\n",
    "\n",
    "print(f\"  ∂L/∂b = {b_grad:.6f}\")\n",
    "if b_grad > 0:\n",
    "    print(f\"    → b を減らすと損失が下がる（b は大きすぎる）\")\n",
    "else:\n",
    "    print(f\"    → b を増やすと損失が下がる（b は小さすぎる）\")\n",
    "\n",
    "# 学習ステップのシミュレーション\n",
    "print(f\"\\n【1ステップの学習をシミュレート（学習率 η = 0.5）】\")\n",
    "lr = 0.5\n",
    "W_new = graph['W'].value.item() - lr * W_grad\n",
    "b_new = graph['b'].value.item() - lr * b_grad\n",
    "print(f\"  W: {graph['W'].value.item():.4f} → {W_new:.4f}\")\n",
    "print(f\"  b: {graph['b'].value.item():.4f} → {b_new:.4f}\")\n",
    "\n",
    "# 新しいパラメータでの予測\n",
    "z_new = W_new * graph['x'].value.item() + b_new\n",
    "y_new = 1 / (1 + np.exp(-z_new))\n",
    "L_new = 0.5 * (y_new - graph['target']) ** 2\n",
    "print(f\"\\n  新しい予測 y = {y_new:.4f}（元: {graph['y'].value.item():.4f}）\")\n",
    "print(f\"  新しい損失 L = {L_new:.6f}（元: {graph['loss'].value.item():.6f}）\")\n",
    "print(f\"  損失の変化: {L_new - graph['loss'].value.item():.6f}（負なら改善）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 演習問題\n",
    "\n",
    "### 演習 8.1: 2入力ニューロンの勾配計算\n",
    "\n",
    "$y = \\sigma(w_1 x_1 + w_2 x_2 + b)$ の計算グラフを構築し、全パラメータの勾配を計算してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.1: 解答欄\n",
    "\n",
    "def build_two_input_neuron(x1_val, x2_val, w1_val, w2_val, b_val, target):\n",
    "    \"\"\"\n",
    "    y = σ(w1*x1 + w2*x2 + b) + MSE損失の計算グラフを構築\n",
    "    \n",
    "    TODO: 以下を実装\n",
    "    1. 入力変数（x1, x2）と学習パラメータ（w1, w2, b）を定義\n",
    "    2. 計算グラフを構築\n",
    "    3. MSE損失を追加\n",
    "    4. 辞書で返す\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# テストコード（実装後にコメントを外して実行）\n",
    "# graph2 = build_two_input_neuron(1.0, 2.0, 0.5, -0.3, 0.1, 0.8)\n",
    "# forward_pass(graph2['loss'], verbose=True)\n",
    "# backward_pass(graph2['loss'], verbose=True)\n",
    "# gradient_check([graph2['w1'], graph2['w2'], graph2['b']], graph2['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 8.2: ReLU活性化関数を使った勾配計算\n",
    "\n",
    "シグモイドの代わりにReLUを使って同様の計算グラフを構築し、勾配を計算してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.2: 解答欄\n",
    "\n",
    "def build_relu_neuron(x_val, W_val, b_val, target):\n",
    "    \"\"\"\n",
    "    y = ReLU(Wx + b) + MSE損失の計算グラフを構築\n",
    "    \n",
    "    TODO: 実装\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# テストコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 8.3: 分岐を持つグラフ\n",
    "\n",
    "$y = x^2 + 2x$ のように、$x$ が複数の経路に分岐するグラフを構築し、勾配が正しく加算されることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.3: 解答欄\n",
    "\n",
    "def build_branching_graph(x_val):\n",
    "    \"\"\"\n",
    "    y = x² + 2x の計算グラフを構築\n",
    "    \n",
    "    注意: x は2つの経路（x² と 2x）に分岐する\n",
    "    逆伝播では、両経路からの勾配が x に加算されるべき\n",
    "    \n",
    "    解析的な勾配: dy/dx = 2x + 2\n",
    "    \n",
    "    TODO: 実装\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# テストコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **逆伝播の原理**: 上流からの勾配 × 局所的な勾配 = 下流への勾配\n",
    "\n",
    "2. **各ノードの backward()** 実装:\n",
    "   - Add: 勾配をそのまま分配\n",
    "   - Multiply: 勾配に相手の値を掛けて分配\n",
    "   - Sigmoid: 勾配に σ(x)(1-σ(x)) を掛ける\n",
    "\n",
    "3. **自動逆伝播**: トポロジカルソートの逆順で backward() を呼び出す\n",
    "\n",
    "4. **勾配チェック**: 数値微分との比較で実装を検証\n",
    "\n",
    "5. **勾配の意味**: パラメータをどの方向にどれだけ変化させれば損失が下がるか\n",
    "\n",
    "### 次のノートブック（74: 行列で並列化）への橋渡し\n",
    "\n",
    "このノートブックでは **スカラー** を扱いました。次のノートブックでは：\n",
    "\n",
    "- **バッチ処理**: 複数のサンプルを同時に処理\n",
    "- **行列微分**: ヤコビアンの概念\n",
    "- **効率的な実装**: NumPyによるベクトル化\n",
    "\n",
    "を学び、実用的なニューラルネットワークエンジンを構築します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 付録: 逆伝播の公式集\n",
    "\n",
    "| ノード | 順伝播 | 逆伝播（∂L/∂入力） |\n",
    "|--------|--------|--------------------|\n",
    "| Add (z = x + y) | z = x + y | ∂L/∂x = ∂L/∂z, ∂L/∂y = ∂L/∂z |\n",
    "| Multiply (z = xy) | z = xy | ∂L/∂x = y·∂L/∂z, ∂L/∂y = x·∂L/∂z |\n",
    "| Sigmoid (y = σ(x)) | y = 1/(1+e⁻ˣ) | ∂L/∂x = y(1-y)·∂L/∂y |\n",
    "| ReLU (y = max(0,x)) | y = max(0,x) | ∂L/∂x = 1_{x>0}·∂L/∂y |\n",
    "| Square (y = x²) | y = x² | ∂L/∂x = 2x·∂L/∂y |\n",
    "| MSE (L = (y-t)²/2) | L = (y-t)²/2 | ∂L/∂y = (y-t)·∂L/∂L |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
