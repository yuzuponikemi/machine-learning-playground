{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 74: 行列で並列化 ― ベクトル版逆伝播\n",
    "\n",
    "## Matrix Backpropagation: Vectorized Implementation\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Unit 0.0「ニューラルエンジンの深部」** の第5章として、スカラー版の逆伝播を **行列演算** に拡張し、バッチ処理を可能にします。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **行列微分** の基礎（ヤコビアン）を理解する\n",
    "2. **バッチ処理** における勾配計算を実装する\n",
    "3. **Linear層** と **活性化層** をクラスとして設計する\n",
    "4. **多層パーセプトロン（MLP）** を NumPy のみで構築する\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 70-73 の内容\n",
    "- 行列演算（行列積、転置）の基礎\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [スカラーから行列へ：次元の整理](#1-スカラーから行列へ次元の整理)\n",
    "2. [行列微分の基礎](#2-行列微分の基礎)\n",
    "3. [Linear層の実装](#3-linear層の実装)\n",
    "4. [活性化層の実装](#4-活性化層の実装)\n",
    "5. [損失関数の実装](#5-損失関数の実装)\n",
    "6. [多層パーセプトロンの構築](#6-多層パーセプトロンの構築)\n",
    "7. [勾配チェック](#7-勾配チェック)\n",
    "8. [演習問題](#8-演習問題)\n",
    "9. [まとめと次のステップ](#9-まとめと次のステップ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. スカラーから行列へ：次元の整理\n",
    "\n",
    "### 1.1 バッチ処理の必要性\n",
    "\n",
    "実際のニューラルネットワークでは、複数のサンプルを **同時に** 処理します（バッチ処理）。\n",
    "\n",
    "| スカラー版 | 行列版 |\n",
    "|-----------|--------|\n",
    "| 1サンプルずつ処理 | N サンプル同時処理 |\n",
    "| for ループが必要 | 行列演算で一括 |\n",
    "| 遅い | 高速（SIMD/GPU活用） |\n",
    "\n",
    "### 1.2 次元の規約\n",
    "\n",
    "```\n",
    "入力 X:  (N, D_in)   - N: バッチサイズ, D_in: 入力次元\n",
    "重み W:  (D_in, D_out) - D_out: 出力次元\n",
    "バイアス b: (D_out,)   - ブロードキャスト\n",
    "出力 Y:  (N, D_out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次元の確認\n",
    "N = 4       # バッチサイズ\n",
    "D_in = 3    # 入力次元\n",
    "D_out = 2   # 出力次元\n",
    "\n",
    "# ダミーデータ\n",
    "X = np.random.randn(N, D_in)\n",
    "W = np.random.randn(D_in, D_out)\n",
    "b = np.random.randn(D_out)\n",
    "\n",
    "# 線形変換: Y = XW + b\n",
    "Y = X @ W + b  # @ は行列積\n",
    "\n",
    "print(\"【次元の確認】\")\n",
    "print(f\"X (入力):    {X.shape} = (バッチサイズ, 入力次元)\")\n",
    "print(f\"W (重み):    {W.shape} = (入力次元, 出力次元)\")\n",
    "print(f\"b (バイアス): {b.shape} = (出力次元,)\")\n",
    "print(f\"Y (出力):    {Y.shape} = (バッチサイズ, 出力次元)\")\n",
    "\n",
    "print(f\"\\n【行列積の計算】\")\n",
    "print(f\"X @ W: ({N}, {D_in}) @ ({D_in}, {D_out}) → ({N}, {D_out})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 行列微分の基礎\n",
    "\n",
    "### 2.1 スカラー関数の行列微分\n",
    "\n",
    "損失 $L$ はスカラー値です。$L$ を行列 $W$ で微分すると、$W$ と同じ形状の行列が得られます：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} \\in \\mathbb{R}^{D_{in} \\times D_{out}}\n",
    "$$\n",
    "\n",
    "### 2.2 線形層の勾配導出\n",
    "\n",
    "線形変換 $Y = XW + b$ の勾配を考えます。\n",
    "\n",
    "上流から $\\frac{\\partial L}{\\partial Y}$ が来たとき：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = X^T \\frac{\\partial L}{\\partial Y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial Y_i} = \\mathbf{1}^T \\frac{\\partial L}{\\partial Y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行列微分の導出を確認\n",
    "print(\"【線形層 Y = XW + b の勾配】\\n\")\n",
    "\n",
    "print(\"上流からの勾配: dL/dY の形状 = (N, D_out)\")\n",
    "print(\"\")\n",
    "print(\"1. dL/dW = X^T @ dL/dY\")\n",
    "print(f\"   ({D_in}, {N}) @ ({N}, {D_out}) → ({D_in}, {D_out})\")\n",
    "print(\"\")\n",
    "print(\"2. dL/db = sum(dL/dY, axis=0)\")\n",
    "print(f\"   ({N}, {D_out}) → ({D_out},)\")\n",
    "print(\"\")\n",
    "print(\"3. dL/dX = dL/dY @ W^T\")\n",
    "print(f\"   ({N}, {D_out}) @ ({D_out}, {D_in}) → ({N}, {D_in})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Linear層の実装\n",
    "\n",
    "### 3.1 Layer基底クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    \"\"\"\n",
    "    ニューラルネットワーク層の基底クラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = {}    # 学習パラメータ\n",
    "        self.grads = {}     # パラメータの勾配\n",
    "        self.cache = {}     # 逆伝播用のキャッシュ\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        \"\"\"順伝播\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, dout):\n",
    "        \"\"\"逆伝播: 上流の勾配 dout を受け取り、下流への勾配を返す\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "print(\"Layer基底クラスを定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Linear層（全結合層）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    全結合層（線形変換）: Y = XW + b\n",
    "    \n",
    "    Args:\n",
    "        in_features: 入力次元\n",
    "        out_features: 出力次元\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Xavier初期化\n",
    "        scale = np.sqrt(2.0 / (in_features + out_features))\n",
    "        self.params['W'] = np.random.randn(in_features, out_features) * scale\n",
    "        self.params['b'] = np.zeros(out_features)\n",
    "        \n",
    "        self.grads['W'] = None\n",
    "        self.grads['b'] = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        順伝播: Y = XW + b\n",
    "        \n",
    "        Args:\n",
    "            x: 入力 (N, in_features)\n",
    "        Returns:\n",
    "            出力 (N, out_features)\n",
    "        \"\"\"\n",
    "        self.cache['x'] = x  # 逆伝播で使う\n",
    "        return x @ self.params['W'] + self.params['b']\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \n",
    "        Args:\n",
    "            dout: 上流からの勾配 (N, out_features)\n",
    "        Returns:\n",
    "            下流への勾配 (N, in_features)\n",
    "        \"\"\"\n",
    "        x = self.cache['x']\n",
    "        \n",
    "        # 勾配の計算\n",
    "        self.grads['W'] = x.T @ dout          # (in, N) @ (N, out) = (in, out)\n",
    "        self.grads['b'] = np.sum(dout, axis=0) # (out,)\n",
    "        \n",
    "        # 下流への勾配\n",
    "        dx = dout @ self.params['W'].T         # (N, out) @ (out, in) = (N, in)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "# テスト\n",
    "linear = Linear(3, 2)\n",
    "x = np.random.randn(4, 3)  # バッチサイズ4, 入力次元3\n",
    "\n",
    "# 順伝播\n",
    "y = linear.forward(x)\n",
    "print(f\"【Linear層テスト】\")\n",
    "print(f\"入力 x: {x.shape}\")\n",
    "print(f\"出力 y: {y.shape}\")\n",
    "\n",
    "# 逆伝播\n",
    "dout = np.ones_like(y)  # 上流からの勾配（仮）\n",
    "dx = linear.backward(dout)\n",
    "print(f\"\\n逆伝播:\")\n",
    "print(f\"dL/dW: {linear.grads['W'].shape}\")\n",
    "print(f\"dL/db: {linear.grads['b'].shape}\")\n",
    "print(f\"dL/dx: {dx.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 活性化層の実装\n",
    "\n",
    "### 4.1 Sigmoid層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    シグモイド活性化関数: y = 1 / (1 + exp(-x))\n",
    "    \n",
    "    逆伝播: dx = dout * y * (1 - y)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 数値安定性のための実装\n",
    "        y = np.where(\n",
    "            x >= 0,\n",
    "            1 / (1 + np.exp(-x)),\n",
    "            np.exp(x) / (1 + np.exp(x))\n",
    "        )\n",
    "        self.cache['y'] = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        y = self.cache['y']\n",
    "        return dout * y * (1 - y)\n",
    "\n",
    "\n",
    "# テスト\n",
    "sigmoid = Sigmoid()\n",
    "x = np.array([[-1, 0, 1], [2, -2, 0]])\n",
    "y = sigmoid.forward(x)\n",
    "print(f\"Sigmoid forward: x.shape={x.shape}, y.shape={y.shape}\")\n",
    "print(f\"y = \\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ReLU層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"\n",
    "    ReLU活性化関数: y = max(0, x)\n",
    "    \n",
    "    逆伝播: dx = dout * (x > 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache['mask'] = (x > 0).astype(float)\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.cache['mask']\n",
    "\n",
    "\n",
    "# テスト\n",
    "relu = ReLU()\n",
    "x = np.array([[-1, 0, 1], [2, -2, 0.5]])\n",
    "y = relu.forward(x)\n",
    "print(f\"ReLU forward:\")\n",
    "print(f\"x = \\n{x}\")\n",
    "print(f\"y = \\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Tanh層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    \"\"\"\n",
    "    Tanh活性化関数: y = tanh(x)\n",
    "    \n",
    "    逆伝播: dx = dout * (1 - y²)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = np.tanh(x)\n",
    "        self.cache['y'] = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        y = self.cache['y']\n",
    "        return dout * (1 - y ** 2)\n",
    "\n",
    "\n",
    "print(\"活性化層を定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 損失関数の実装\n",
    "\n",
    "### 5.1 MSE損失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    \"\"\"\n",
    "    平均二乗誤差損失: L = (1/N) * Σ(y - t)²\n",
    "    \n",
    "    逆伝播: dL/dy = (2/N) * (y - t)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, y, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y: 予測値 (N, D)\n",
    "            t: 正解値 (N, D)\n",
    "        Returns:\n",
    "            損失（スカラー）\n",
    "        \"\"\"\n",
    "        self.cache['y'] = y\n",
    "        self.cache['t'] = t\n",
    "        self.cache['N'] = y.shape[0]\n",
    "        \n",
    "        loss = np.mean((y - t) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            dL/dy (N, D)\n",
    "        \"\"\"\n",
    "        y = self.cache['y']\n",
    "        t = self.cache['t']\n",
    "        N = self.cache['N']\n",
    "        \n",
    "        return (2 / N) * (y - t)\n",
    "\n",
    "\n",
    "# テスト\n",
    "mse = MSELoss()\n",
    "y = np.array([[0.5, 0.8], [0.3, 0.9]])\n",
    "t = np.array([[1.0, 1.0], [0.0, 1.0]])\n",
    "\n",
    "loss = mse.forward(y, t)\n",
    "grad = mse.backward()\n",
    "print(f\"MSE Loss: {loss:.6f}\")\n",
    "print(f\"勾配 dL/dy:\\n{grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 CrossEntropy損失（Softmax + CrossEntropy）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    \"\"\"\n",
    "    Softmax + CrossEntropy 損失（分類問題用）\n",
    "    \n",
    "    Softmax: p_i = exp(x_i) / Σexp(x_j)\n",
    "    CrossEntropy: L = -Σ t_i * log(p_i)\n",
    "    \n",
    "    逆伝播: dL/dx = p - t（シンプルな形！）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        \"\"\"数値安定性を考慮したSoftmax\"\"\"\n",
    "        x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x_shifted)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: ロジット (N, C) - Softmax前の値\n",
    "            t: 正解ラベル (N, C) - one-hot または (N,) - クラスインデックス\n",
    "        Returns:\n",
    "            損失（スカラー）\n",
    "        \"\"\"\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # one-hot変換（必要な場合）\n",
    "        if t.ndim == 1:\n",
    "            t_onehot = np.zeros_like(x)\n",
    "            t_onehot[np.arange(N), t] = 1\n",
    "            t = t_onehot\n",
    "        \n",
    "        p = self._softmax(x)\n",
    "        self.cache['p'] = p\n",
    "        self.cache['t'] = t\n",
    "        self.cache['N'] = N\n",
    "        \n",
    "        # クロスエントロピー: -Σ t * log(p)\n",
    "        loss = -np.sum(t * np.log(p + 1e-10)) / N\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        p = self.cache['p']\n",
    "        t = self.cache['t']\n",
    "        N = self.cache['N']\n",
    "        \n",
    "        # dL/dx = (p - t) / N\n",
    "        return (p - t) / N\n",
    "\n",
    "\n",
    "# テスト\n",
    "sce = SoftmaxCrossEntropy()\n",
    "x = np.array([[2.0, 1.0, 0.1], [0.5, 2.5, 0.3]])\n",
    "t = np.array([0, 1])  # クラスインデックス\n",
    "\n",
    "loss = sce.forward(x, t)\n",
    "grad = sce.backward()\n",
    "print(f\"CrossEntropy Loss: {loss:.6f}\")\n",
    "print(f\"Softmax出力:\\n{sce.cache['p']}\")\n",
    "print(f\"勾配 dL/dx:\\n{grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 多層パーセプトロンの構築\n",
    "\n",
    "### 6.1 MLPクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    多層パーセプトロン（Multi-Layer Perceptron）\n",
    "    \n",
    "    NumPyのみで構築した、順伝播・逆伝播が可能なニューラルネットワーク\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layer_dims: 各層の次元リスト [入力, 隠れ1, 隠れ2, ..., 出力]\n",
    "            activation: 活性化関数 ('relu', 'sigmoid', 'tanh')\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        \n",
    "        # 活性化関数の選択\n",
    "        activation_class = {\n",
    "            'relu': ReLU,\n",
    "            'sigmoid': Sigmoid,\n",
    "            'tanh': Tanh\n",
    "        }[activation]\n",
    "        \n",
    "        # 層の構築\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            # 線形層\n",
    "            self.layers.append(Linear(layer_dims[i], layer_dims[i+1]))\n",
    "            \n",
    "            # 最後の層以外は活性化関数を追加\n",
    "            if i < len(layer_dims) - 2:\n",
    "                self.layers.append(activation_class())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"順伝播\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"逆伝播\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"全パラメータを取得\"\"\"\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                for name, param in layer.params.items():\n",
    "                    params.append((layer, name, param))\n",
    "        return params\n",
    "    \n",
    "    def get_grads(self):\n",
    "        \"\"\"全勾配を取得\"\"\"\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'grads'):\n",
    "                for name, grad in layer.grads.items():\n",
    "                    grads.append((layer, name, grad))\n",
    "        return grads\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"勾配をゼロにリセット\"\"\"\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'grads'):\n",
    "                for name in layer.grads:\n",
    "                    layer.grads[name] = None\n",
    "\n",
    "\n",
    "# テスト\n",
    "mlp = MLP([3, 4, 2], activation='relu')\n",
    "print(f\"【MLP構造】\")\n",
    "for i, layer in enumerate(mlp.layers):\n",
    "    print(f\"  Layer {i}: {layer.__class__.__name__}\")\n",
    "\n",
    "# 順伝播テスト\n",
    "x = np.random.randn(5, 3)  # バッチサイズ5, 入力次元3\n",
    "y = mlp.forward(x)\n",
    "print(f\"\\n入力: {x.shape} → 出力: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 完全な順伝播・逆伝播のテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完全なテスト: 回帰問題\n",
    "print(\"=\"*60)\n",
    "print(\"MLP 回帰問題テスト\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# データ\n",
    "np.random.seed(42)\n",
    "N = 10  # サンプル数\n",
    "X = np.random.randn(N, 2)\n",
    "t = np.random.randn(N, 1)  # ターゲット\n",
    "\n",
    "# モデル\n",
    "model = MLP([2, 4, 1], activation='sigmoid')\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "# 順伝播\n",
    "y = model.forward(X)\n",
    "loss = loss_fn.forward(y, t)\n",
    "print(f\"\\n【順伝播】\")\n",
    "print(f\"入力 X: {X.shape}\")\n",
    "print(f\"予測 y: {y.shape}\")\n",
    "print(f\"損失 L: {loss:.6f}\")\n",
    "\n",
    "# 逆伝播\n",
    "dout = loss_fn.backward()\n",
    "model.backward(dout)\n",
    "print(f\"\\n【逆伝播】\")\n",
    "for layer, name, grad in model.get_grads():\n",
    "    if grad is not None:\n",
    "        print(f\"{layer.__class__.__name__}.{name}: {grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 勾配チェック\n",
    "\n",
    "### 7.1 数値微分による検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_check(model, loss_fn, X, t, h=1e-5):\n",
    "    \"\"\"\n",
    "    数値微分と解析的勾配を比較して検証\n",
    "    \"\"\"\n",
    "    print(\"\\n【勾配チェック】\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 解析的勾配を計算\n",
    "    y = model.forward(X)\n",
    "    loss = loss_fn.forward(y, t)\n",
    "    dout = loss_fn.backward()\n",
    "    model.backward(dout)\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    for layer, name, param in model.get_params():\n",
    "        analytical_grad = layer.grads[name]\n",
    "        numerical_grad = np.zeros_like(param)\n",
    "        \n",
    "        # 数値微分\n",
    "        it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            original = param[idx]\n",
    "            \n",
    "            # +h\n",
    "            param[idx] = original + h\n",
    "            y_plus = model.forward(X)\n",
    "            loss_plus = loss_fn.forward(y_plus, t)\n",
    "            \n",
    "            # -h\n",
    "            param[idx] = original - h\n",
    "            y_minus = model.forward(X)\n",
    "            loss_minus = loss_fn.forward(y_minus, t)\n",
    "            \n",
    "            # 数値勾配\n",
    "            numerical_grad[idx] = (loss_plus - loss_minus) / (2 * h)\n",
    "            \n",
    "            # 元に戻す\n",
    "            param[idx] = original\n",
    "            it.iternext()\n",
    "        \n",
    "        # 相対誤差\n",
    "        diff = np.abs(analytical_grad - numerical_grad)\n",
    "        denom = np.maximum(np.abs(analytical_grad), np.abs(numerical_grad)) + 1e-10\n",
    "        relative_error = np.max(diff / denom)\n",
    "        \n",
    "        passed = relative_error < 1e-4\n",
    "        all_passed = all_passed and passed\n",
    "        status = \"✓ OK\" if passed else \"✗ NG\"\n",
    "        \n",
    "        print(f\"{layer.__class__.__name__}.{name}: 相対誤差 = {relative_error:.2e} {status}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    if all_passed:\n",
    "        print(\"全パラメータで勾配チェックに合格しました！\")\n",
    "    else:\n",
    "        print(\"警告: 一部のパラメータで勾配が一致しません\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "\n",
    "# 勾配チェック実行\n",
    "np.random.seed(123)\n",
    "model = MLP([2, 3, 1], activation='sigmoid')\n",
    "loss_fn = MSELoss()\n",
    "X = np.random.randn(5, 2)\n",
    "t = np.random.randn(5, 1)\n",
    "\n",
    "numerical_gradient_check(model, loss_fn, X, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 演習問題\n",
    "\n",
    "### 演習 8.1: Softmax + CrossEntropyでの勾配チェック\n",
    "\n",
    "分類問題用のモデルを構築し、勾配チェックを実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.1: 解答欄\n",
    "\n",
    "# 分類問題用モデル\n",
    "# TODO: MLP([2, 4, 3]) を構築（3クラス分類）\n",
    "# TODO: SoftmaxCrossEntropy損失で勾配チェック\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 8.2: より深いネットワーク\n",
    "\n",
    "4層のMLPを構築し、勾配が正しく逆伝播することを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 8.2: 解答欄\n",
    "\n",
    "# TODO: MLP([3, 8, 8, 4, 1]) を構築\n",
    "# TODO: 勾配チェック\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **バッチ処理の次元規約**: X(N, D_in), W(D_in, D_out), Y(N, D_out)\n",
    "\n",
    "2. **線形層の勾配**:\n",
    "   - $\\frac{\\partial L}{\\partial W} = X^T \\frac{\\partial L}{\\partial Y}$\n",
    "   - $\\frac{\\partial L}{\\partial b} = \\sum \\frac{\\partial L}{\\partial Y}$\n",
    "   - $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W^T$\n",
    "\n",
    "3. **活性化層の勾配**: 要素ごとの微分をそのまま適用\n",
    "\n",
    "4. **損失関数の勾配**: MSE, CrossEntropy\n",
    "\n",
    "5. **MLP**: 複数の層を連結した多層パーセプトロン\n",
    "\n",
    "### 次のノートブック（75: 学習ループ）への橋渡し\n",
    "\n",
    "逆伝播で勾配が計算できるようになりました。次のノートブックでは：\n",
    "\n",
    "- **SGD（確率的勾配降下法）** の実装\n",
    "- **学習ループ** の完成\n",
    "- 実際にXOR問題やMNISTを学習"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
