{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 71: 連鎖律の解剖 ― 合成関数を分解する\n",
    "\n",
    "## Chain Rule Decomposition: Breaking Down Composite Functions\n",
    "\n",
    "---\n",
    "\n",
    "### このノートブックの位置づけ\n",
    "\n",
    "**Unit 0.0「ニューラルエンジンの深部」** の第2章として、誤差逆伝播の数学的基盤である **連鎖律（Chain Rule）** を徹底的に理解します。\n",
    "\n",
    "### 学習目標\n",
    "\n",
    "1. **連鎖律** を直感的・数式的に理解する\n",
    "2. 複雑な合成関数を **段階的に分解** して微分する\n",
    "3. 多変数関数への連鎖律の拡張を理解する\n",
    "4. ニューラルネットワーク $y = \\sigma(Wx + b)$ の各成分の微分を導出する\n",
    "\n",
    "### 前提知識\n",
    "\n",
    "- Notebook 70 の内容（微分、偏微分、勾配）\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "1. [連鎖律とは：直感的な理解](#1-連鎖律とは直感的な理解)\n",
    "2. [単純な連鎖：1変数の合成関数](#2-単純な連鎖1変数の合成関数)\n",
    "3. [多段階の連鎖：深い合成関数](#3-多段階の連鎖深い合成関数)\n",
    "4. [多変数への拡張：全微分と連鎖律](#4-多変数への拡張全微分と連鎖律)\n",
    "5. [ニューラルネットワークへの適用](#5-ニューラルネットワークへの適用)\n",
    "6. [連鎖律の効率性：なぜO(n)なのか](#6-連鎖律の効率性なぜonなのか)\n",
    "7. [演習問題](#7-演習問題)\n",
    "8. [まとめと次のステップ](#8-まとめと次のステップ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境セットアップ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 日本語フォント設定\n",
    "plt.rcParams['font.family'] = ['Hiragino Sans', 'Arial Unicode MS', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 連鎖律とは：直感的な理解\n",
    "\n",
    "### 1.1 日常のアナロジー：歯車の連鎖\n",
    "\n",
    "連鎖律は「**変化が伝播する**」仕組みを表しています。\n",
    "\n",
    "例えば、自転車のギア比を考えてみましょう：\n",
    "\n",
    "```\n",
    "ペダル → 前ギア → チェーン → 後ギア → 車輪\n",
    "```\n",
    "\n",
    "- ペダルを1回転させると、前ギア（ギア比2）で2回転\n",
    "- 後ギア（ギア比0.5）を通って、車輪は1回転\n",
    "- 全体の変換比 = 2 × 0.5 = 1\n",
    "\n",
    "**連鎖律も同じ原理です**：各段階の「変化率」を掛け合わせます。\n",
    "\n",
    "### 1.2 数学的な定義\n",
    "\n",
    "関数 $y = f(u)$ と $u = g(x)$ が合成されているとき：\n",
    "\n",
    "$$\n",
    "y = f(g(x))\n",
    "$$\n",
    "\n",
    "この合成関数の $x$ に関する微分は：\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "$$\n",
    "\n",
    "**言葉で言うと**：\n",
    "- $x$ が変化したとき $u$ がどれだけ変化するか（$\\frac{du}{dx}$）\n",
    "- $u$ が変化したとき $y$ がどれだけ変化するか（$\\frac{dy}{du}$）\n",
    "- これらを掛け合わせると、$x$ → $y$ の変化率が得られる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 歯車のアナロジーを可視化\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 3)\n",
    "ax.axis('off')\n",
    "\n",
    "# ノード（歯車）を描画\n",
    "nodes = [\n",
    "    (1.5, 1.5, 'x', '入力'),\n",
    "    (4.5, 1.5, 'u = g(x)', '中間'),\n",
    "    (7.5, 1.5, 'y = f(u)', '出力'),\n",
    "]\n",
    "\n",
    "for x, y, label, sublabel in nodes:\n",
    "    circle = plt.Circle((x, y), 0.8, fill=True, color='lightblue', \n",
    "                         edgecolor='navy', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, y, label, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(x, y - 1.2, sublabel, ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "# 矢印（変化の伝播）を描画\n",
    "arrow_style = dict(arrowstyle='->', color='darkgreen', lw=2, mutation_scale=15)\n",
    "ax.annotate('', xy=(3.5, 1.5), xytext=(2.5, 1.5), arrowprops=arrow_style)\n",
    "ax.annotate('', xy=(6.5, 1.5), xytext=(5.5, 1.5), arrowprops=arrow_style)\n",
    "\n",
    "# 微分係数のラベル\n",
    "ax.text(3.0, 2.0, r'$\\frac{du}{dx}$', ha='center', va='center', fontsize=14, color='darkgreen')\n",
    "ax.text(6.0, 2.0, r'$\\frac{dy}{du}$', ha='center', va='center', fontsize=14, color='darkgreen')\n",
    "\n",
    "# 全体の連鎖律\n",
    "ax.text(10.5, 1.5, r'$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$', \n",
    "        ha='center', va='center', fontsize=16, \n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='orange'))\n",
    "\n",
    "ax.set_title('連鎖律：変化が段階的に伝播する', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【連鎖律の核心】\")\n",
    "print(\"各段階の『局所的な変化率』を掛け合わせると、」\")\n",
    "print(\"『全体の変化率』が得られる\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 単純な連鎖：1変数の合成関数\n",
    "\n",
    "### 2.1 例題：$(2x + 1)^3$ の微分\n",
    "\n",
    "関数 $y = (2x + 1)^3$ を考えます。\n",
    "\n",
    "**分解**：\n",
    "- 外側の関数: $y = u^3$ where $u = 2x + 1$\n",
    "- 内側の関数: $u = 2x + 1$\n",
    "\n",
    "**連鎖律の適用**：\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} = 3u^2 \\cdot 2 = 6(2x + 1)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例題の検証\n",
    "def y_func(x):\n",
    "    \"\"\"y = (2x + 1)^3\"\"\"\n",
    "    return (2*x + 1)**3\n",
    "\n",
    "\n",
    "def dy_dx_analytical(x):\n",
    "    \"\"\"解析的な導関数: dy/dx = 6(2x + 1)^2\"\"\"\n",
    "    return 6 * (2*x + 1)**2\n",
    "\n",
    "\n",
    "def numerical_diff(f, x, h=1e-7):\n",
    "    \"\"\"中心差分による数値微分\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "\n",
    "# 検証\n",
    "test_points = [-1.0, 0.0, 0.5, 1.0, 2.0]\n",
    "\n",
    "print(\"y = (2x + 1)³ の微分の検証\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'x':>6} | {'解析的 dy/dx':>15} | {'数値微分':>15} | {'誤差':>12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for x in test_points:\n",
    "    analytical = dy_dx_analytical(x)\n",
    "    numerical = numerical_diff(y_func, x)\n",
    "    error = abs(analytical - numerical)\n",
    "    print(f\"{x:>6.1f} | {analytical:>15.6f} | {numerical:>15.6f} | {error:>12.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 段階ごとの分解を可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 段階ごとの値と微分を追跡\n",
    "def trace_chain_rule(x_val):\n",
    "    \"\"\"連鎖律の各段階を追跡\"\"\"\n",
    "    print(f\"\\n【x = {x_val} での連鎖律の追跡】\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Step 1: 内側の関数\n",
    "    u = 2 * x_val + 1\n",
    "    du_dx = 2  # u = 2x + 1 の微分\n",
    "    print(f\"\\nStep 1: u = 2x + 1\")\n",
    "    print(f\"  u = 2 × {x_val} + 1 = {u}\")\n",
    "    print(f\"  du/dx = 2\")\n",
    "    \n",
    "    # Step 2: 外側の関数\n",
    "    y = u ** 3\n",
    "    dy_du = 3 * u ** 2  # y = u^3 の微分\n",
    "    print(f\"\\nStep 2: y = u³\")\n",
    "    print(f\"  y = {u}³ = {y}\")\n",
    "    print(f\"  dy/du = 3u² = 3 × {u}² = {dy_du}\")\n",
    "    \n",
    "    # Step 3: 連鎖律で合成\n",
    "    dy_dx = dy_du * du_dx\n",
    "    print(f\"\\nStep 3: 連鎖律の適用\")\n",
    "    print(f\"  dy/dx = dy/du × du/dx\")\n",
    "    print(f\"        = {dy_du} × {du_dx}\")\n",
    "    print(f\"        = {dy_dx}\")\n",
    "    \n",
    "    return dy_dx\n",
    "\n",
    "\n",
    "# 複数の点で追跡\n",
    "for x in [0, 1, 2]:\n",
    "    trace_chain_rule(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 さらに複雑な例：$\\sin(x^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = sin(x²) の微分\n",
    "# 分解: y = sin(u), u = x²\n",
    "# dy/dx = dy/du × du/dx = cos(u) × 2x = 2x cos(x²)\n",
    "\n",
    "def sin_x_squared(x):\n",
    "    return np.sin(x**2)\n",
    "\n",
    "\n",
    "def sin_x_squared_derivative(x):\n",
    "    \"\"\"解析的導関数: 2x cos(x²)\"\"\"\n",
    "    return 2 * x * np.cos(x**2)\n",
    "\n",
    "\n",
    "# 可視化\n",
    "x = np.linspace(-3, 3, 500)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 関数と導関数\n",
    "axes[0].plot(x, sin_x_squared(x), 'b-', linewidth=2.5, label=r'$y = \\sin(x^2)$')\n",
    "axes[0].plot(x, sin_x_squared_derivative(x), 'r--', linewidth=2, label=r\"$y' = 2x\\cos(x^2)$\")\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('y', fontsize=12)\n",
    "axes[0].set_title(r'$y = \\sin(x^2)$ とその導関数', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# 連鎖律の構造を図示\n",
    "axes[1].axis('off')\n",
    "axes[1].set_xlim(0, 10)\n",
    "axes[1].set_ylim(0, 6)\n",
    "\n",
    "# ボックスとラベル\n",
    "boxes = [\n",
    "    (1, 3, 'x'),\n",
    "    (4, 3, r'$u = x^2$'),\n",
    "    (7, 3, r'$y = \\sin(u)$'),\n",
    "]\n",
    "\n",
    "for bx, by, label in boxes:\n",
    "    rect = FancyBboxPatch((bx-0.8, by-0.5), 1.6, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='lightblue', edgecolor='navy', linewidth=2)\n",
    "    axes[1].add_patch(rect)\n",
    "    axes[1].text(bx, by, label, ha='center', va='center', fontsize=14)\n",
    "\n",
    "# 矢印\n",
    "axes[1].annotate('', xy=(3, 3), xytext=(2, 3),\n",
    "                 arrowprops=dict(arrowstyle='->', color='darkgreen', lw=2))\n",
    "axes[1].annotate('', xy=(6, 3), xytext=(5, 3),\n",
    "                 arrowprops=dict(arrowstyle='->', color='darkgreen', lw=2))\n",
    "\n",
    "# 微分のラベル\n",
    "axes[1].text(2.5, 3.8, r'$\\frac{du}{dx} = 2x$', ha='center', fontsize=12, color='darkgreen')\n",
    "axes[1].text(5.5, 3.8, r'$\\frac{dy}{du} = \\cos(u)$', ha='center', fontsize=12, color='darkgreen')\n",
    "\n",
    "# 結果\n",
    "axes[1].text(5, 1.2, r'$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} = \\cos(x^2) \\cdot 2x = 2x\\cos(x^2)$',\n",
    "             ha='center', fontsize=14, \n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='orange'))\n",
    "\n",
    "axes[1].set_title('連鎖律の適用プロセス', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 多段階の連鎖：深い合成関数\n",
    "\n",
    "### 3.1 3層以上の合成\n",
    "\n",
    "ニューラルネットワークは何層にも重なった合成関数です。3層の例を考えます：\n",
    "\n",
    "$$\n",
    "y = f(g(h(x)))\n",
    "$$\n",
    "\n",
    "連鎖律を繰り返し適用：\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{dv} \\cdot \\frac{dv}{du} \\cdot \\frac{du}{dx}\n",
    "$$\n",
    "\n",
    "where $u = h(x)$, $v = g(u)$, $y = f(v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3層の合成関数の例\n",
    "# y = exp(sin(x²))\n",
    "# 分解: h(x) = x², g(u) = sin(u), f(v) = exp(v)\n",
    "\n",
    "def three_layer_composite(x):\n",
    "    \"\"\"y = exp(sin(x²))\"\"\"\n",
    "    return np.exp(np.sin(x**2))\n",
    "\n",
    "\n",
    "def three_layer_derivative(x):\n",
    "    \"\"\"\n",
    "    dy/dx = dy/dv × dv/du × du/dx\n",
    "          = exp(v) × cos(u) × 2x\n",
    "          = exp(sin(x²)) × cos(x²) × 2x\n",
    "    \"\"\"\n",
    "    u = x**2\n",
    "    v = np.sin(u)\n",
    "    return np.exp(v) * np.cos(u) * 2 * x\n",
    "\n",
    "\n",
    "def trace_three_layer(x_val):\n",
    "    \"\"\"3層の連鎖律を追跡\"\"\"\n",
    "    print(f\"\\n【x = {x_val} での3層連鎖律】\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 順伝播（Forward）\n",
    "    print(\"\\n[順伝播: 値を前へ送る]\")\n",
    "    u = x_val ** 2\n",
    "    print(f\"  x = {x_val}\")\n",
    "    print(f\"  u = h(x) = x² = {u:.4f}\")\n",
    "    \n",
    "    v = np.sin(u)\n",
    "    print(f\"  v = g(u) = sin(u) = sin({u:.4f}) = {v:.4f}\")\n",
    "    \n",
    "    y = np.exp(v)\n",
    "    print(f\"  y = f(v) = exp(v) = exp({v:.4f}) = {y:.4f}\")\n",
    "    \n",
    "    # 逆伝播（Backward）- 微分を逆向きに計算\n",
    "    print(\"\\n[逆伝播: 微分を後ろへ送る]\")\n",
    "    \n",
    "    dy_dv = np.exp(v)\n",
    "    print(f\"  dy/dv = exp(v) = {dy_dv:.4f}\")\n",
    "    \n",
    "    dv_du = np.cos(u)\n",
    "    print(f\"  dv/du = cos(u) = cos({u:.4f}) = {dv_du:.4f}\")\n",
    "    \n",
    "    du_dx = 2 * x_val\n",
    "    print(f\"  du/dx = 2x = {du_dx:.4f}\")\n",
    "    \n",
    "    # 連鎖律で合成\n",
    "    dy_dx = dy_dv * dv_du * du_dx\n",
    "    print(f\"\\n[連鎖律の適用]\")\n",
    "    print(f\"  dy/dx = dy/dv × dv/du × du/dx\")\n",
    "    print(f\"        = {dy_dv:.4f} × {dv_du:.4f} × {du_dx:.4f}\")\n",
    "    print(f\"        = {dy_dx:.4f}\")\n",
    "    \n",
    "    # 数値微分で検証\n",
    "    numerical = numerical_diff(three_layer_composite, x_val)\n",
    "    print(f\"\\n[検証] 数値微分: {numerical:.4f}\")\n",
    "    print(f\"       誤差: {abs(dy_dx - numerical):.2e}\")\n",
    "\n",
    "\n",
    "# 追跡実行\n",
    "trace_three_layer(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 計算の流れを図解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3層の計算グラフを可視化\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# 上: 順伝播（Forward Pass）\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 3)\n",
    "ax.axis('off')\n",
    "\n",
    "# ノード\n",
    "nodes_forward = [\n",
    "    (1, 1.5, 'x', 'x=1'),\n",
    "    (4, 1.5, '$u = x^2$', 'u=1'),\n",
    "    (7, 1.5, '$v = \\\\sin(u)$', 'v=0.84'),\n",
    "    (10, 1.5, '$y = e^v$', 'y=2.32'),\n",
    "]\n",
    "\n",
    "for nx, ny, label, value in nodes_forward:\n",
    "    rect = FancyBboxPatch((nx-1, ny-0.5), 2, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='lightgreen', edgecolor='darkgreen', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(nx, ny + 0.1, label, ha='center', va='center', fontsize=11)\n",
    "    ax.text(nx, ny - 0.3, value, ha='center', va='center', fontsize=10, color='blue')\n",
    "\n",
    "# 矢印（順方向）\n",
    "for i in range(3):\n",
    "    ax.annotate('', xy=(nodes_forward[i+1][0]-1.1, 1.5), \n",
    "                xytext=(nodes_forward[i][0]+1.1, 1.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "ax.set_title('順伝播（Forward Pass）: 値を前へ送る →', fontsize=14, color='darkgreen', pad=10)\n",
    "ax.text(12.5, 1.5, '値の\\n計算', ha='center', va='center', fontsize=11,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# 下: 逆伝播（Backward Pass）\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 3)\n",
    "ax.axis('off')\n",
    "\n",
    "# ノード（微分値付き）\n",
    "nodes_backward = [\n",
    "    (1, 1.5, 'x', '$\\\\frac{du}{dx}=2$'),\n",
    "    (4, 1.5, 'u', '$\\\\frac{dv}{du}=0.54$'),\n",
    "    (7, 1.5, 'v', '$\\\\frac{dy}{dv}=2.32$'),\n",
    "    (10, 1.5, 'y', '(出力)'),\n",
    "]\n",
    "\n",
    "for nx, ny, label, deriv in nodes_backward:\n",
    "    rect = FancyBboxPatch((nx-1, ny-0.5), 2, 1, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor='lightyellow', edgecolor='orange', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(nx, ny + 0.1, label, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(nx, ny - 0.3, deriv, ha='center', va='center', fontsize=9, color='red')\n",
    "\n",
    "# 矢印（逆方向）\n",
    "for i in range(3):\n",
    "    ax.annotate('', xy=(nodes_backward[i][0]+1.1, 1.5),\n",
    "                xytext=(nodes_backward[i+1][0]-1.1, 1.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "ax.set_title('逆伝播（Backward Pass）: 勾配を後ろへ送る ←', fontsize=14, color='darkred', pad=10)\n",
    "\n",
    "# 最終結果\n",
    "ax.text(12.5, 1.5, 'dy/dx\\n=2.51', ha='center', va='center', fontsize=11,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【重要な洞察】\")\n",
    "print(\"・順伝播: x → u → v → y の順に『値』を計算\")\n",
    "print(\"・逆伝播: y → v → u → x の順に『微分』を伝播\")\n",
    "print(\"・これが誤差逆伝播法（Backpropagation）の本質\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 多変数への拡張：全微分と連鎖律\n",
    "\n",
    "### 4.1 複数の入力を持つ関数\n",
    "\n",
    "ニューラルネットワークでは、1つの出力が **複数の入力** から計算されます。\n",
    "\n",
    "例えば、$z = f(x, y)$ で $x = x(t)$, $y = y(t)$ のとき：\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x} \\cdot \\frac{dx}{dt} + \\frac{\\partial z}{\\partial y} \\cdot \\frac{dy}{dt}\n",
    "$$\n",
    "\n",
    "これは **全微分の連鎖律** と呼ばれます。\n",
    "\n",
    "### 4.2 加算ノードと乗算ノードの分岐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分岐と合流を持つ計算グラフの例\n",
    "# z = x*y + sin(x)\n",
    "\n",
    "def branching_example(x, y):\n",
    "    \"\"\"z = x*y + sin(x)\"\"\"\n",
    "    return x * y + np.sin(x)\n",
    "\n",
    "\n",
    "def trace_branching(x_val, y_val):\n",
    "    \"\"\"\n",
    "    z = x*y + sin(x) の勾配を追跡\n",
    "    \n",
    "    計算グラフ:\n",
    "         x ──┬── (*) ──┐\n",
    "             │         │\n",
    "         y ──┘         (+) ── z\n",
    "             │         │\n",
    "         x ──(sin)────┘\n",
    "    \n",
    "    ∂z/∂x = ∂z/∂(x*y) × ∂(x*y)/∂x + ∂z/∂(sin(x)) × ∂(sin(x))/∂x\n",
    "          = 1 × y + 1 × cos(x)\n",
    "          = y + cos(x)\n",
    "    \n",
    "    ∂z/∂y = ∂z/∂(x*y) × ∂(x*y)/∂y\n",
    "          = 1 × x\n",
    "          = x\n",
    "    \"\"\"\n",
    "    print(f\"\\n【分岐のある計算グラフ】\")\n",
    "    print(f\"z = x*y + sin(x) at (x, y) = ({x_val}, {y_val})\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 順伝播\n",
    "    print(\"\\n[順伝播]\")\n",
    "    term1 = x_val * y_val\n",
    "    print(f\"  x*y = {x_val} × {y_val} = {term1}\")\n",
    "    term2 = np.sin(x_val)\n",
    "    print(f\"  sin(x) = sin({x_val}) = {term2:.4f}\")\n",
    "    z = term1 + term2\n",
    "    print(f\"  z = {term1} + {term2:.4f} = {z:.4f}\")\n",
    "    \n",
    "    # 逆伝播（勾配）\n",
    "    print(\"\\n[逆伝播]\")\n",
    "    print(\"  dz/dz = 1（出発点）\")\n",
    "    print(\"\")\n",
    "    print(\"  加算ノード (+) で分岐:\")\n",
    "    print(\"    → x*y への勾配 = 1\")\n",
    "    print(\"    → sin(x) への勾配 = 1\")\n",
    "    print(\"\")\n",
    "    print(\"  乗算ノード (x*y) で:\")\n",
    "    print(f\"    → x への勾配 = y = {y_val}\")\n",
    "    print(f\"    → y への勾配 = x = {x_val}\")\n",
    "    print(\"\")\n",
    "    print(\"  sin ノード で:\")\n",
    "    print(f\"    → x への勾配 = cos(x) = cos({x_val}) = {np.cos(x_val):.4f}\")\n",
    "    print(\"\")\n",
    "    print(\"  x は2つの経路から勾配を受け取る（合流）:\")\n",
    "    dz_dx = y_val + np.cos(x_val)\n",
    "    dz_dy = x_val\n",
    "    print(f\"    ∂z/∂x = y + cos(x) = {y_val} + {np.cos(x_val):.4f} = {dz_dx:.4f}\")\n",
    "    print(f\"    ∂z/∂y = x = {dz_dy}\")\n",
    "    \n",
    "    return dz_dx, dz_dy\n",
    "\n",
    "\n",
    "# 実行と検証\n",
    "x, y = 2.0, 3.0\n",
    "dz_dx, dz_dy = trace_branching(x, y)\n",
    "\n",
    "# 数値微分で検証\n",
    "h = 1e-7\n",
    "numerical_dz_dx = (branching_example(x + h, y) - branching_example(x - h, y)) / (2 * h)\n",
    "numerical_dz_dy = (branching_example(x, y + h) - branching_example(x, y - h)) / (2 * h)\n",
    "\n",
    "print(f\"\\n[数値微分による検証]\")\n",
    "print(f\"  ∂z/∂x: 解析的 = {dz_dx:.6f}, 数値 = {numerical_dz_dx:.6f}\")\n",
    "print(f\"  ∂z/∂y: 解析的 = {dz_dy:.6f}, 数値 = {numerical_dz_dy:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 分岐と合流の視覚化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分岐・合流を持つ計算グラフを図示\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "# ノードの定義\n",
    "nodes = {\n",
    "    'x': (1, 4),\n",
    "    'y': (1, 2),\n",
    "    'mul': (4, 3),\n",
    "    'sin': (4, 5),\n",
    "    'add': (7, 4),\n",
    "    'z': (10, 4),\n",
    "}\n",
    "\n",
    "# ノードを描画\n",
    "node_labels = {\n",
    "    'x': ('x', 'lightblue'),\n",
    "    'y': ('y', 'lightblue'),\n",
    "    'mul': ('×', 'lightgreen'),\n",
    "    'sin': ('sin', 'lightyellow'),\n",
    "    'add': ('+', 'lightgreen'),\n",
    "    'z': ('z', 'lightcoral'),\n",
    "}\n",
    "\n",
    "for name, (nx, ny) in nodes.items():\n",
    "    label, color = node_labels[name]\n",
    "    circle = plt.Circle((nx, ny), 0.5, facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(nx, ny, label, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# エッジ（順伝播）\n",
    "edges = [\n",
    "    ('x', 'mul', '', 'black'),\n",
    "    ('y', 'mul', '', 'black'),\n",
    "    ('x', 'sin', '', 'black'),\n",
    "    ('mul', 'add', '', 'black'),\n",
    "    ('sin', 'add', '', 'black'),\n",
    "    ('add', 'z', '', 'black'),\n",
    "]\n",
    "\n",
    "for start, end, label, color in edges:\n",
    "    sx, sy = nodes[start]\n",
    "    ex, ey = nodes[end]\n",
    "    ax.annotate('', xy=(ex - 0.5, ey), xytext=(sx + 0.5, sy),\n",
    "                arrowprops=dict(arrowstyle='->', color=color, lw=1.5))\n",
    "\n",
    "# 勾配のラベル（逆伝播の説明）\n",
    "gradient_labels = [\n",
    "    (2.5, 3.8, r'$\\frac{\\partial(xy)}{\\partial x} = y$', 'blue'),\n",
    "    (2.5, 2.2, r'$\\frac{\\partial(xy)}{\\partial y} = x$', 'blue'),\n",
    "    (2.5, 4.8, r'$\\frac{\\partial\\sin(x)}{\\partial x} = \\cos(x)$', 'blue'),\n",
    "    (5.5, 3.8, '1', 'red'),\n",
    "    (5.5, 4.8, '1', 'red'),\n",
    "    (8.5, 4.2, '1', 'red'),\n",
    "]\n",
    "\n",
    "for lx, ly, text, color in gradient_labels:\n",
    "    ax.text(lx, ly, text, ha='center', va='center', fontsize=10, color=color)\n",
    "\n",
    "# 分岐点の強調\n",
    "ax.annotate('分岐\\n(x が2経路へ)', xy=(1, 4), xytext=(0, 5.5),\n",
    "            fontsize=10, ha='center',\n",
    "            arrowprops=dict(arrowstyle='->', color='purple', lw=1))\n",
    "\n",
    "# 合流点の強調\n",
    "ax.annotate('合流\\n(勾配が加算される)', xy=(7, 4), xytext=(7, 1.5),\n",
    "            fontsize=10, ha='center',\n",
    "            arrowprops=dict(arrowstyle='->', color='purple', lw=1))\n",
    "\n",
    "ax.set_title('分岐と合流を持つ計算グラフ: $z = xy + \\\\sin(x)$', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【重要なルール】\")\n",
    "print(\"・分岐: 同じ変数が複数の演算に使われる\")\n",
    "print(\"・合流: 逆伝播では、複数経路からの勾配を『加算』する\")\n",
    "print(\"\")\n",
    "print(\"∂z/∂x = (経路1: xy経由) + (経路2: sin経由)\")\n",
    "print(\"       = y + cos(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ニューラルネットワークへの適用\n",
    "\n",
    "### 5.1 単純なニューロン $y = \\sigma(Wx + b)$\n",
    "\n",
    "最も基本的なニューラルネットワークの単位を微分してみましょう。\n",
    "\n",
    "**パラメータ**:\n",
    "- 入力: $x$\n",
    "- 重み: $W$\n",
    "- バイアス: $b$\n",
    "- 活性化関数: $\\sigma$ (シグモイド)\n",
    "\n",
    "**計算グラフ**:\n",
    "$$\n",
    "x \\xrightarrow{W \\cdot} z_1 = Wx \\xrightarrow{+ b} z_2 = Wx + b \\xrightarrow{\\sigma} y = \\sigma(Wx + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = σ(Wx + b) の完全な順伝播・逆伝播\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "def neuron_forward_backward(x, W, b, verbose=True):\n",
    "    \"\"\"\n",
    "    y = σ(Wx + b) の順伝播と逆伝播\n",
    "    \n",
    "    Returns:\n",
    "        y: 出力\n",
    "        grads: 各パラメータの勾配 {dy_dx, dy_dW, dy_db}\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n【単一ニューロン y = σ(Wx + b)】\")\n",
    "        print(f\"入力: x = {x}, W = {W}, b = {b}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # ========== 順伝播 ==========\n",
    "    if verbose:\n",
    "        print(\"\\n[順伝播]\")\n",
    "    \n",
    "    # Step 1: z1 = W * x\n",
    "    z1 = W * x\n",
    "    if verbose:\n",
    "        print(f\"  z1 = W × x = {W} × {x} = {z1}\")\n",
    "    \n",
    "    # Step 2: z2 = z1 + b\n",
    "    z2 = z1 + b\n",
    "    if verbose:\n",
    "        print(f\"  z2 = z1 + b = {z1} + {b} = {z2}\")\n",
    "    \n",
    "    # Step 3: y = σ(z2)\n",
    "    y = sigmoid(z2)\n",
    "    if verbose:\n",
    "        print(f\"  y = σ(z2) = σ({z2}) = {y:.6f}\")\n",
    "    \n",
    "    # ========== 逆伝播 ==========\n",
    "    if verbose:\n",
    "        print(\"\\n[逆伝播]\")\n",
    "        print(\"  dy/dy = 1（出発点）\")\n",
    "    \n",
    "    # Step 3の逆: dy/dz2 = σ'(z2) = σ(z2)(1 - σ(z2))\n",
    "    dy_dz2 = sigmoid_derivative(z2)\n",
    "    if verbose:\n",
    "        print(f\"\\n  σノード:\")\n",
    "        print(f\"    dy/dz2 = σ'(z2) = σ(z2)(1-σ(z2)) = {y:.6f} × {1-y:.6f} = {dy_dz2:.6f}\")\n",
    "    \n",
    "    # Step 2の逆: dz2/dz1 = 1, dz2/db = 1\n",
    "    dz2_dz1 = 1\n",
    "    dz2_db = 1\n",
    "    if verbose:\n",
    "        print(f\"\\n  加算ノード (+b):\")\n",
    "        print(f\"    dz2/dz1 = 1\")\n",
    "        print(f\"    dz2/db = 1\")\n",
    "    \n",
    "    # 連鎖律: dy/dz1, dy/db\n",
    "    dy_dz1 = dy_dz2 * dz2_dz1\n",
    "    dy_db = dy_dz2 * dz2_db\n",
    "    if verbose:\n",
    "        print(f\"    dy/dz1 = dy/dz2 × dz2/dz1 = {dy_dz2:.6f} × 1 = {dy_dz1:.6f}\")\n",
    "        print(f\"    dy/db = dy/dz2 × dz2/db = {dy_dz2:.6f} × 1 = {dy_db:.6f}\")\n",
    "    \n",
    "    # Step 1の逆: dz1/dW = x, dz1/dx = W\n",
    "    dz1_dW = x\n",
    "    dz1_dx = W\n",
    "    if verbose:\n",
    "        print(f\"\\n  乗算ノード (W×x):\")\n",
    "        print(f\"    dz1/dW = x = {x}\")\n",
    "        print(f\"    dz1/dx = W = {W}\")\n",
    "    \n",
    "    # 連鎖律: dy/dW, dy/dx\n",
    "    dy_dW = dy_dz1 * dz1_dW\n",
    "    dy_dx = dy_dz1 * dz1_dx\n",
    "    if verbose:\n",
    "        print(f\"    dy/dW = dy/dz1 × dz1/dW = {dy_dz1:.6f} × {x} = {dy_dW:.6f}\")\n",
    "        print(f\"    dy/dx = dy/dz1 × dz1/dx = {dy_dz1:.6f} × {W} = {dy_dx:.6f}\")\n",
    "    \n",
    "    # 結果のまとめ\n",
    "    if verbose:\n",
    "        print(\"\\n[最終結果]\")\n",
    "        print(f\"  ∂y/∂x = {dy_dx:.6f}\")\n",
    "        print(f\"  ∂y/∂W = {dy_dW:.6f}\")\n",
    "        print(f\"  ∂y/∂b = {dy_db:.6f}\")\n",
    "    \n",
    "    return y, {'dy_dx': dy_dx, 'dy_dW': dy_dW, 'dy_db': dy_db}\n",
    "\n",
    "\n",
    "# 実行\n",
    "x, W, b = 2.0, 0.5, -0.3\n",
    "y, grads = neuron_forward_backward(x, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値微分で検証\n",
    "def neuron_output(x, W, b):\n",
    "    return sigmoid(W * x + b)\n",
    "\n",
    "\n",
    "h = 1e-7\n",
    "x, W, b = 2.0, 0.5, -0.3\n",
    "\n",
    "# 数値微分\n",
    "numerical_dy_dx = (neuron_output(x + h, W, b) - neuron_output(x - h, W, b)) / (2 * h)\n",
    "numerical_dy_dW = (neuron_output(x, W + h, b) - neuron_output(x, W - h, b)) / (2 * h)\n",
    "numerical_dy_db = (neuron_output(x, W, b + h) - neuron_output(x, W, b - h)) / (2 * h)\n",
    "\n",
    "# 比較\n",
    "y, grads = neuron_forward_backward(x, W, b, verbose=False)\n",
    "\n",
    "print(\"【勾配チェック: y = σ(Wx + b)】\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'パラメータ':>10} | {'解析的勾配':>12} | {'数値微分':>12} | {'誤差':>10}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'∂y/∂x':>10} | {grads['dy_dx']:>12.8f} | {numerical_dy_dx:>12.8f} | {abs(grads['dy_dx'] - numerical_dy_dx):>10.2e}\")\n",
    "print(f\"{'∂y/∂W':>10} | {grads['dy_dW']:>12.8f} | {numerical_dy_dW:>12.8f} | {abs(grads['dy_dW'] - numerical_dy_dW):>10.2e}\")\n",
    "print(f\"{'∂y/∂b':>10} | {grads['dy_db']:>12.8f} | {numerical_dy_db:>12.8f} | {abs(grads['dy_db'] - numerical_dy_db):>10.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 損失関数を含めた完全な逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数を追加: L = (y - t)² / 2\n",
    "# t: 教師信号（正解）\n",
    "\n",
    "def full_forward_backward(x, W, b, t, verbose=True):\n",
    "    \"\"\"\n",
    "    完全な順伝播・逆伝播\n",
    "    \n",
    "    順伝播: x → z = Wx + b → y = σ(z) → L = (y - t)² / 2\n",
    "    逆伝播: dL/dW, dL/db を計算\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n【完全な順伝播・逆伝播】\")\n",
    "        print(f\"入力: x = {x}, W = {W}, b = {b}, t(正解) = {t}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # ========== 順伝播 ==========\n",
    "    z = W * x + b\n",
    "    y = sigmoid(z)\n",
    "    L = 0.5 * (y - t) ** 2\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n[順伝播]\")\n",
    "        print(f\"  z = Wx + b = {W} × {x} + {b} = {z}\")\n",
    "        print(f\"  y = σ(z) = {y:.6f}\")\n",
    "        print(f\"  L = (y - t)²/2 = ({y:.6f} - {t})²/2 = {L:.6f}\")\n",
    "    \n",
    "    # ========== 逆伝播 ==========\n",
    "    if verbose:\n",
    "        print(\"\\n[逆伝播]\")\n",
    "    \n",
    "    # dL/dy = y - t\n",
    "    dL_dy = y - t\n",
    "    if verbose:\n",
    "        print(f\"  dL/dy = y - t = {y:.6f} - {t} = {dL_dy:.6f}\")\n",
    "    \n",
    "    # dy/dz = σ'(z)\n",
    "    dy_dz = sigmoid_derivative(z)\n",
    "    if verbose:\n",
    "        print(f\"  dy/dz = σ'(z) = {dy_dz:.6f}\")\n",
    "    \n",
    "    # dL/dz = dL/dy × dy/dz (これが δ: デルタ と呼ばれる)\n",
    "    dL_dz = dL_dy * dy_dz\n",
    "    if verbose:\n",
    "        print(f\"  dL/dz = dL/dy × dy/dz = {dL_dy:.6f} × {dy_dz:.6f} = {dL_dz:.6f}\")\n",
    "        print(f\"  (この dL/dz が『誤差信号 δ』と呼ばれる)\")\n",
    "    \n",
    "    # dL/dW = dL/dz × dz/dW = δ × x\n",
    "    dL_dW = dL_dz * x\n",
    "    if verbose:\n",
    "        print(f\"\\n  dL/dW = δ × x = {dL_dz:.6f} × {x} = {dL_dW:.6f}\")\n",
    "    \n",
    "    # dL/db = dL/dz × dz/db = δ × 1\n",
    "    dL_db = dL_dz * 1\n",
    "    if verbose:\n",
    "        print(f\"  dL/db = δ × 1 = {dL_db:.6f}\")\n",
    "    \n",
    "    # dL/dx = dL/dz × dz/dx = δ × W（次の層への逆伝播用）\n",
    "    dL_dx = dL_dz * W\n",
    "    if verbose:\n",
    "        print(f\"  dL/dx = δ × W = {dL_dz:.6f} × {W} = {dL_dx:.6f}（次の層へ伝播）\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n[最終結果: パラメータ更新に使う勾配]\")\n",
    "        print(f\"  ∂L/∂W = {dL_dW:.6f}\")\n",
    "        print(f\"  ∂L/∂b = {dL_db:.6f}\")\n",
    "    \n",
    "    return L, {'dL_dW': dL_dW, 'dL_db': dL_db, 'dL_dx': dL_dx}\n",
    "\n",
    "\n",
    "# 実行\n",
    "x, W, b, t = 2.0, 0.5, -0.3, 1.0\n",
    "L, grads = full_forward_backward(x, W, b, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 連鎖律の効率性：なぜO(n)なのか\n",
    "\n",
    "### 6.1 計算量の分析\n",
    "\n",
    "$n$ 層のネットワークの全パラメータに対する勾配を計算する場合：\n",
    "\n",
    "**ナイーブな方法（数値微分）**:\n",
    "- 各パラメータごとに順伝播を2回 → $O(n \\times \\text{パラメータ数})$\n",
    "- 数百万パラメータのモデルでは非現実的\n",
    "\n",
    "**連鎖律（逆伝播）**:\n",
    "- 順伝播1回 + 逆伝播1回 → $O(n)$\n",
    "- パラメータ数に依存しない！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算量の比較を可視化\n",
    "import time\n",
    "\n",
    "def benchmark_gradient_computation(n_params_list):\n",
    "    \"\"\"勾配計算の計算時間を比較\"\"\"\n",
    "    numerical_times = []\n",
    "    analytical_times = []\n",
    "    \n",
    "    for n_params in n_params_list:\n",
    "        # ダミーの重み\n",
    "        W = np.random.randn(n_params)\n",
    "        x = np.random.randn()\n",
    "        \n",
    "        # 簡単な関数: y = Σ W_i * x\n",
    "        def forward(W, x):\n",
    "            return np.sum(W * x)\n",
    "        \n",
    "        # 数値微分（各パラメータに対して）\n",
    "        start = time.time()\n",
    "        h = 1e-7\n",
    "        numerical_grads = np.zeros(n_params)\n",
    "        for i in range(n_params):\n",
    "            W_plus = W.copy()\n",
    "            W_plus[i] += h\n",
    "            W_minus = W.copy()\n",
    "            W_minus[i] -= h\n",
    "            numerical_grads[i] = (forward(W_plus, x) - forward(W_minus, x)) / (2 * h)\n",
    "        numerical_times.append(time.time() - start)\n",
    "        \n",
    "        # 解析的勾配（連鎖律：一度に全部）\n",
    "        start = time.time()\n",
    "        analytical_grads = np.full(n_params, x)  # dy/dW_i = x\n",
    "        analytical_times.append(time.time() - start)\n",
    "    \n",
    "    return numerical_times, analytical_times\n",
    "\n",
    "\n",
    "# ベンチマーク実行\n",
    "n_params_list = [10, 50, 100, 500, 1000, 5000]\n",
    "numerical_times, analytical_times = benchmark_gradient_computation(n_params_list)\n",
    "\n",
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(n_params_list, numerical_times, 'ro-', label='数値微分 O(n × パラメータ数)', linewidth=2, markersize=8)\n",
    "ax.plot(n_params_list, analytical_times, 'bs-', label='連鎖律（逆伝播） O(n)', linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('パラメータ数', fontsize=12)\n",
    "ax.set_ylabel('計算時間 (秒)', fontsize=12)\n",
    "ax.set_title('勾配計算の計算量比較', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【計算効率の違い】\")\n",
    "print(f\"パラメータ数 5000 の場合:\")\n",
    "print(f\"  数値微分: {numerical_times[-1]:.4f} 秒\")\n",
    "print(f\"  連鎖律:   {analytical_times[-1]:.6f} 秒\")\n",
    "print(f\"  速度比:   {numerical_times[-1] / analytical_times[-1]:.0f} 倍\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 演習問題\n",
    "\n",
    "### 演習 7.1: 連鎖律の手計算\n",
    "\n",
    "次の合成関数の微分を、連鎖律を使って手計算で求めてください。\n",
    "\n",
    "$$\n",
    "y = \\ln(1 + e^{2x})\n",
    "$$\n",
    "\n",
    "ヒント: softplus関数と呼ばれる活性化関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.1: 解答欄\n",
    "\n",
    "def softplus_2x(x):\n",
    "    \"\"\"y = ln(1 + e^(2x))\"\"\"\n",
    "    return np.log(1 + np.exp(2*x))\n",
    "\n",
    "\n",
    "def softplus_2x_derivative(x):\n",
    "    \"\"\"\n",
    "    TODO: 連鎖律で導出した導関数を実装\n",
    "    \n",
    "    ヒント:\n",
    "    - u = 2x, v = e^u, w = 1 + v, y = ln(w)\n",
    "    - dy/dx = dy/dw × dw/dv × dv/du × du/dx\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# テストコード（実装後にコメントを外して実行）\n",
    "# test_points = [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "# print(\"softplus(2x) = ln(1 + e^(2x)) の勾配チェック\")\n",
    "# for x in test_points:\n",
    "#     analytical = softplus_2x_derivative(x)\n",
    "#     numerical = numerical_diff(softplus_2x, x)\n",
    "#     print(f\"x = {x:5.1f}: 解析的 = {analytical:.6f}, 数値 = {numerical:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 7.2: 2入力ニューロン\n",
    "\n",
    "入力が2つの場合のニューロン $y = \\sigma(w_1 x_1 + w_2 x_2 + b)$ について、$\\frac{\\partial y}{\\partial w_1}$ と $\\frac{\\partial y}{\\partial w_2}$ を導出し、実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.2: 解答欄\n",
    "\n",
    "def two_input_neuron_forward(x1, x2, w1, w2, b):\n",
    "    \"\"\"y = σ(w1*x1 + w2*x2 + b)\"\"\"\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    y = sigmoid(z)\n",
    "    return y\n",
    "\n",
    "\n",
    "def two_input_neuron_backward(x1, x2, w1, w2, b):\n",
    "    \"\"\"\n",
    "    TODO: 逆伝播を実装\n",
    "    \n",
    "    Returns:\n",
    "        dy_dw1, dy_dw2, dy_db\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# テストコード（実装後にコメントを外して実行）\n",
    "# x1, x2, w1, w2, b = 1.0, 2.0, 0.5, -0.3, 0.1\n",
    "# dy_dw1, dy_dw2, dy_db = two_input_neuron_backward(x1, x2, w1, w2, b)\n",
    "# \n",
    "# # 数値微分で検証\n",
    "# h = 1e-7\n",
    "# num_dy_dw1 = (two_input_neuron_forward(x1, x2, w1+h, w2, b) - \n",
    "#               two_input_neuron_forward(x1, x2, w1-h, w2, b)) / (2*h)\n",
    "# print(f\"dy/dw1: 解析的 = {dy_dw1:.6f}, 数値 = {num_dy_dw1:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習 7.3: ReLU活性化関数\n",
    "\n",
    "シグモイドの代わりにReLUを使った場合、$y = \\text{ReLU}(Wx + b)$ の逆伝播を実装してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演習 7.3: 解答欄\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"\n",
    "    TODO: ReLUの導関数を実装\n",
    "    ヒント: z > 0 なら 1, そうでなければ 0\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def relu_neuron_backward(x, W, b):\n",
    "    \"\"\"\n",
    "    y = ReLU(Wx + b) の逆伝播\n",
    "    \n",
    "    TODO: dy/dW, dy/db, dy/dx を計算\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# テストコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. まとめと次のステップ\n",
    "\n",
    "### このノートブックで学んだこと\n",
    "\n",
    "1. **連鎖律の本質**: 合成関数の微分 = 局所的な微分の積\n",
    "   $$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n",
    "\n",
    "2. **多段階の連鎖**: 深い合成関数でも同じ原理を繰り返し適用\n",
    "\n",
    "3. **分岐と合流**:\n",
    "   - 分岐: 1つの変数が複数の経路に影響\n",
    "   - 合流: 複数の経路からの勾配を **加算**\n",
    "\n",
    "4. **ニューラルネットワークへの適用**:\n",
    "   - $y = \\sigma(Wx + b)$ の逆伝播\n",
    "   - 誤差信号 $\\delta = \\frac{\\partial L}{\\partial z}$ の概念\n",
    "\n",
    "5. **計算効率**: 連鎖律により $O(n)$ で全パラメータの勾配を計算可能\n",
    "\n",
    "### 次のノートブック（72: 計算グラフ）への橋渡し\n",
    "\n",
    "連鎖律の計算を **自動化** するために、計算を **グラフ構造** として表現します：\n",
    "\n",
    "- 各演算をノード（Node）として表現\n",
    "- 順伝播: データがグラフを **前向き** に流れる\n",
    "- 逆伝播: 勾配がグラフを **後ろ向き** に流れる\n",
    "\n",
    "次のノートブックでは、この計算グラフをPythonクラスで実装します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 付録: 連鎖律の公式集\n",
    "\n",
    "### 基本形\n",
    "\n",
    "| 形式 | 連鎖律 |\n",
    "|------|--------|\n",
    "| $y = f(g(x))$ | $\\frac{dy}{dx} = f'(g(x)) \\cdot g'(x)$ |\n",
    "| $y = f(g(h(x)))$ | $\\frac{dy}{dx} = f'(g(h(x))) \\cdot g'(h(x)) \\cdot h'(x)$ |\n",
    "\n",
    "### 多変数への拡張\n",
    "\n",
    "| 状況 | 連鎖律 |\n",
    "|------|--------|\n",
    "| $z = f(x, y)$, $x = x(t)$, $y = y(t)$ | $\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial z}{\\partial y}\\frac{dy}{dt}$ |\n",
    "| 分岐（$x$ が複数経路に影響） | 各経路からの勾配を **加算** |\n",
    "\n",
    "### ニューラルネットワークでよく使う形\n",
    "\n",
    "| 演算 | 順伝播 | 逆伝播 |\n",
    "|------|--------|--------|\n",
    "| 加算 $z = x + y$ | そのまま | $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z}$, $\\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial z}$ |\n",
    "| 乗算 $z = x \\cdot y$ | そのまま | $\\frac{\\partial L}{\\partial x} = y \\cdot \\frac{\\partial L}{\\partial z}$, $\\frac{\\partial L}{\\partial y} = x \\cdot \\frac{\\partial L}{\\partial z}$ |\n",
    "| シグモイド $y = \\sigma(x)$ | $\\sigma(x)$ | $\\frac{\\partial L}{\\partial x} = \\sigma(x)(1-\\sigma(x)) \\cdot \\frac{\\partial L}{\\partial y}$ |\n",
    "| ReLU $y = \\max(0, x)$ | $\\max(0, x)$ | $\\frac{\\partial L}{\\partial x} = \\mathbf{1}_{x > 0} \\cdot \\frac{\\partial L}{\\partial y}$ |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
