{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. Tabularディープラーニング - Neural Networks for Tabular Data\n",
    "\n",
    "## 概要\n",
    "テーブルデータに対してニューラルネットワークを適用し、GBDTとの性能比較を行います。\n",
    "\n",
    "## 学習目標\n",
    "- Tabularデータでのディープラーニングの特徴を理解できる\n",
    "- PyTorchでシンプルなニューラルネットワークを実装できる\n",
    "- GBDTとの比較ができる\n",
    "- 実務での使い分けができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 設定\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# デバイス設定\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tabularデータとディープラーニング\n",
    "\n",
    "### Tabularデータの特徴\n",
    "\n",
    "テーブルデータ（行列形式）は最も一般的なデータ形式です。\n",
    "\n",
    "### 従来の常識\n",
    "\n",
    "**GBDT（Gradient Boosting Decision Trees）が最強**\n",
    "- XGBoost、LightGBM、CatBoost\n",
    "- Kaggleコンペで圧倒的な実績\n",
    "- 特徴量エンジニアリング不要\n",
    "\n",
    "### ディープラーニングの可能性\n",
    "\n",
    "最近の研究により、適切に設計すればNNもGBDTと同等以上の性能を発揮できることが分かってきました。\n",
    "\n",
    "**いつNNを使うべきか？**\n",
    "- データサイズが大きい（10万サンプル以上）\n",
    "- 埋め込み学習が必要\n",
    "- エンドツーエンドの学習が必要\n",
    "- GPUが使える環境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データサイズ: (569, 30)\n",
      "特徴量数: 30\n",
      "サンプル数: 569\n",
      "\n",
      "ターゲット分布:\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n",
      "\n",
      "特徴量の例（最初の5列）:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness\n",
      "0        17.99         10.38          122.80     1001.0          0.11840\n",
      "1        20.57         17.77          132.90     1326.0          0.08474\n",
      "2        19.69         21.25          130.00     1203.0          0.10960\n",
      "3        11.42         20.38           77.58      386.1          0.14250\n",
      "4        20.29         14.34          135.10     1297.0          0.10030\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み（乳がん診断データ）\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "print(f\"データサイズ: {X.shape}\")\n",
    "print(f\"特徴量数: {X.shape[1]}\")\n",
    "print(f\"サンプル数: {X.shape[0]}\")\n",
    "print(f\"\\nターゲット分布:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\n特徴量の例（最初の5列）:\")\n",
    "print(X.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練データ: (364, 30)\n",
      "検証データ: (91, 30)\n",
      "テストデータ: (114, 30)\n",
      "\n",
      "重要: ディープラーニングでは特徴量のスケーリングが必須\n",
      "訓練データでfitし、検証・テストデータはtransformのみ\n"
     ]
    }
   ],
   "source": [
    "# データ分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# さらに検証データを分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"訓練データ: {X_train.shape}\")\n",
    "print(f\"検証データ: {X_val.shape}\")\n",
    "print(f\"テストデータ: {X_test.shape}\")\n",
    "\n",
    "# スケーリング（ディープラーニングには必須）\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n重要: ディープラーニングでは特徴量のスケーリングが必須\")\n",
    "print(\"訓練データでfitし、検証・テストデータはtransformのみ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. シンプルなニューラルネットワークの構築\n",
    "\n",
    "### アーキテクチャの設計\n",
    "\n",
    "Tabularデータには深すぎないネットワークが有効です。\n",
    "\n",
    "```\n",
    "Input (30 features)\n",
    "    ↓\n",
    "Linear(30 → 64) + ReLU + Dropout(0.3)\n",
    "    ↓\n",
    "Linear(64 → 32) + ReLU + Dropout(0.2)\n",
    "    ↓\n",
    "Linear(32 → 2)\n",
    "    ↓\n",
    "Output (2 classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルアーキテクチャ:\n",
      "SimpleTabularNN(\n",
      "  (fc1): Linear(in_features=30, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "\n",
      "パラメータ数: 4,130\n"
     ]
    }
   ],
   "source": [
    "# シンプルなニューラルネットワーク\n",
    "class SimpleTabularNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=64, hidden_dim2=32, output_dim=2, dropout1=0.3, dropout2=0.2):\n",
    "        super(SimpleTabularNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# モデルの初期化\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = SimpleTabularNN(input_dim=input_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"モデルアーキテクチャ:\")\n",
    "print(model)\n",
    "print(f\"\\nパラメータ数: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習の準備完了\n",
      "- 損失関数: CrossEntropyLoss\n",
      "- オプティマイザ: Adam (lr=0.001)\n",
      "- バッチサイズ: 32\n"
     ]
    }
   ],
   "source": [
    "# データローダーの準備\n",
    "def create_dataloader(X, y, batch_size=32, shuffle=True):\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    y_tensor = torch.LongTensor(y.values if isinstance(y, pd.Series) else y)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = create_dataloader(X_train_scaled, y_train, batch_size=32, shuffle=True)\n",
    "val_loader = create_dataloader(X_val_scaled, y_val, batch_size=32, shuffle=False)\n",
    "test_loader = create_dataloader(X_test_scaled, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# 損失関数とオプティマイザ\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "print(\"学習の準備完了\")\n",
    "print(f\"- 損失関数: CrossEntropyLoss\")\n",
    "print(f\"- オプティマイザ: Adam (lr=0.001)\")\n",
    "print(f\"- バッチサイズ: 32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習関数の準備完了\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 学習関数\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# 評価関数\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    return total_loss / len(loader), correct / total, auc\n",
    "\n",
    "print(\"学習関数の準備完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習の実行\n",
    "num_epochs = 100\n",
    "best_val_auc = 0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_auc': []\n",
    "}\n",
    "\n",
    "print(\"学習開始...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 訓練\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # 検証\n",
    "    val_loss, val_acc, val_auc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # スケジューラの更新\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # 履歴の保存\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_epoch = epoch\n",
    "        patience_counter = 0\n",
    "        # ベストモデルの保存\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "        print(f\"  Best Val AUC: {best_val_auc:.4f} (Epoch {best_epoch+1})\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# ベストモデルをロード\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "print(f\"\\n学習完了\")\n",
    "print(f\"最良エポック: {best_epoch+1}\")\n",
    "print(f\"最良検証AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 学習曲線の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線の可視化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss曲線\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].axvline(best_epoch, color='red', linestyle='--', \n",
    "                label=f'Best Epoch ({best_epoch+1})', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy曲線\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].axvline(best_epoch, color='red', linestyle='--', \n",
    "                label=f'Best Epoch ({best_epoch+1})', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# AUC曲線\n",
    "axes[2].plot(history['val_auc'], label='Validation AUC', linewidth=2, color='orange')\n",
    "axes[2].axvline(best_epoch, color='red', linestyle='--', \n",
    "                label=f'Best Epoch ({best_epoch+1})', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch', fontsize=11)\n",
    "axes[2].set_ylabel('AUC', fontsize=11)\n",
    "axes[2].set_title('Validation AUC', fontsize=12, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n観察:\")\n",
    "print(f\"- 最良エポック: {best_epoch+1}\")\n",
    "print(f\"- Early stoppingにより過学習を防止\")\n",
    "print(f\"- 検証AUCは {best_val_auc:.4f} に到達\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. テストデータでの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータで評価\n",
    "model.eval()\n",
    "y_pred_nn = []\n",
    "y_proba_nn = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = outputs.max(1)\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "        \n",
    "        y_pred_nn.extend(predicted.cpu().numpy())\n",
    "        y_proba_nn.extend(probs.cpu().numpy())\n",
    "\n",
    "y_pred_nn = np.array(y_pred_nn)\n",
    "y_proba_nn = np.array(y_proba_nn)\n",
    "\n",
    "# 評価\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "auc_nn = roc_auc_score(y_test, y_proba_nn)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Neural Networkの性能（テストデータ）\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_nn:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_nn:.4f}\")\n",
    "print(\"\\n混同行列:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n",
    "print(\"\\n詳細レポート:\")\n",
    "print(classification_report(y_test, y_pred_nn, \n",
    "                           target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. GBDTとの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "print(\"LightGBMの学習中...\")\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "y_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "auc_lgb = roc_auc_score(y_test, y_proba_lgb)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LightGBMの性能（テストデータ）\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_lgb:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_lgb:.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "print(\"\\nXGBoostの学習中...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"XGBoostの性能（テストデータ）\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"ROC-AUC:  {auc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能比較\n",
    "comparison_df = pd.DataFrame([\n",
    "    {'Model': 'Neural Network', 'Accuracy': accuracy_nn, 'ROC-AUC': auc_nn},\n",
    "    {'Model': 'LightGBM', 'Accuracy': accuracy_lgb, 'ROC-AUC': auc_lgb},\n",
    "    {'Model': 'XGBoost', 'Accuracy': accuracy_xgb, 'ROC-AUC': auc_xgb}\n",
    "]).sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"モデル性能比較\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "colors = ['steelblue', 'lightgreen', 'coral']\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Accuracy'], \n",
    "            alpha=0.7, edgecolor='black', color=colors)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[0].text(row.name, row['Accuracy'], f\"{row['Accuracy']:.4f}\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['ROC-AUC'], \n",
    "            alpha=0.7, edgecolor='black', color=colors)\n",
    "axes[1].set_ylabel('ROC-AUC', fontsize=11)\n",
    "axes[1].set_title('ROC-AUC Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[1].text(row.name, row['ROC-AUC'], f\"{row['ROC-AUC']:.4f}\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n観察:\")\n",
    "print(\"- ニューラルネットワークはGBDTと競争力のある性能を示す\")\n",
    "print(\"- 小規模データではGBDTも依然として強力\")\n",
    "print(\"- データサイズや問題によって最適なモデルは異なる\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 特徴量重要度の比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBMの特徴量重要度\n",
    "lgb_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': lgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# XGBoostの特徴量重要度\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# LightGBM\n",
    "top_lgb = lgb_importance.head(10)\n",
    "axes[0].barh(range(len(top_lgb)), top_lgb['importance'], color='lightgreen', edgecolor='black')\n",
    "axes[0].set_yticks(range(len(top_lgb)))\n",
    "axes[0].set_yticklabels(top_lgb['feature'], fontsize=9)\n",
    "axes[0].set_xlabel('Importance', fontsize=11)\n",
    "axes[0].set_title('LightGBM - Top 10 Features', fontsize=12, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# XGBoost\n",
    "top_xgb = xgb_importance.head(10)\n",
    "axes[1].barh(range(len(top_xgb)), top_xgb['importance'], color='coral', edgecolor='black')\n",
    "axes[1].set_yticks(range(len(top_xgb)))\n",
    "axes[1].set_yticklabels(top_xgb['feature'], fontsize=9)\n",
    "axes[1].set_xlabel('Importance', fontsize=11)\n",
    "axes[1].set_title('XGBoost - Top 10 Features', fontsize=12, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 重要な特徴量:\")\n",
    "print(\"\\nLightGBM:\")\n",
    "for i, row in lgb_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nXGBoost:\")\n",
    "for i, row in xgb_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:30s}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. まとめ\n",
    "\n",
    "### 本ノートブックで学んだこと\n",
    "\n",
    "1. **Tabularデータでのディープラーニング**\n",
    "   - シンプルなアーキテクチャで十分な性能\n",
    "   - スケーリングが必須\n",
    "   - Dropout、Early stoppingで過学習防止\n",
    "\n",
    "2. **GBDTとの比較**\n",
    "   - NNもGBDTと競争力のある性能\n",
    "   - データサイズや問題によって最適なモデルは異なる\n",
    "   - 両方試して比較するのがベスト\n",
    "\n",
    "3. **実務での使い分け**\n",
    "   - 小規模データ（<10k）: GBDT推奨\n",
    "   - 大規模データ（>100k）: NN検討\n",
    "   - GPU利用可能: NN有利\n",
    "   - 解釈可能性重視: GBDTの特徴量重要度\n",
    "\n",
    "### 重要なポイント\n",
    "\n",
    "- スケーリング必須（StandardScaler推奨）\n",
    "- Early stoppingで過学習を防ぐ\n",
    "- 深すぎないネットワーク（2-3層）\n",
    "- まずGBDTを試し、NNで改善を図る\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "- より複雑なアーキテクチャの実験\n",
    "- ハイパーパラメータチューニング\n",
    "- Kaggleコンペでの実践\n",
    "- TabNet、FT-Transformerなど専用アーキテクチャの学習"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
