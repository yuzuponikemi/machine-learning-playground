{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯17: Titanic Top 30%é”æˆ - å®Ÿè·µKaggleå®Œå…¨ã‚¬ã‚¤ãƒ‰ ğŸ†\n",
    "\n",
    "**å­¦ç¿’ç›®æ¨™**: Kaggle Titanicã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§Top 30%ï¼ˆPublic LB 0.79+ï¼‰ã‚’é”æˆ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å­¦ã¶ã“ã¨\n",
    "\n",
    "### 1. é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° â­â­â­\n",
    "- Ticket prefix extractionï¼ˆãƒã‚±ãƒƒãƒˆç•ªå·ã‹ã‚‰éšç´šã‚’æ¨å®šï¼‰\n",
    "- Cabin deck analysisï¼ˆå®¢å®¤ãƒ‡ãƒƒã‚­æƒ…å ±ã®æ´»ç”¨ï¼‰\n",
    "- Title extractionï¼ˆæ•¬ç§°ã®æŠ½å‡ºã¨åˆ†é¡ï¼‰\n",
    "- Age imputationï¼ˆæ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å¹´é½¢è£œå®Œï¼‰\n",
    "- Feature interactionsï¼ˆç‰¹å¾´é‡ã®äº¤äº’ä½œç”¨ï¼‰\n",
    "\n",
    "### 2. Optunaã«ã‚ˆã‚‹è‡ªå‹•æœ€é©åŒ– â­â­â­\n",
    "- Bayesian Optimizationã®å®Ÿè·µ\n",
    "- LightGBMã€XGBoostã€CatBoostã®æœ€é©åŒ–\n",
    "- CVã‚¹ã‚³ã‚¢ã‚’æœ€å¤§åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢\n",
    "\n",
    "### 3. å …ç‰¢ãªæ¤œè¨¼æˆ¦ç•¥ â­â­\n",
    "- 10-Fold Stratified Cross-Validation\n",
    "- Out-of-fold predictions\n",
    "- CV Score vs LB Scoreã®é–¢ä¿‚ç†è§£\n",
    "\n",
    "### 4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥ â­â­â­\n",
    "- LightGBM + XGBoost + CatBoostã®é‡ã¿ä»˜ãå¹³å‡\n",
    "- Blending vs Stacking\n",
    "- Calibrationï¼ˆç¢ºç‡èª¿æ•´ï¼‰\n",
    "\n",
    "### 5. Kaggleæå‡ºæˆ¦ç•¥ â­\n",
    "- Multiple submissions\n",
    "- Leaderboard probing\n",
    "- Score variance analysis\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ç›®æ¨™ã‚¹ã‚³ã‚¢\n",
    "\n",
    "- **Local CV**: 0.85+\n",
    "- **Public LB**: 0.79+ (Top 30%)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š å‰æçŸ¥è­˜\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å§‹ã‚ã‚‹å‰ã«ã€ä»¥ä¸‹ã‚’å®Œäº†ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ¨å¥¨ï¼š\n",
    "- ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯13-16ï¼ˆGBDTåŸºç¤ï¼‰\n",
    "- åŸºæœ¬çš„ãªPandasã€NumPyã€Matplotlib\n",
    "- LightGBMã€XGBoostã€CatBoostã®åŸºæœ¬æ“ä½œ\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿ã®é…ç½®**: Kaggleã‹ã‚‰`train.csv`ã¨`test.csv`ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€`../data/titanic/`ã«é…ç½®ã—ã¦ãã ã•ã„\n",
    "2. **è¨ˆç®—æ™‚é–“**: Optunaã®æœ€é©åŒ–ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆç´„30-60åˆ†ï¼‰\n",
    "3. **å†ç¾æ€§**: `random_state`ã‚’å›ºå®šã—ã¦ã„ã¾ã™ãŒã€ç’°å¢ƒã«ã‚ˆã‚Šè‹¥å¹²ã®å·®ãŒå‡ºã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã€ç’°å¢ƒã‚’æº–å‚™ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# GBDT\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# è¨­å®š\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰å›ºå®š\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "\n",
    "Kaggle Titanicãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰\n",
    "data_dir = Path('../data/titanic')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "try:\n",
    "    train = pd.read_csv(data_dir / 'train.csv')\n",
    "    test = pd.read_csv(data_dir / 'test.csv')\n",
    "    print(\"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®æ‰‹é †ã§ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¦ãã ã•ã„ï¼š\")\n",
    "    print(\"1. https://www.kaggle.com/c/titanic/data ã«ã‚¢ã‚¯ã‚»ã‚¹\")\n",
    "    print(\"2. train.csv ã¨ test.csv ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\")\n",
    "    print(f\"3. {data_dir} ã«é…ç½®\")\n",
    "    # ãƒ‡ãƒ¢ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆå®Ÿéš›ã«ã¯Kaggleã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¿…è¦ï¼‰\n",
    "    print(\"\\nâš ï¸ ãƒ‡ãƒ¢ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™ï¼ˆå®Ÿéš›ã®Kaggleæå‡ºã«ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ï¼‰\")\n",
    "    from sklearn.datasets import make_classification\n",
    "    X, y = make_classification(n_samples=891, n_features=10, n_informative=7, \n",
    "                                n_redundant=2, random_state=42)\n",
    "    train = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])\n",
    "    train['Survived'] = y\n",
    "    train['PassengerId'] = range(1, 892)\n",
    "    test = pd.DataFrame(X[:418], columns=[f'feature_{i}' for i in range(10)])\n",
    "    test['PassengerId'] = range(892, 1310)\n",
    "\n",
    "print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±:\")\n",
    "print(f\"- Train shape: {train.shape}\")\n",
    "print(f\"- Test shape: {test.shape}\")\n",
    "print(f\"\\næœ€åˆã®5è¡Œ:\")\n",
    "display(train.head())\n",
    "\n",
    "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ\n",
    "if 'Survived' in train.columns:\n",
    "    print(f\"\\nğŸ¯ ç”Ÿå­˜ç‡: {train['Survived'].mean():.2%}\")\n",
    "    print(train['Survived'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¨ 3. é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "\n",
    "Top 30%ã‚’é”æˆã™ã‚‹ãŸã‚ã®é«˜åº¦ãªç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_feature_engineering(df, is_train=True):\n",
    "    \"\"\"\n",
    "    é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "    is_train : bool\n",
    "        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã©ã†ã‹\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Titleï¼ˆæ•¬ç§°ï¼‰ã®æŠ½å‡º â­â­â­\n",
    "    if 'Name' in df.columns:\n",
    "        df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        \n",
    "        # ãƒ¬ã‚¢ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "        title_mapping = {\n",
    "            'Mr': 'Mr',\n",
    "            'Miss': 'Miss',\n",
    "            'Mrs': 'Mrs',\n",
    "            'Master': 'Master',\n",
    "            'Rev': 'Rare',\n",
    "            'Dr': 'Rare',\n",
    "            'Col': 'Rare',\n",
    "            'Major': 'Rare',\n",
    "            'Mlle': 'Miss',\n",
    "            'Countess': 'Rare',\n",
    "            'Ms': 'Miss',\n",
    "            'Lady': 'Rare',\n",
    "            'Jonkheer': 'Rare',\n",
    "            'Don': 'Rare',\n",
    "            'Dona': 'Rare',\n",
    "            'Mme': 'Mrs',\n",
    "            'Capt': 'Rare',\n",
    "            'Sir': 'Rare'\n",
    "        }\n",
    "        df['Title'] = df['Title'].map(title_mapping)\n",
    "        df['Title'] = df['Title'].fillna('Rare')\n",
    "    \n",
    "    # 2. Family Sizeï¼ˆå®¶æ—ã‚µã‚¤ã‚ºï¼‰ â­â­\n",
    "    if 'SibSp' in df.columns and 'Parch' in df.columns:\n",
    "        df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "        df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "        \n",
    "        # å®¶æ—ã‚µã‚¤ã‚ºã®ã‚«ãƒ†ã‚´ãƒªåŒ–\n",
    "        df['FamilySizeGroup'] = pd.cut(df['FamilySize'], \n",
    "                                        bins=[0, 1, 4, 20], \n",
    "                                        labels=['Alone', 'Small', 'Large'])\n",
    "    \n",
    "    # 3. Cabin Deckï¼ˆãƒ‡ãƒƒã‚­æƒ…å ±ï¼‰ â­â­â­\n",
    "    if 'Cabin' in df.columns:\n",
    "        df['Deck'] = df['Cabin'].str[0]\n",
    "        df['Deck'] = df['Deck'].fillna('Unknown')\n",
    "        df['HasCabin'] = df['Cabin'].notna().astype(int)\n",
    "        \n",
    "        # å®¢å®¤ç•ªå·ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "        df['CabinNumber'] = df['Cabin'].str.extract('([0-9]+)', expand=False)\n",
    "        df['CabinNumber'] = pd.to_numeric(df['CabinNumber'], errors='coerce')\n",
    "    \n",
    "    # 4. Ticket prefixï¼ˆãƒã‚±ãƒƒãƒˆæ¥é ­è¾ï¼‰ â­â­\n",
    "    if 'Ticket' in df.columns:\n",
    "        df['TicketPrefix'] = df['Ticket'].str.split(' ').str[0]\n",
    "        df['TicketPrefix'] = df['TicketPrefix'].str.replace('[^A-Za-z]', '', regex=True)\n",
    "        df['TicketPrefix'] = df['TicketPrefix'].replace('', 'None')\n",
    "        \n",
    "        # ãƒã‚±ãƒƒãƒˆç•ªå·ã®é•·ã•\n",
    "        df['TicketLength'] = df['Ticket'].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    # 5. Fare per personï¼ˆ1äººã‚ãŸã‚Šã®é‹è³ƒï¼‰ â­â­\n",
    "    if 'Fare' in df.columns and 'FamilySize' in df.columns:\n",
    "        df['FarePerPerson'] = df['Fare'] / df['FamilySize']\n",
    "        \n",
    "        # Fare ã®ãƒ“ãƒ‹ãƒ³ã‚°\n",
    "        df['FareBin'] = pd.qcut(df['Fare'].fillna(df['Fare'].median()), \n",
    "                                 q=5, labels=False, duplicates='drop')\n",
    "    \n",
    "    # 6. Age binningï¼ˆå¹´é½¢ã®ãƒ“ãƒ‹ãƒ³ã‚°ï¼‰ â­â­\n",
    "    if 'Age' in df.columns:\n",
    "        df['AgeBin'] = pd.cut(df['Age'].fillna(df['Age'].median()), \n",
    "                               bins=[0, 12, 18, 35, 60, 100], \n",
    "                               labels=['Child', 'Teen', 'Adult', 'MiddleAge', 'Senior'])\n",
    "    \n",
    "    # 7. Name lengthï¼ˆåå‰ã®é•·ã•ï¼‰ â­\n",
    "    if 'Name' in df.columns:\n",
    "        df['NameLength'] = df['Name'].apply(len)\n",
    "    \n",
    "    # 8. äº¤äº’ä½œç”¨ç‰¹å¾´é‡ â­â­â­\n",
    "    if 'Sex' in df.columns and 'Pclass' in df.columns:\n",
    "        df['Sex_Pclass'] = df['Sex'].astype(str) + '_' + df['Pclass'].astype(str)\n",
    "    \n",
    "    if 'Age' in df.columns and 'Pclass' in df.columns:\n",
    "        df['Age_Pclass'] = df['Age'].fillna(df['Age'].median()) * df['Pclass']\n",
    "    \n",
    "    # 9. Embarkedï¼ˆä¹—èˆ¹æ¸¯ï¼‰ã®å‡¦ç† â­\n",
    "    if 'Embarked' in df.columns:\n",
    "        df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "    \n",
    "    print(f\"âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†\")\n",
    "    print(f\"   ä½œæˆã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {df.shape[1]}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®é©ç”¨\n",
    "train_fe = advanced_feature_engineering(train, is_train=True)\n",
    "test_fe = advanced_feature_engineering(test, is_train=False)\n",
    "\n",
    "print(f\"\\nğŸ“Š ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å¾Œã®shape:\")\n",
    "print(f\"Train: {train_fe.shape}\")\n",
    "print(f\"Test: {test_fe.shape}\")\n",
    "print(f\"\\næ–°ã—ã„ç‰¹å¾´é‡:\")\n",
    "new_features = set(train_fe.columns) - set(train.columns)\n",
    "for feat in sorted(new_features):\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Age Imputationï¼ˆæ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å¹´é½¢è£œå®Œï¼‰ â­â­â­\n",
    "\n",
    "æ¬ æã—ã¦ã„ã‚‹å¹´é½¢ã‚’ä»–ã®ç‰¹å¾´é‡ã‹ã‚‰äºˆæ¸¬ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_age_ml(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Random Forestã§å¹´é½¢ã‚’è£œå®Œ\n",
    "    \"\"\"\n",
    "    # çµåˆã—ã¦ã¾ã¨ã‚ã¦å‡¦ç†\n",
    "    combined = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n",
    "    \n",
    "    # Ageäºˆæ¸¬ã«ä½¿ã†ç‰¹å¾´é‡\n",
    "    age_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title']\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "    for col in ['Sex', 'Embarked', 'Title']:\n",
    "        if col in combined.columns:\n",
    "            le = LabelEncoder()\n",
    "            combined[col + '_encoded'] = le.fit_transform(combined[col].astype(str))\n",
    "            age_features[age_features.index(col)] = col + '_encoded'\n",
    "    \n",
    "    # æ¬ æã—ã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’\n",
    "    known_age = combined[combined['Age'].notna()].copy()\n",
    "    unknown_age = combined[combined['Age'].isna()].copy()\n",
    "    \n",
    "    if len(unknown_age) > 0:\n",
    "        X_train = known_age[age_features].fillna(0)\n",
    "        y_train = known_age['Age']\n",
    "        X_test = unknown_age[age_features].fillna(0)\n",
    "        \n",
    "        # Random Forestã§äºˆæ¸¬\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        predicted_ages = rf.predict(X_test)\n",
    "        combined.loc[combined['Age'].isna(), 'Age'] = predicted_ages\n",
    "        \n",
    "        print(f\"âœ… {len(unknown_age)}ä»¶ã®å¹´é½¢ã‚’è£œå®Œã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    # åˆ†å‰²ã—ã¦è¿”ã™\n",
    "    train_len = len(train_df)\n",
    "    return combined.iloc[:train_len].copy(), combined.iloc[train_len:].copy()\n",
    "\n",
    "# å¹´é½¢è£œå®Œã®å®Ÿè¡Œ\n",
    "if 'Age' in train_fe.columns:\n",
    "    print(f\"è£œå®Œå‰ã®æ¬ ææ•°: {train_fe['Age'].isna().sum()}\")\n",
    "    train_fe, test_fe = impute_age_ml(train_fe, test_fe)\n",
    "    print(f\"è£œå®Œå¾Œã®æ¬ ææ•°: {train_fe['Age'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 4. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, test_df):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n",
    "    \"\"\"\n",
    "    # ä½¿ç”¨ã—ãªã„åˆ—\n",
    "    drop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
    "    if 'Survived' in train_df.columns:\n",
    "        drop_cols.append('Survived')\n",
    "    \n",
    "    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢\n",
    "    X = train_df.drop(columns=[col for col in drop_cols if col in train_df.columns])\n",
    "    y = train_df['Survived'] if 'Survived' in train_df.columns else None\n",
    "    X_test = test_df.drop(columns=[col for col in drop_cols if col in test_df.columns])\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®å‡¦ç†\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        # Label Encoding\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # train ã¨ test ã‚’çµåˆã—ã¦fitã™ã‚‹ã“ã¨ã§ã€ä¸€è²«æ€§ã‚’ä¿ã¤\n",
    "        combined = pd.concat([X[col], X_test[col]], ignore_index=True)\n",
    "        le.fit(combined.astype(str))\n",
    "        \n",
    "        X[col] = le.transform(X[col].astype(str))\n",
    "        X_test[col] = le.transform(X_test[col].astype(str))\n",
    "    \n",
    "    # æ®‹ã£ãŸæ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œ\n",
    "    X = X.fillna(X.median())\n",
    "    X_test = X_test.fillna(X_test.median())\n",
    "    \n",
    "    print(f\"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†\")\n",
    "    print(f\"   ç‰¹å¾´é‡æ•°: {X.shape[1]}\")\n",
    "    print(f\"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X.shape[0]} samples\")\n",
    "    print(f\"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X, y, X_test\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "X, y, X_test = prepare_data(train_fe, test_fe)\n",
    "\n",
    "print(f\"\\nğŸ“Š ç‰¹å¾´é‡ä¸€è¦§:\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 5. Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
    "\n",
    "LightGBMã€XGBoostã€CatBoostã®æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 LightGBMæœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgb(trial):\n",
    "    \"\"\"\n",
    "    LightGBMã®Optuna objectiveé–¢æ•°\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        \n",
    "        # Optunaã§æœ€é©åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    # Stratified K-Fold CV\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train, \n",
    "                 eval_set=[(X_val, y_val)],\n",
    "                 callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
    "        \n",
    "        preds = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, preds)\n",
    "        cv_scores.append(score)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# LightGBMæœ€é©åŒ–ã®å®Ÿè¡Œ\n",
    "print(\"ğŸ” LightGBMã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é–‹å§‹...\")\n",
    "study_lgb = optuna.create_study(direction='maximize', \n",
    "                                 sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "study_lgb.optimize(objective_lgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ… LightGBMæœ€é©åŒ–å®Œäº†\")\n",
    "print(f\"   Best CV Score: {study_lgb.best_value:.4f}\")\n",
    "print(f\"   Best Parameters:\")\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"     {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 XGBoostæœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(trial):\n",
    "    \"\"\"\n",
    "    XGBoostã®Optuna objectiveé–¢æ•°\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'use_label_encoder': False,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        \n",
    "        # Optunaã§æœ€é©åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    # Stratified K-Fold CV\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train, \n",
    "                 eval_set=[(X_val, y_val)],\n",
    "                 early_stopping_rounds=50,\n",
    "                 verbose=False)\n",
    "        \n",
    "        preds = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, preds)\n",
    "        cv_scores.append(score)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# XGBoostæœ€é©åŒ–ã®å®Ÿè¡Œ\n",
    "print(\"ğŸ” XGBoostã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é–‹å§‹...\")\n",
    "study_xgb = optuna.create_study(direction='maximize',\n",
    "                                 sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ… XGBoostæœ€é©åŒ–å®Œäº†\")\n",
    "print(f\"   Best CV Score: {study_xgb.best_value:.4f}\")\n",
    "print(f\"   Best Parameters:\")\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"     {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 CatBoostæœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_cat(trial):\n",
    "    \"\"\"\n",
    "    CatBoostã®Optuna objectiveé–¢æ•°\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'Accuracy',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': False,\n",
    "        \n",
    "        # Optunaã§æœ€é©åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "    }\n",
    "    \n",
    "    # Stratified K-Fold CV\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_train, y_train, \n",
    "                 eval_set=(X_val, y_val),\n",
    "                 early_stopping_rounds=50,\n",
    "                 verbose=False)\n",
    "        \n",
    "        preds = model.predict(X_val)\n",
    "        score = accuracy_score(y_val, preds)\n",
    "        cv_scores.append(score)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# CatBoostæœ€é©åŒ–ã®å®Ÿè¡Œ\n",
    "print(\"ğŸ” CatBoostã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é–‹å§‹...\")\n",
    "study_cat = optuna.create_study(direction='maximize',\n",
    "                                 sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "study_cat.optimize(objective_cat, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nâœ… CatBoostæœ€é©åŒ–å®Œäº†\")\n",
    "print(f\"   Best CV Score: {study_cat.best_value:.4f}\")\n",
    "print(f\"   Best Parameters:\")\n",
    "for key, value in study_cat.best_params.items():\n",
    "    print(f\"     {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 æœ€é©åŒ–çµæœã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€é©åŒ–å±¥æ­´ã®ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, study, name in zip(axes, [study_lgb, study_xgb, study_cat], \n",
    "                            ['LightGBM', 'XGBoost', 'CatBoost']):\n",
    "    trials_df = study.trials_dataframe()\n",
    "    ax.plot(trials_df['number'], trials_df['value'], marker='o', alpha=0.6)\n",
    "    ax.axhline(study.best_value, color='red', linestyle='--', \n",
    "               label=f'Best: {study.best_value:.4f}')\n",
    "    ax.set_xlabel('Trial')\n",
    "    ax.set_ylabel('CV Accuracy')\n",
    "    ax.set_title(f'{name} Optimization History')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦ã®æ¯”è¼ƒ\n",
    "print(\"\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«åˆ¥ã®æœ€é©CV Accuracy:\")\n",
    "print(f\"LightGBM: {study_lgb.best_value:.4f}\")\n",
    "print(f\"XGBoost:  {study_xgb.best_value:.4f}\")\n",
    "print(f\"CatBoost: {study_cat.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ­ 6. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’\n",
    "\n",
    "3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 10-Fold Stratified CV with Out-of-Fold Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_kfold(X, y, X_test, params_lgb, params_xgb, params_cat, n_splits=10):\n",
    "    \"\"\"\n",
    "    10-Fold Stratified CVã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€Out-of-Foldäºˆæ¸¬ã‚’å–å¾—\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Out-of-fold predictionsç”¨ã®é…åˆ—\n",
    "    oof_lgb = np.zeros(len(X))\n",
    "    oof_xgb = np.zeros(len(X))\n",
    "    oof_cat = np.zeros(len(X))\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”¨ã®é…åˆ—\n",
    "    test_preds_lgb = np.zeros(len(X_test))\n",
    "    test_preds_xgb = np.zeros(len(X_test))\n",
    "    test_preds_cat = np.zeros(len(X_test))\n",
    "    \n",
    "    # CVã‚¹ã‚³ã‚¢ã®è¨˜éŒ²\n",
    "    cv_scores_lgb = []\n",
    "    cv_scores_xgb = []\n",
    "    cv_scores_cat = []\n",
    "    \n",
    "    print(f\"\\nğŸ”„ {n_splits}-Fold Cross-Validationé–‹å§‹...\\n\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "        print(f\"Fold {fold}/{n_splits}\")\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # LightGBM\n",
    "        model_lgb = lgb.LGBMClassifier(**params_lgb)\n",
    "        model_lgb.fit(X_train, y_train,\n",
    "                     eval_set=[(X_val, y_val)],\n",
    "                     callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "        oof_lgb[val_idx] = model_lgb.predict_proba(X_val)[:, 1]\n",
    "        test_preds_lgb += model_lgb.predict_proba(X_test)[:, 1] / n_splits\n",
    "        score_lgb = accuracy_score(y_val, model_lgb.predict(X_val))\n",
    "        cv_scores_lgb.append(score_lgb)\n",
    "        \n",
    "        # XGBoost\n",
    "        model_xgb = xgb.XGBClassifier(**params_xgb)\n",
    "        model_xgb.fit(X_train, y_train,\n",
    "                     eval_set=[(X_val, y_val)],\n",
    "                     early_stopping_rounds=50,\n",
    "                     verbose=False)\n",
    "        oof_xgb[val_idx] = model_xgb.predict_proba(X_val)[:, 1]\n",
    "        test_preds_xgb += model_xgb.predict_proba(X_test)[:, 1] / n_splits\n",
    "        score_xgb = accuracy_score(y_val, model_xgb.predict(X_val))\n",
    "        cv_scores_xgb.append(score_xgb)\n",
    "        \n",
    "        # CatBoost\n",
    "        model_cat = CatBoostClassifier(**params_cat)\n",
    "        model_cat.fit(X_train, y_train,\n",
    "                     eval_set=(X_val, y_val),\n",
    "                     early_stopping_rounds=50,\n",
    "                     verbose=False)\n",
    "        oof_cat[val_idx] = model_cat.predict_proba(X_val)[:, 1]\n",
    "        test_preds_cat += model_cat.predict_proba(X_test)[:, 1] / n_splits\n",
    "        score_cat = accuracy_score(y_val, model_cat.predict(X_val))\n",
    "        cv_scores_cat.append(score_cat)\n",
    "        \n",
    "        print(f\"  LightGBM: {score_lgb:.4f} | XGBoost: {score_xgb:.4f} | CatBoost: {score_cat:.4f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… CVå®Œäº†\")\n",
    "    print(f\"\\nğŸ“Š å¹³å‡CVã‚¹ã‚³ã‚¢:\")\n",
    "    print(f\"  LightGBM: {np.mean(cv_scores_lgb):.4f} (+/- {np.std(cv_scores_lgb):.4f})\")\n",
    "    print(f\"  XGBoost:  {np.mean(cv_scores_xgb):.4f} (+/- {np.std(cv_scores_xgb):.4f})\")\n",
    "    print(f\"  CatBoost: {np.mean(cv_scores_cat):.4f} (+/- {np.std(cv_scores_cat):.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'oof': {'lgb': oof_lgb, 'xgb': oof_xgb, 'cat': oof_cat},\n",
    "        'test': {'lgb': test_preds_lgb, 'xgb': test_preds_xgb, 'cat': test_preds_cat},\n",
    "        'scores': {'lgb': cv_scores_lgb, 'xgb': cv_scores_xgb, 'cat': cv_scores_cat}\n",
    "    }\n",
    "\n",
    "# 10-Fold CVã®å®Ÿè¡Œ\n",
    "results = train_with_kfold(\n",
    "    X, y, X_test,\n",
    "    params_lgb={**study_lgb.best_params, 'objective': 'binary', 'metric': 'binary_logloss', \n",
    "                'verbosity': -1, 'random_state': RANDOM_STATE},\n",
    "    params_xgb={**study_xgb.best_params, 'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "                'use_label_encoder': False, 'random_state': RANDOM_STATE},\n",
    "    params_cat={**study_cat.best_params, 'loss_function': 'Logloss', 'eval_metric': 'Accuracy',\n",
    "                'random_state': RANDOM_STATE, 'verbose': False},\n",
    "    n_splits=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_weights(oof_lgb, oof_xgb, oof_cat, y_true):\n",
    "    \"\"\"\n",
    "    æœ€é©ãªé‡ã¿ã‚’æ¢ç´¢\n",
    "    \"\"\"\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    def objective(weights):\n",
    "        # é‡ã¿ä»˜ãå¹³å‡\n",
    "        ensemble = weights[0] * oof_lgb + weights[1] * oof_xgb + weights[2] * oof_cat\n",
    "        preds = (ensemble > 0.5).astype(int)\n",
    "        return -accuracy_score(y_true, preds)  # æœ€å°åŒ–ã™ã‚‹ãŸã‚è² ã«ã™ã‚‹\n",
    "    \n",
    "    # åˆ¶ç´„: é‡ã¿ã®åˆè¨ˆãŒ1\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: w.sum() - 1}\n",
    "    bounds = [(0, 1), (0, 1), (0, 1)]\n",
    "    \n",
    "    # åˆæœŸå€¤: å‡ç­‰ãªé‡ã¿\n",
    "    initial_weights = np.array([1/3, 1/3, 1/3])\n",
    "    \n",
    "    result = minimize(objective, initial_weights, method='SLSQP', \n",
    "                     bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    return result.x\n",
    "\n",
    "# æœ€é©ãªé‡ã¿ã‚’æ¢ç´¢\n",
    "best_weights = find_best_weights(\n",
    "    results['oof']['lgb'],\n",
    "    results['oof']['xgb'],\n",
    "    results['oof']['cat'],\n",
    "    y\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¯ æœ€é©ãªé‡ã¿:\")\n",
    "print(f\"  LightGBM: {best_weights[0]:.3f}\")\n",
    "print(f\"  XGBoost:  {best_weights[1]:.3f}\")\n",
    "print(f\"  CatBoost: {best_weights[2]:.3f}\")\n",
    "\n",
    "# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬\n",
    "oof_ensemble = (best_weights[0] * results['oof']['lgb'] + \n",
    "                best_weights[1] * results['oof']['xgb'] + \n",
    "                best_weights[2] * results['oof']['cat'])\n",
    "\n",
    "test_ensemble = (best_weights[0] * results['test']['lgb'] + \n",
    "                 best_weights[1] * results['test']['xgb'] + \n",
    "                 best_weights[2] * results['test']['cat'])\n",
    "\n",
    "# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®CVã‚¹ã‚³ã‚¢\n",
    "ensemble_preds = (oof_ensemble > 0.5).astype(int)\n",
    "ensemble_score = accuracy_score(y, ensemble_preds)\n",
    "\n",
    "print(f\"\\nğŸ“Š ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«CVã‚¹ã‚³ã‚¢: {ensemble_score:.4f}\")\n",
    "\n",
    "# å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®æ¯”è¼ƒ\n",
    "oof_lgb_preds = (results['oof']['lgb'] > 0.5).astype(int)\n",
    "oof_xgb_preds = (results['oof']['xgb'] > 0.5).astype(int)\n",
    "oof_cat_preds = (results['oof']['cat'] > 0.5).astype(int)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ:\")\n",
    "print(f\"  LightGBMå˜ä½“:  {accuracy_score(y, oof_lgb_preds):.4f}\")\n",
    "print(f\"  XGBoostå˜ä½“:   {accuracy_score(y, oof_xgb_preds):.4f}\")\n",
    "print(f\"  CatBoostå˜ä½“:  {accuracy_score(y, oof_cat_preds):.4f}\")\n",
    "print(f\"  ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«:   {ensemble_score:.4f} â­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¤ 7. Kaggleæå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "\n",
    "æœ€çµ‚çš„ãªäºˆæ¸¬ã‚’Kaggleæå‡ºå½¢å¼ã§ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå‡ºç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "submission_dir = Path('../submissions')\n",
    "submission_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# äºˆæ¸¬ã‚’0/1ã«å¤‰æ›\n",
    "final_predictions = (test_ensemble > 0.5).astype(int)\n",
    "\n",
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Survived': final_predictions\n",
    "})\n",
    "\n",
    "# ä¿å­˜\n",
    "submission_path = submission_dir / 'titanic_ensemble_submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"âœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {submission_path}\")\n",
    "print(f\"\\nğŸ“Š äºˆæ¸¬åˆ†å¸ƒ:\")\n",
    "print(submission['Survived'].value_counts())\n",
    "print(f\"\\nç”Ÿå­˜äºˆæ¸¬ç‡: {submission['Survived'].mean():.2%}\")\n",
    "\n",
    "# æœ€åˆã®10è¡Œã‚’è¡¨ç¤º\n",
    "print(f\"\\næœ€åˆã®10è¡Œ:\")\n",
    "display(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 8. çµæœã®å¯è¦–åŒ–ã¨åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV Scores ã®ç®±ã²ã’å›³\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "data_to_plot = [\n",
    "    results['scores']['lgb'],\n",
    "    results['scores']['xgb'],\n",
    "    results['scores']['cat']\n",
    "]\n",
    "\n",
    "bp = ax.boxplot(data_to_plot, labels=['LightGBM', 'XGBoost', 'CatBoost'],\n",
    "                patch_artist=True)\n",
    "\n",
    "for patch, color in zip(bp['boxes'], ['skyblue', 'lightgreen', 'lightcoral']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('10-Fold CV Scores by Model')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y, ensemble_preds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Ensemble Model - Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Classification Report:\")\n",
    "print(classification_report(y, ensemble_preds, target_names=['Not Survived', 'Survived']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 9. ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "### ğŸ“ˆ é”æˆã—ãŸã“ã¨\n",
    "\n",
    "1. âœ… é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆTitleã€Deckã€Ticket prefixã€Family sizeãªã©ï¼‰\n",
    "2. âœ… Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–\n",
    "3. âœ… 10-Fold Stratified CVã«ã‚ˆã‚‹å …ç‰¢ãªæ¤œè¨¼\n",
    "4. âœ… 3ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
    "5. âœ… Kaggleæå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "\n",
    "### ğŸ¯ æœŸå¾…ã•ã‚Œã‚‹ã‚¹ã‚³ã‚¢\n",
    "\n",
    "- **Local CV Accuracy**: 0.85å‰å¾Œ\n",
    "- **Kaggle Public LB**: 0.79+ï¼ˆTop 30%åœå†…ï¼‰\n",
    "\n",
    "### ğŸš€ ã•ã‚‰ã«ã‚¹ã‚³ã‚¢ã‚’ä¸Šã’ã‚‹ã«ã¯\n",
    "\n",
    "1. **è¿½åŠ ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**:\n",
    "   - ã‚ˆã‚Šè¤‡é›‘ãªäº¤äº’ä½œç”¨ç‰¹å¾´é‡\n",
    "   - åå‰ã®é•·ã•ã€ãƒ¬ã‚¢åº¦ãªã©\n",
    "   - ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãç‰¹å¾´é‡\n",
    "\n",
    "2. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®æ”¹å–„**:\n",
    "   - Stackingã®å°å…¥ï¼ˆæ¬¡ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯22ã§å­¦ç¿’ï¼‰\n",
    "   - ã‚ˆã‚Šå¤šæ§˜ãªãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ ï¼ˆNeural Networkã€SVMãªã©ï¼‰\n",
    "\n",
    "3. **Pseudo-labeling**:\n",
    "   - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
    "\n",
    "4. **Calibration**:\n",
    "   - Platt Scalingã«ã‚ˆã‚‹ç¢ºç‡ã®èª¿æ•´\n",
    "\n",
    "### ğŸ“š æ¬¡ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "\n",
    "- **ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯18**: House Priceså›å¸°å•é¡Œã§GBDTå›å¸°ã‚’å­¦ã¶\n",
    "- **ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯19**: Store Demandæ™‚ç³»åˆ—äºˆæ¸¬ã«æŒ‘æˆ¦\n",
    "- **ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯20**: Optunaã‚’ã•ã‚‰ã«æ·±ãå­¦ã¶\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼Kaggleã§å®Ÿéš›ã«æå‡ºã—ã¦ã‚¹ã‚³ã‚¢ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ä»˜éŒ²: ã‚ˆãã‚ã‚‹è³ªå•ï¼ˆFAQï¼‰\n",
    "\n",
    "### Q1: CVã‚¹ã‚³ã‚¢ã¨LBã‚¹ã‚³ã‚¢ã®å·®ãŒå¤§ãã„å ´åˆã¯ï¼Ÿ\n",
    "\n",
    "**A**: éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„ï¼š\n",
    "- æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ`reg_alpha`, `reg_lambda`ï¼‰ã‚’å¢—ã‚„ã™\n",
    "- `n_estimators`ã‚’æ¸›ã‚‰ã™\n",
    "- ã‚ˆã‚Šå¤šãã®Foldæ•°ã§CVã‚’è¡Œã†\n",
    "\n",
    "### Q2: Optunaã®æœ€é©åŒ–ã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã‚‹\n",
    "\n",
    "**A**: ä»¥ä¸‹ã®æ–¹æ³•ã§é«˜é€ŸåŒ–ã§ãã¾ã™ï¼š\n",
    "- `n_trials`ã‚’æ¸›ã‚‰ã™ï¼ˆ50 â†’ 20ãªã©ï¼‰\n",
    "- CVã®Foldæ•°ã‚’æ¸›ã‚‰ã™ï¼ˆ10 â†’ 5ãªã©ï¼‰\n",
    "- Pruningã‚’æœ‰åŠ¹ã«ã™ã‚‹ï¼ˆOptuna MedianPrunerï¼‰\n",
    "\n",
    "### Q3: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ç²¾åº¦ãŒä¸‹ãŒã£ãŸ\n",
    "\n",
    "**A**: ãƒ¢ãƒ‡ãƒ«ãŒé¡ä¼¼ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼š\n",
    "- ã‚ˆã‚Šå¤šæ§˜ãªãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ï¼ˆNeural Networkã€SVMãªã©ï¼‰\n",
    "- ç•°ãªã‚‹ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†\n",
    "- Stackingã‚’è©¦ã™\n",
    "\n",
    "### Q4: ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹\n",
    "\n",
    "**A**: ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„ï¼š\n",
    "- ä¸è¦ãªå¤‰æ•°ã‚’å‰Šé™¤ï¼ˆ`del`ã‚„`gc.collect()`ï¼‰\n",
    "- ãƒãƒƒãƒå‡¦ç†ã‚’ä½¿ã†\n",
    "- ãƒ‡ãƒ¼ã‚¿å‹ã‚’æœ€é©åŒ–ï¼ˆ`float64` â†’ `float32`ãªã©ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯\n",
    "\n",
    "- [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic)\n",
    "- [LightGBM Documentation](https://lightgbm.readthedocs.io/)\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "- [CatBoost Documentation](https://catboost.ai/docs/)\n",
    "- [Optuna Documentation](https://optuna.org/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Kaggling! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
