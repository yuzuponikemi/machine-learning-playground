# Machine Learning Learning Plan with Jupyter Notebooks

A hands-on curriculum for learning ML through simulations, parameter exploration, and MLP training using scikit-learn.

---

## Overview

This plan focuses on **learning by doing** - each notebook contains:
- Synthetic data generation with controllable parameters
- Visual waveform/outcome analysis
- Interactive parameter exploration
- Model training and evaluation
- Hyperparameter tuning across parameter space

---

## Phase 1: Foundations (Notebooks 1-3)

### Notebook 1: `01_data_simulation_basics.ipynb`
**Goal**: Understand how data parameters affect outcomes

**Contents**:
- Generate synthetic waveforms (sine, noise, combinations)
- Visualize how parameters (amplitude, frequency, noise) affect signals
- Create regression datasets with `make_regression()`
- Create classification datasets with `make_classification()`, `make_moons()`, `make_circles()`
- Interactive sliders for parameter exploration (ipywidgets)

**Key Skills**:
- numpy for data generation
- matplotlib/seaborn for visualization
- Understanding feature distributions

---

### Notebook 2: `02_preprocessing_and_feature_engineering.ipynb`
**Goal**: Prepare data for ML models

**Contents**:
- Scaling: StandardScaler, MinMaxScaler, RobustScaler
- Encoding: OneHotEncoder, LabelEncoder
- Feature selection techniques
- Train/test/validation splits
- Visualize how preprocessing affects data distribution

**Key Skills**:
- sklearn.preprocessing
- sklearn.model_selection
- Pipeline construction

---

### Notebook 3: `03_model_evaluation_metrics.ipynb`
**Goal**: Understand how to measure model performance

**Contents**:
- Regression metrics: MSE, RMSE, MAE, RÂ²
- Classification metrics: Accuracy, Precision, Recall, F1, ROC-AUC
- Confusion matrices visualization
- Cross-validation strategies
- Learning curves and validation curves

**Key Skills**:
- sklearn.metrics
- sklearn.model_selection.cross_val_score
- Interpreting model performance

---

## Phase 2: Classical ML Models (Notebooks 4-6)

### Notebook 4: `04_linear_models_simulation.ipynb`
**Goal**: Deep dive into linear models with parameter simulation

**Contents**:
- Linear Regression on synthetic waveforms
- Simulate: noise levels, feature correlations, outliers
- Ridge, Lasso, ElasticNet regularization
- Visualize decision boundaries
- Parameter sweep: alpha values, regularization strength

**Experiments**:
```python
# Example: Sweep regularization parameters
alphas = np.logspace(-4, 4, 50)
for alpha in alphas:
    model = Ridge(alpha=alpha)
    # Train, evaluate, store results
# Plot: alpha vs MSE, alpha vs coefficient magnitude
```

---

### Notebook 5: `05_tree_and_ensemble_models.ipynb`
**Goal**: Understand tree-based models and ensembles

**Contents**:
- Decision Trees: max_depth, min_samples_split effects
- Random Forest: n_estimators, max_features exploration
- Gradient Boosting: learning_rate, n_estimators tuning
- Visualize tree structures
- Feature importance analysis

**Experiments**:
- Simulate overfitting with deep trees
- Compare single tree vs ensemble performance
- Parameter grid visualization (heatmaps)

---

### Notebook 6: `06_svm_and_kernels.ipynb`
**Goal**: Understand SVM and kernel methods

**Contents**:
- Linear SVM on linearly separable data
- Kernel SVM: RBF, polynomial kernels
- Simulate non-linear decision boundaries
- C and gamma parameter exploration
- Visualize support vectors and margins

---

## Phase 3: Neural Networks & MLP (Notebooks 7-9) â­

### Notebook 7: `07_mlp_fundamentals.ipynb`
**Goal**: Build intuition for MLP architecture

**Contents**:
- MLP architecture explanation with diagrams
- Forward pass visualization
- Activation functions: ReLU, tanh, sigmoid (visualized)
- sklearn MLPClassifier and MLPRegressor basics
- Simple classification on make_moons data

**Experiments**:
```python
# Visualize activation functions
x = np.linspace(-5, 5, 100)
activations = {
    'relu': np.maximum(0, x),
    'tanh': np.tanh(x),
    'sigmoid': 1 / (1 + np.exp(-x))
}
# Plot all on same axes
```

---

### Notebook 8: `08_mlp_parameter_space_exploration.ipynb` â­â­â­
**Goal**: Comprehensive MLP hyperparameter tuning

**Contents**:
- **Architecture Search**:
  - hidden_layer_sizes: (10,), (50,), (100,), (50,50), (100,50,25)
  - Visualize how depth/width affects learning

- **Learning Parameters**:
  - learning_rate_init: 0.0001 to 0.1
  - alpha (L2 regularization): 0.0001 to 1.0
  - batch_size effects

- **Optimization**:
  - solver: 'adam', 'sgd', 'lbfgs'
  - momentum for SGD
  - early_stopping effects

**Complete Simulation**:
```python
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV, learning_curve
import numpy as np

# Generate synthetic data with known patterns
X, y = make_classification(
    n_samples=2000,
    n_features=20,
    n_informative=10,
    n_redundant=5,
    n_classes=3,
    random_state=42
)

# Define parameter space
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50,50), (100,50)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001, 0.01, 0.1],
    'learning_rate_init': [0.001, 0.01],
    'solver': ['adam'],
    'max_iter': [500]
}

# Grid search with cross-validation
mlp = MLPClassifier(random_state=42, early_stopping=True)
grid_search = GridSearchCV(
    mlp, param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=2,
    return_train_score=True
)
grid_search.fit(X_train, y_train)

# Visualize results
results_df = pd.DataFrame(grid_search.cv_results_)
# Create heatmaps, line plots, etc.
```

**Visualizations**:
- Heatmap: hidden_layer_sizes vs alpha (accuracy)
- Learning curves for best models
- Loss curves during training
- Decision boundary evolution (for 2D data)

---

### Notebook 9: `09_mlp_regression_waveforms.ipynb` â­â­â­
**Goal**: MLP for waveform/signal prediction

**Contents**:
- Generate complex waveforms (sum of sines, damped oscillations)
- Train MLP to predict waveform values
- Visualize predicted vs actual waveforms
- Parameter effects on waveform reconstruction

**Complete Simulation**:
```python
from sklearn.neural_network import MLPRegressor

# Generate complex waveform
t = np.linspace(0, 4*np.pi, 1000)
y_true = (np.sin(t) +
          0.5*np.sin(3*t) +
          0.25*np.sin(5*t) +
          0.1*np.random.randn(len(t)))

# Create features (e.g., time-delayed values)
def create_features(y, n_lags=10):
    X = np.array([y[i:i+n_lags] for i in range(len(y)-n_lags)])
    y_target = y[n_lags:]
    return X, y_target

X, y = create_features(y_true, n_lags=20)

# Train MLP Regressor
mlp = MLPRegressor(
    hidden_layer_sizes=(100, 50),
    activation='relu',
    solver='adam',
    alpha=0.001,
    learning_rate_init=0.001,
    max_iter=1000,
    early_stopping=True,
    validation_fraction=0.1,
    random_state=42
)
mlp.fit(X_train, y_train)

# Plot predicted vs actual waveform
y_pred = mlp.predict(X_test)
plt.figure(figsize=(14, 5))
plt.plot(y_test, label='Actual', alpha=0.7)
plt.plot(y_pred, label='Predicted', alpha=0.7)
plt.legend()
plt.title('MLP Waveform Prediction')
```

---

## Phase 4: Advanced Topics (Notebooks 10-12)

### Notebook 10: `10_automated_hyperparameter_tuning.ipynb`
**Goal**: Systematic parameter optimization

**Contents**:
- GridSearchCV deep dive
- RandomizedSearchCV for large spaces
- Bayesian optimization (sklearn-optimize)
- Visualizing search results
- Best practices for hyperparameter tuning

---

### Notebook 11: `11_model_comparison_and_selection.ipynb`
**Goal**: Compare models systematically

**Contents**:
- Compare Linear, Tree, SVM, MLP on same data
- Statistical significance testing
- Ensemble methods (VotingClassifier, Stacking)
- When to use which model

---

### Notebook 12: `12_complete_ml_pipeline.ipynb`
**Goal**: End-to-end ML workflow

**Contents**:
- Data loading and exploration
- Preprocessing pipeline
- Model selection
- Hyperparameter optimization
- Final evaluation
- Model persistence (joblib)

---

## Recommended Learning Path

### Week 1-2: Foundations
- Complete Notebooks 1-3
- Practice: Generate 5 different synthetic datasets

### Week 3-4: Classical Models
- Complete Notebooks 4-6
- Practice: Parameter sweeps on each model type

### Week 5-6: MLP Deep Dive â­
- Complete Notebooks 7-9
- Practice: Full parameter space exploration
- Goal: Achieve >95% accuracy on synthetic classification

### Week 7-8: Advanced & Integration
- Complete Notebooks 10-12
- Practice: Build complete pipeline for a real dataset

---

## Key Libraries to Install

```bash
pip install numpy pandas matplotlib seaborn scikit-learn ipywidgets jupyter
```

For enhanced visualizations:
```bash
pip install plotly yellowbrick
```

---

## Quick Start: Your First Experiment

Create `00_quick_start.ipynb` with this code to get started immediately:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler

# 1. Generate Data
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)

# 2. Preprocess
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# 3. Define Parameter Space
param_grid = {
    'hidden_layer_sizes': [(10,), (50,), (20, 10)],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate_init': [0.001, 0.01]
}

# 4. Grid Search
mlp = MLPClassifier(max_iter=500, random_state=42)
grid_search = GridSearchCV(mlp, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

# 5. Results
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best CV Score: {grid_search.best_score_:.4f}")
print(f"Test Score: {grid_search.score(X_test, y_test):.4f}")

# 6. Visualize Decision Boundary
def plot_decision_boundary(model, X, y):
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')
    plt.title('MLP Decision Boundary')

plt.figure(figsize=(10, 6))
plot_decision_boundary(grid_search.best_estimator_, X_scaled, y)
plt.show()
```

---

## Success Metrics

By the end of this curriculum, you should be able to:

1. **Generate** synthetic data with controlled parameters
2. **Visualize** how data parameters affect model outcomes
3. **Train** MLP models with various architectures
4. **Explore** parameter spaces systematically
5. **Evaluate** models using appropriate metrics
6. **Select** optimal hyperparameters using grid/random search
7. **Build** complete ML pipelines

---

---

## Phase 5: GBDT Master Course (Notebooks 13-16) ğŸŒ³

### Notebook 13: `13_gbdt_introduction.ipynb`
**Goal**: Master LightGBM and XGBoost fundamentals

**Contents**:
- GBDT basic concepts (boosting, trees, gradients)
- LightGBM architecture and parameters
- XGBoost implementation
- Key hyperparameters: learning_rate, num_leaves, max_depth
- Overfitting detection and early stopping

**Key Skills**:
- lightgbm, xgboost libraries
- Understanding tree-based boosting
- Feature importance visualization

---

### Notebook 14: `14_catboost_categorical.ipynb`
**Goal**: Master categorical feature handling with CatBoost

**Contents**:
- CatBoost's categorical feature processing
- Comparison: LightGBM vs XGBoost vs CatBoost
- Ordered boosting mechanism
- GPU acceleration
- Handling high-cardinality categoricals

**Key Skills**:
- catboost library
- Categorical encoding strategies
- Performance optimization

---

### Notebook 15: `15_titanic_eda_feature_engineering.ipynb`
**Goal**: Professional EDA and feature engineering

**Contents**:
- Comprehensive Titanic dataset EDA
- Missing value imputation strategies
- Feature creation: Title extraction, Family size, Fare bins
- Feature interaction engineering
- Correlation analysis and feature selection

**Key Skills**:
- pandas profiling
- Advanced feature engineering
- Domain knowledge application

---

### Notebook 16: `16_titanic_gbdt_modeling.ipynb`
**Goal**: GBDT modeling with cross-validation and ensemble

**Contents**:
- Stratified K-Fold cross-validation
- LightGBM, XGBoost, CatBoost training
- Hyperparameter tuning basics
- Model ensemble (averaging, weighted voting)
- Kaggle submission preparation

**Key Skills**:
- Cross-validation strategies
- Ensemble methods
- Kaggle workflow

---

## Phase 6: Kaggle Competition Practice (Notebooks 17-19) ğŸ†

### Notebook 17: `17_titanic_top30_submission.ipynb` â­â­â­
**Goal**: Achieve Kaggle Titanic Top 30% (Target: 0.79+ accuracy)

**Contents**:
- **Advanced Feature Engineering**:
  - Ticket prefix extraction (class indicator)
  - Cabin deck analysis
  - Name length and rare names
  - Age imputation with ML models
  - Feature polynomial combinations

- **Advanced Model Tuning**:
  - Optuna for hyperparameter optimization (preview)
  - 10-fold Stratified CV for robust validation
  - Out-of-fold predictions
  - Pseudo-labeling from test set

- **Ensemble Strategy**:
  - 3-model weighted ensemble (LightGBM + XGBoost + CatBoost)
  - Blending vs Stacking comparison
  - Calibration (Platt scaling)

- **Submission Strategy**:
  - Multiple submission versions
  - Leaderboard probing techniques
  - Score variance analysis

**Target Performance**:
- Local CV: 0.85+
- Public LB: 0.79+ (Top 30%)

---

### Notebook 18: `18_house_prices_regression.ipynb` â­â­â­
**Goal**: Master regression with GBDT on House Prices competition

**Contents**:
- **Regression-Specific Challenges**:
  - Target transformation (log, Box-Cox)
  - Outlier detection and handling (IsolationForest, Z-score)
  - Skewness correction
  - Heavy-tailed distributions

- **Advanced Feature Engineering**:
  - 200+ features â†’ feature selection
  - Polynomial features for numerical variables
  - Neighborhood encoding (target encoding, frequency)
  - Time-based features (YearBuilt, YearRemodeled)

- **GBDT for Regression**:
  - Objective functions: rmse, mae, huber
  - Quantile regression for uncertainty
  - Feature interactions in trees

- **Model Evaluation**:
  - RMSLE (Root Mean Squared Log Error)
  - Residual analysis
  - Prediction interval estimation

**Target Performance**:
- Local CV RMSLE: 0.12-0.13
- Public LB: Top 20%

**Datasets**: Kaggle House Prices Competition

---

### Notebook 19: `19_store_demand_timeseries.ipynb` â­â­â­
**Goal**: Time series forecasting with GBDT

**Contents**:
- **Time Series Fundamentals**:
  - Trend, seasonality, residuals decomposition
  - Autocorrelation (ACF/PACF)
  - Train/validation temporal split (NO shuffling!)

- **Time Series Features for GBDT**:
  - Lag features (1, 7, 14, 28, 365 days)
  - Rolling window statistics (mean, std, min, max)
  - Expanding window features
  - Day of week, month, year, holidays
  - Fourier features for seasonality

- **GBDT Time Series Modeling**:
  - Walk-forward validation
  - Multi-step forecasting
  - Handling zero-inflated data
  - Store-item hierarchy modeling

- **Advanced Techniques**:
  - Exogenous variables (promotions, events)
  - LGBM with dart booster for time series
  - Residual modeling

**Target Performance**:
- SMAPE < 15%
- Kaggle Store Item Demand: Top 25%

**Datasets**: Kaggle Store Item Demand Forecasting Challenge

---

## Phase 7: Advanced GBDT Techniques (Notebooks 20-22) ğŸš€

### Notebook 20: `20_optuna_hyperparameter_optimization.ipynb` â­â­â­
**Goal**: Master automated hyperparameter tuning with Optuna

**Contents**:
- **Optuna Fundamentals**:
  - Tree-structured Parzen Estimator (TPE)
  - Bayesian optimization vs Grid/Random search
  - Study object and trial management
  - Pruning unpromising trials

- **LightGBM + Optuna**:
  - Complete parameter space definition
  - Categorical parameters (boosting_type, objective)
  - Integer parameters (num_leaves, max_depth)
  - Float parameters (learning_rate, feature_fraction)
  - Conditional parameters

- **Advanced Optimization**:
  - Multi-objective optimization (accuracy + inference time)
  - Parallel optimization (multiple workers)
  - Visualization (optimization history, parameter importance)
  - Saving/loading studies

- **Integration with CV**:
  - Cross-validation inside Optuna objective
  - Early stopping with callbacks
  - Best model selection

**Complete Example**:
```python
import optuna
from lightgbm import LGBMClassifier
from sklearn.model_selection import cross_val_score

def objective(trial):
    params = {
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
    }

    model = LGBMClassifier(**params, random_state=42)
    score = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc').mean()
    return score

study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())
study.optimize(objective, n_trials=100, show_progress_bar=True)

print(f"Best params: {study.best_params}")
print(f"Best score: {study.best_value}")

# Visualization
optuna.visualization.plot_optimization_history(study)
optuna.visualization.plot_param_importances(study)
```

**Datasets**: Titanic, House Prices (apply to previous competitions)

---

### Notebook 21: `21_shap_model_interpretation.ipynb` â­â­â­
**Goal**: Master model interpretation with SHAP

**Contents**:
- **SHAP Fundamentals**:
  - Shapley values from game theory
  - TreeSHAP for tree-based models
  - Local vs global interpretability
  - Feature attribution

- **SHAP Visualizations**:
  - Waterfall plots (single prediction explanation)
  - Force plots (interactive explanations)
  - Summary plots (global feature importance)
  - Dependence plots (feature interactions)
  - Decision plots (multi-class classification)

- **Practical Applications**:
  - Debugging model errors
  - Finding spurious correlations
  - Feature selection guided by SHAP
  - Explaining predictions to stakeholders
  - Detecting bias in models

- **Advanced SHAP**:
  - Clustering SHAP values
  - SHAP interaction values
  - Comparing SHAP across models
  - SHAP for regression vs classification

**Complete Example**:
```python
import shap
import lightgbm as lgb

# Train model
model = lgb.LGBMClassifier()
model.fit(X_train, y_train)

# Create SHAP explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Global importance
shap.summary_plot(shap_values, X_test, plot_type="bar")
shap.summary_plot(shap_values, X_test)

# Local explanation for single prediction
idx = 0
shap.waterfall_plot(shap.Explanation(
    values=shap_values[idx],
    base_values=explainer.expected_value,
    data=X_test.iloc[idx],
    feature_names=X_test.columns.tolist()
))

# Feature interactions
shap.dependence_plot("Age", shap_values, X_test, interaction_index="Sex")
```

**Datasets**: Titanic (interpret predictions), Credit Default (bias detection)

---

### Notebook 22: `22_stacking_meta_learning.ipynb` â­â­â­
**Goal**: Master stacking and meta-learning ensemble

**Contents**:
- **Stacking Fundamentals**:
  - Multi-level ensemble architecture
  - Base models (level 0) and meta-model (level 1)
  - Out-of-fold predictions to avoid overfitting
  - Diversity in base models

- **Implementation**:
  - Manual stacking with cross-validation
  - sklearn StackingClassifier/StackingRegressor
  - mlxtend StackingCVClassifier

- **Base Model Selection**:
  - LightGBM (fast, accurate)
  - XGBoost (robust)
  - CatBoost (categorical handling)
  - Neural networks (MLP)
  - Linear models (for diversity)

- **Meta-Model Selection**:
  - Logistic Regression (simple, fast)
  - LightGBM (capture interactions)
  - Ridge/Lasso (regularized linear)

- **Advanced Stacking**:
  - Multi-level stacking (3+ levels)
  - Feature engineering for meta-model
  - Including original features in meta-model
  - Blending vs Stacking comparison

**Complete Example**:
```python
from sklearn.ensemble import StackingClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression

# Define base models
base_models = [
    ('lgb', LGBMClassifier(n_estimators=100, learning_rate=0.05, random_state=42)),
    ('xgb', XGBClassifier(n_estimators=100, learning_rate=0.05, random_state=42)),
    ('cat', CatBoostClassifier(iterations=100, learning_rate=0.05, random_state=42, verbose=0)),
]

# Define meta-model
meta_model = LogisticRegression()

# Create stacking ensemble
stacking_clf = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5,  # Out-of-fold predictions
    stack_method='predict_proba',  # Use probabilities
    n_jobs=-1
)

# Train
stacking_clf.fit(X_train, y_train)

# Evaluate
print(f"Stacking CV Score: {stacking_clf.score(X_val, y_val)}")

# Compare with base models
for name, model in base_models:
    model.fit(X_train, y_train)
    print(f"{name} Score: {model.score(X_val, y_val)}")
```

**Datasets**: Titanic, House Prices (boost scores by 1-2%)

---

## Phase 8: Specialized Topics (Notebooks 23-25) ğŸ¯

### Notebook 23: `23_imbalanced_data_strategies.ipynb`
**Goal**: Handle imbalanced datasets effectively

**Contents**:
- **Imbalance Detection**:
  - Class distribution analysis
  - Stratified sampling importance

- **Sampling Techniques**:
  - Oversampling (SMOTE, ADASYN)
  - Undersampling (RandomUnderSampler, Tomek Links)
  - Hybrid (SMOTEENN, SMOTETomek)

- **GBDT-Specific Techniques**:
  - scale_pos_weight (XGBoost)
  - is_unbalance parameter (LightGBM)
  - class_weights parameter (CatBoost)
  - Focal Loss for extreme imbalance

- **Evaluation**:
  - Precision-Recall curves (NOT ROC for imbalanced)
  - F1-Score, F-beta score
  - Confusion matrix analysis

**Datasets**: Credit Card Fraud Detection (1% fraud rate)

---

### Notebook 24: `24_timeseries_feature_engineering.ipynb`
**Goal**: Advanced time series feature creation

**Contents**:
- **Temporal Features**:
  - Cyclical encoding (sin/cos for month, day)
  - Holiday indicators
  - Business day flags

- **Lag Features**:
  - Multiple lag windows
  - Lag feature selection (PACF)

- **Rolling Statistics**:
  - Moving averages (7-day, 30-day, 90-day)
  - Rolling std, min, max, quantiles
  - Exponentially weighted moving average (EWMA)

- **Domain-Specific Features**:
  - Promotional calendars
  - Event-based features
  - External variables (weather, economic indicators)

**Datasets**: Rossmann Store Sales, M5 Forecasting

---

### Notebook 25: `25_categorical_encoding_advanced.ipynb`
**Goal**: Master categorical variable encoding

**Contents**:
- **Traditional Encoding**:
  - One-Hot Encoding (low cardinality)
  - Label Encoding (ordinal)

- **Advanced Encoding**:
  - Target Encoding (mean encoding)
  - Frequency Encoding
  - Binary Encoding
  - Hash Encoding (high cardinality)

- **Embedding Methods**:
  - Entity Embeddings for categorical features
  - Category Encoding library

- **Handling High Cardinality**:
  - Rare category grouping
  - Hierarchical encoding
  - CatBoost's native handling vs manual encoding

**Datasets**: Avazu Click-Through Rate Prediction (high cardinality)

---

## Phase 9: Final Projects & Integration (Notebooks 26-28) ğŸ“

### Notebook 26: `26_tabular_deep_learning.ipynb`
**Goal**: Explore deep learning for tabular data

**Contents**:
- **TabNet**:
  - Attention-based architecture
  - Interpretability with masks
  - Comparison with GBDT

- **Neural Oblivious Decision Ensembles (NODE)**:
  - Differentiable trees
  - Hybrid approach

- **When to use DL vs GBDT**:
  - GBDT wins on most tabular data
  - DL advantages (multi-modal, transfer learning)

**Datasets**: Porto Seguro Safe Driver Prediction

---

### Notebook 27: `27_kaggle_competition_workflow.ipynb`
**Goal**: Complete competition workflow from start to finish

**Contents**:
- **Phase 1: Understanding**:
  - Read competition description and rules
  - Understand evaluation metric
  - Review starter kernels

- **Phase 2: EDA & Feature Engineering**:
  - Comprehensive EDA
  - Feature creation strategy
  - Validation strategy

- **Phase 3: Modeling**:
  - Baseline model
  - Hyperparameter tuning
  - Ensemble creation

- **Phase 4: Submission**:
  - Multiple submissions
  - Leaderboard analysis
  - Final model selection

**Practice Competition**: Choose from active Kaggle competitions

---

### Notebook 28: `28_your_project_comprehensive.ipynb` â­â­â­
**Goal**: Build your own complete ML project

**Contents**:
- **Project Ideas**:
  - Find dataset (UCI, Kaggle Datasets, government data)
  - Define business problem
  - Set evaluation criteria

- **Complete Pipeline**:
  - Data collection and cleaning
  - EDA and insights
  - Feature engineering
  - Model selection and tuning
  - Ensemble creation
  - Model interpretation (SHAP)
  - Deployment preparation

- **Documentation**:
  - README with project description
  - Jupyter notebook with clear sections
  - Model performance report

**Deliverable**: Complete portfolio project showcasing all learned skills

---

## Recommended Learning Path (Phases 5-9)

### Months 1-2: GBDT Foundations (Notebooks 13-16)
- Master LightGBM, XGBoost, CatBoost
- Practice: Titanic competition (baseline)

### Month 3: Kaggle Practice (Notebooks 17-19)
- Achieve Top 30% on Titanic
- Complete House Prices regression
- Tackle time series forecasting

### Month 4: Advanced Techniques (Notebooks 20-22)
- Master Optuna optimization
- Learn SHAP interpretation
- Build stacking ensembles

### Month 5: Specialized Topics (Notebooks 23-25)
- Handle imbalanced data
- Master time series features
- Advanced categorical encoding

### Month 6: Integration & Projects (Notebooks 26-28)
- Explore tabular DL
- Complete full Kaggle workflow
- Build final portfolio project

---

## Success Metrics (GBDT Phase)

By completing this curriculum, you will have:

âœ… **Competition Skills**:
- Kaggle Titanic: Top 30% (0.79+ accuracy)
- Kaggle House Prices: Top 20% (RMSLE < 0.13)
- Store Demand: Top 25% (SMAPE < 15%)

âœ… **Technical Mastery**:
- LightGBM, XGBoost, CatBoost proficiency
- Optuna hyperparameter optimization
- SHAP model interpretation
- Stacking ensembles
- Time series forecasting with GBDT

âœ… **Portfolio**:
- 3+ Kaggle competition submissions
- 1 complete end-to-end project
- Model interpretation reports

---

## Additional Resources

### Books
- "Kaggleã§å‹ã¤ãƒ‡ãƒ¼ã‚¿åˆ†æã®æŠ€è¡“" by é–€è„‡å¤§è¼” et al.
- "Hands-On Gradient Boosting with XGBoost and scikit-learn"
- "Feature Engineering for Machine Learning" by Alice Zheng

### Communities
- Kaggle Discussions and Kernels
- GitHub: Kaggle Solutions (e.g., Kazuki Onodera's repos)
- Reddit: r/MachineLearning, r/kaggle

### Tools
- Optuna: https://optuna.org/
- SHAP: https://shap.readthedocs.io/
- Category Encoders: https://contrib.scikit-learn.org/category_encoders/

---

## Next Steps

After completing this comprehensive plan:
1. **Compete regularly**: Enter 1-2 active Kaggle competitions per month
2. **Specialize**: Choose a domain (NLP, CV, Time Series, Recommender Systems)
3. **Contribute**: Share kernels, write blog posts, mentor others
4. **Advanced ML**: MLOps, model deployment, production systems
5. **Research**: Read papers, implement state-of-the-art methods

---

## Phase 10: Optimization Methods (H_optimization/) ğŸ¯

ã“ã®å˜å…ƒã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã§åºƒãä½¿ã‚ã‚Œã‚‹ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–æ‰‹æ³•ã®ç†è«–ã¨å®Ÿè£…ã‚’å­¦ã³ã¾ã™ã€‚
Notebook 20ï¼ˆOptunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼‰ã®å‰æçŸ¥è­˜ã¨ã—ã¦æ¨å¥¨ã•ã‚Œã¾ã™ã€‚

> **ğŸ“ å‘½åè¦å‰‡ï¼ˆæ–°æ–¹å¼ï¼‰**
> - å˜å…ƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: `H_optimization/`ï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã§å˜å…ƒé–“ã®é †åºã‚’ç®¡ç†ï¼‰
> - ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯: ç•ªå·ãªã—ã€å†…å®¹ã‚’è¡¨ã™åå‰ã®ã¿
> - é †åº: ã“ã®ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã§ä¸€å…ƒç®¡ç†

---

### `blackbox_intro.ipynb`
**Goal**: ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–ã®å…¨ä½“åƒã‚’ç†è§£ã™ã‚‹

**Contents**:
- **æœ€é©åŒ–å•é¡Œã®å®šå¼åŒ–**:
  - ç›®çš„é–¢æ•°ã€åˆ¶ç´„æ¡ä»¶ã€æ¢ç´¢ç©ºé–“
  - é€£ç¶šæœ€é©åŒ– vs é›¢æ•£æœ€é©åŒ–
  - å‹¾é…ãƒ™ãƒ¼ã‚¹ vs å‹¾é…ãƒ•ãƒªãƒ¼æ‰‹æ³•

- **ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–ã®ç‰¹å¾´**:
  - ç›®çš„é–¢æ•°ã®å†…éƒ¨æ§‹é€ ãŒä¸æ˜
  - å‹¾é…æƒ…å ±ãŒåˆ©ç”¨ã§ããªã„
  - è©•ä¾¡ã‚³ã‚¹ãƒˆãŒé«˜ã„å ´åˆã®æˆ¦ç•¥

- **å±€æ‰€è§£ vs å¤§åŸŸè§£**:
  - å¤šå³°æ€§é–¢æ•°ã®å¯è¦–åŒ–
  - å±€æ‰€è§£ã«é™¥ã‚‹å•é¡Œ
  - å¤§åŸŸæœ€é©åŒ–ã®å›°é›£ã•

- **æ‰‹æ³•ã®åˆ†é¡ã¨æ¯”è¼ƒ**:
  - ç„¡æƒ…å ±æ¢ç´¢ï¼ˆGrid/Random Searchï¼‰
  - ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ï¼ˆãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼‰
  - ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ï¼ˆé€²åŒ–è¨ˆç®—ã€ã‚¹ã‚¦ã‚©ãƒ¼ãƒ çŸ¥èƒ½ï¼‰
  - å„æ‰‹æ³•ã®é•·æ‰€ãƒ»çŸ­æ‰€ãƒ»é©ç”¨å ´é¢

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–¢æ•°**:
- Rastriginé–¢æ•°ã€Rosenbrocké–¢æ•°ã€Ackleyé–¢æ•°
- 2Då¯è¦–åŒ–ã«ã‚ˆã‚‹é–¢æ•°ç‰¹æ€§ã®ç†è§£

---

### `simulated_annealing.ipynb`
**Goal**: ç„¼ããªã¾ã—æ³•ã®åŸç†ã¨å®Ÿè£…ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹

**Contents**:
- **ç‰©ç†çš„ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼**:
  - é‡‘å±ã®ç„¼ããªã¾ã—éç¨‹
  - ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ã¨æœ€é©åŒ–ã®å¯¾å¿œ

- **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©³ç´°**:
  - ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹åŸºæº–ï¼ˆMetropolis criterionï¼‰
  - æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆå†·å´ç‡ï¼‰
  - è¿‘å‚è§£ã®ç”Ÿæˆæ–¹æ³•

- **æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹**:
  - é«˜æ¸©ï¼šåºƒç¯„å›²æ¢ç´¢ï¼ˆexplorationï¼‰
  - ä½æ¸©ï¼šå±€æ‰€æ”¹å–„ï¼ˆexploitationï¼‰
  - æ¸©åº¦æ¸›è¡°ã«ã‚ˆã‚‹æ®µéšçš„åæŸ

- **å®Ÿè£…ã¨å¯è¦–åŒ–**:
  - Pythonã«ã‚ˆã‚‹ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…
  - æ¢ç´¢è»Œè·¡ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
  - æ¸©åº¦ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å½±éŸ¿æ¯”è¼ƒ

- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**:
  - åˆæœŸæ¸©åº¦ã®è¨­å®š
  - å†·å´ç‡ã®é¸æŠ
  - åœæ­¢æ¡ä»¶ã®è¨­è¨ˆ

**å¿œç”¨ä¾‹**: å·¡å›ã‚»ãƒ¼ãƒ«ã‚¹ãƒãƒ³å•é¡Œã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å•é¡Œ

---

### `genetic_algorithm.ipynb`
**Goal**: éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŸç†ã¨å®Ÿè£…ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹

**Contents**:
- **ç”Ÿç‰©å­¦çš„ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼**:
  - è‡ªç„¶é¸æŠã¨é©è€…ç”Ÿå­˜
  - éºä¼å­ã€æŸ“è‰²ä½“ã€å€‹ä½“ã€é›†å›£

- **åŸºæœ¬ã‚ªãƒšãƒ¬ãƒ¼ã‚¿**:
  - é¸æŠï¼ˆSelectionï¼‰ï¼šãƒ«ãƒ¼ãƒ¬ãƒƒãƒˆé¸æŠã€ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆé¸æŠã€ã‚¨ãƒªãƒ¼ãƒˆé¸æŠ
  - äº¤å‰ï¼ˆCrossoverï¼‰ï¼šä¸€ç‚¹äº¤å‰ã€äºŒç‚¹äº¤å‰ã€ä¸€æ§˜äº¤å‰
  - çªç„¶å¤‰ç•°ï¼ˆMutationï¼‰ï¼šãƒ“ãƒƒãƒˆåè»¢ã€ã‚¬ã‚¦ã‚·ã‚¢ãƒ³å¤‰ç•°

- **é©å¿œåº¦é–¢æ•°**:
  - ç›®çš„é–¢æ•°ã‹ã‚‰é©å¿œåº¦ã¸ã®å¤‰æ›
  - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ‰‹æ³•
  - åˆ¶ç´„å‡¦ç†ï¼ˆãƒšãƒŠãƒ«ãƒ†ã‚£æ³•ã€ä¿®å¾©æ³•ï¼‰

- **åæŸæ€§ã¨å¤šæ§˜æ€§**:
  - æ—©ç†ŸåæŸã®å•é¡Œ
  - å¤šæ§˜æ€§ç¶­æŒã®æŠ€æ³•
  - æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹

- **å®Ÿè£…ã¨å¯è¦–åŒ–**:
  - Pythonã«ã‚ˆã‚‹ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…
  - ä¸–ä»£ã”ã¨ã®é›†å›£åˆ†å¸ƒã®å¯è¦–åŒ–
  - é©å¿œåº¦ã®é€²åŒ–éç¨‹

**å¿œç”¨ä¾‹**: é–¢æ•°æœ€é©åŒ–ã€ç‰¹å¾´é¸æŠã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹é€ æ¢ç´¢

---

### `particle_swarm.ipynb`
**Goal**: ç²’å­ç¾¤æœ€é©åŒ–ã®åŸç†ã¨å®Ÿè£…ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹

**Contents**:
- **ç¾¤çŸ¥èƒ½ã®ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼**:
  - é³¥ã®ç¾¤ã‚Œã€é­šã®ç¾¤ã‚Œã®è¡Œå‹•
  - å€‹ä½“ã®å±€æ‰€æƒ…å ±ã¨ç¾¤ã®å¤§åŸŸæƒ…å ±

- **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©³ç´°**:
  - ç²’å­ã®ä½ç½®ã¨é€Ÿåº¦
  - å€‹ä½“æœ€è‰¯ï¼ˆpbestï¼‰ã¨ç¾¤æœ€è‰¯ï¼ˆgbestï¼‰
  - é€Ÿåº¦æ›´æ–°å¼ã¨ä½ç½®æ›´æ–°å¼

- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¹å‰²**:
  - æ…£æ€§é‡ã¿ï¼ˆinertia weightï¼‰
  - èªçŸ¥ä¿‚æ•°ï¼ˆcognitive coefficientï¼‰
  - ç¤¾ä¼šä¿‚æ•°ï¼ˆsocial coefficientï¼‰
  - é€Ÿåº¦åˆ¶é™ï¼ˆvelocity clampingï¼‰

- **ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³**:
  - ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ™ã‚¹ãƒˆ vs ãƒ­ãƒ¼ã‚«ãƒ«ãƒ™ã‚¹ãƒˆ
  - é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
  - QPSOï¼ˆé‡å­ç²’å­ç¾¤æœ€é©åŒ–ï¼‰

- **å®Ÿè£…ã¨å¯è¦–åŒ–**:
  - Pythonã«ã‚ˆã‚‹ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…
  - ç²’å­ã®å‹•ãã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
  - åæŸéç¨‹ã®å¯è¦–åŒ–

**å¿œç”¨ä¾‹**: é€£ç¶šæœ€é©åŒ–ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿æœ€é©åŒ–

---

### `bayesian_optimization_gp.ipynb` â­â­â­
**Goal**: ã‚¬ã‚¦ã‚¹éç¨‹ãƒ™ãƒ¼ã‚¹ã®ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ç†è§£ã™ã‚‹

**Contents**:
- **ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®åŸç†**:
  - ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆä»£ç†ãƒ¢ãƒ‡ãƒ«ï¼‰
  - ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–
  - æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

- **ã‚¬ã‚¦ã‚¹éç¨‹ï¼ˆGaussian Processï¼‰**:
  - ã‚¬ã‚¦ã‚¹éç¨‹å›å¸°ã®åŸºç¤
  - ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ï¼ˆRBFã€MatÃ©rnï¼‰
  - å¹³å‡é–¢æ•°ã¨å…±åˆ†æ•£é–¢æ•°
  - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–

- **ç²å¾—é–¢æ•°ï¼ˆAcquisition Functionï¼‰**:
  - Expected Improvement (EI)
  - Probability of Improvement (PI)
  - Upper Confidence Bound (UCB)
  - ç²å¾—é–¢æ•°ã®å¯è¦–åŒ–ã¨æ¯”è¼ƒ

- **å®Ÿè£…ã¨å¯è¦–åŒ–**:
  - scikit-optimizeã«ã‚ˆã‚‹å®Ÿè£…
  - GPyOptã«ã‚ˆã‚‹å®Ÿè£…
  - ç²å¾—é–¢æ•°ã®å¤‰åŒ–ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
  - ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°éç¨‹

- **GPãƒ™ãƒ¼ã‚¹ã®é™ç•Œ**:
  - è¨ˆç®—é‡ O(nÂ³) ã®å•é¡Œ
  - é«˜æ¬¡å…ƒã§ã®æ€§èƒ½åŠ£åŒ–
  - â†’ TPEã¸ã®å‹•æ©Ÿä»˜ã‘

**å‰æçŸ¥è­˜**: åŸºç¤çš„ãªç¢ºç‡ãƒ»çµ±è¨ˆã®çŸ¥è­˜

---

### `tpe_algorithm.ipynb` â­â­â­
**Goal**: TPEï¼ˆTree-structured Parzen Estimatorï¼‰ã¨Define-by-Runã‚’æ·±ãç†è§£ã™ã‚‹

**Contents**:
- **TPEã®å‹•æ©Ÿ**:
  - ã‚¬ã‚¦ã‚¹éç¨‹ã®é™ç•Œã‚’å…‹æœ
  - Optunaã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

- **Parzenæ¨å®šï¼ˆã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šï¼‰**:
  - ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šã®åŸºç¤
  - ãƒãƒ³ãƒ‰å¹…ã®é¸æŠ
  - å¯è¦–åŒ–ã«ã‚ˆã‚‹ç›´æ„Ÿçš„ç†è§£

- **TPEã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
  - è¦³æ¸¬å€¤ã‚’è‰¯ã„ç¾¤ l(x) ã¨æ‚ªã„ç¾¤ g(x) ã«åˆ†å‰²
  - åˆ†å‰²é–¾å€¤ Î³ï¼ˆä¸Šä½ä½•%ã‚’ã€Œè‰¯ã„ã€ã¨ã™ã‚‹ã‹ï¼‰
  - å„ç¾¤ã®å¯†åº¦æ¨å®š
  - ç²å¾—é–¢æ•°: l(x)/g(x) ã®æœ€å¤§åŒ–

  ```
  è‰¯ã„ã‚¹ã‚³ã‚¢ã®é ˜åŸŸã‚’å¯†ã«ã€æ‚ªã„ã‚¹ã‚³ã‚¢ã®é ˜åŸŸã‚’ç–ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

  l(x): ä¸Šä½Î³%ã®è¦³æ¸¬å€¤ã‹ã‚‰æ¨å®šã—ãŸå¯†åº¦
  g(x): ä¸‹ä½(1-Î³)%ã®è¦³æ¸¬å€¤ã‹ã‚‰æ¨å®šã—ãŸå¯†åº¦

  æ¬¡ã®æ¢ç´¢ç‚¹ = argmax l(x)/g(x)
  ```

- **GPã¨ã®æ¯”è¼ƒ**:
  | è¦³ç‚¹ | Gaussian Process | TPE |
  |------|------------------|-----|
  | è¨ˆç®—é‡ | O(nÂ³) | O(n log n) |
  | é«˜æ¬¡å…ƒ | è‹¦æ‰‹ | æ¯”è¼ƒçš„å¼·ã„ |
  | ã‚«ãƒ†ã‚´ãƒªå¤‰æ•° | æ‰±ã„ã«ãã„ | è‡ªç„¶ã«æ‰±ãˆã‚‹ |
  | ä¸ç¢ºå®Ÿæ€§ | æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ– | æš—é»™çš„ |

- **Tree-structuredã®æ„å‘³**:
  - æ¡ä»¶ä»˜ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ‰±ã„
  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ã®ä¾å­˜é–¢ä¿‚
  - éšå±¤çš„ãªæ¢ç´¢ç©ºé–“ã®è¡¨ç¾

- **Define-by-Runï¼ˆå‹•çš„æ¢ç´¢ç©ºé–“å®šç¾©ï¼‰**:
  - Define-and-Run vs Define-by-Run
    ```python
    # Define-and-Runï¼ˆé™çš„ï¼‰: äº‹å‰ã«å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©
    param_grid = {
        'model': ['RF', 'SVM'],
        'RF__n_estimators': [100, 200],  # SVMã§ã‚‚å®šç¾©ãŒå¿…è¦
        'SVM__C': [0.1, 1.0],             # RFã§ã‚‚å®šç¾©ãŒå¿…è¦
    }

    # Define-by-Runï¼ˆå‹•çš„ï¼‰: å®Ÿè¡Œæ™‚ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã‘å®šç¾©
    def objective(trial):
        model = trial.suggest_categorical('model', ['RF', 'SVM'])
        if model == 'RF':
            n_estimators = trial.suggest_int('n_estimators', 100, 500)
        else:  # SVM
            C = trial.suggest_float('C', 0.1, 10.0)
    ```
  - æ¡ä»¶åˆ†å²ã«ã‚ˆã‚‹æŸ”è»Ÿãªæ¢ç´¢ç©ºé–“
  - TPEã®Treeæ§‹é€ ã¨ã®ç›¸æ€§ã®è‰¯ã•
  - å®Ÿè¡Œæ™‚ã®æ¢ç´¢ç©ºé–“ã®å‹•çš„æ§‹ç¯‰

- **å®Ÿè£…ã¨å¯è¦–åŒ–**:
  - Pythonã«ã‚ˆã‚‹ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…
  - l(x)ã¨g(x)ã®å¯†åº¦åˆ†å¸ƒã®å¯è¦–åŒ–
  - æ¢ç´¢ç‚¹ã®é¸æŠéç¨‹ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
  - æ¡ä»¶ä»˜ãæ¢ç´¢ç©ºé–“ã®DAGå¯è¦–åŒ–

---

### `pruning_early_stopping.ipynb` â­â­
**Goal**: æ—©æœŸçµ‚äº†ï¼ˆPruningï¼‰ã®ç†è«–ã¨å®Ÿè£…ã‚’ç†è§£ã™ã‚‹

**Contents**:
- **Pruningã®å‹•æ©Ÿ**:
  - è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã®åŠ¹ç‡çš„ãªé…åˆ†
  - è¦‹è¾¼ã¿ã®ãªã„è©¦è¡Œã®æ—©æœŸæ‰“ã¡åˆ‡ã‚Š
  - ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã¨ã®é–¢é€£

- **Successive Halving**:
  - ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©³ç´°
    ```
    ãƒ©ã‚¦ãƒ³ãƒ‰0: å…¨81è©¦è¡Œã‚’1ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ â†’ ä¸Šä½1/3ã‚’æ®‹ã™ï¼ˆ27è©¦è¡Œï¼‰
    ãƒ©ã‚¦ãƒ³ãƒ‰1: 27è©¦è¡Œã‚’3ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ â†’ ä¸Šä½1/3ã‚’æ®‹ã™ï¼ˆ9è©¦è¡Œï¼‰
    ãƒ©ã‚¦ãƒ³ãƒ‰2: 9è©¦è¡Œã‚’9ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ â†’ ä¸Šä½1/3ã‚’æ®‹ã™ï¼ˆ3è©¦è¡Œï¼‰
    ãƒ©ã‚¦ãƒ³ãƒ‰3: 3è©¦è¡Œã‚’27ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ â†’ æœ€è‰¯ã®1è©¦è¡Œã‚’é¸æŠ
    ```
  - ç†è«–çš„èƒŒæ™¯ï¼ˆäºˆç®—é…åˆ†å•é¡Œï¼‰
  - è¨ˆç®—é‡ã®å‰Šæ¸›åŠ¹æœ

- **Hyperband**:
  - Successive Halvingã®æ‹¡å¼µ
  - è¤‡æ•°ã®ãƒ–ãƒ©ã‚±ãƒƒãƒˆï¼ˆæ—©æœŸæ‰“ã¡åˆ‡ã‚Šã®å¼·åº¦ï¼‰
  - æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹
  - ç†è«–çš„ä¿è¨¼

- **Median Pruner**:
  - ã‚·ãƒ³ãƒ—ãƒ«ã§å®Ÿç”¨çš„ãªPruner
  - ä¸­å¤®å€¤ã¨ã®æ¯”è¼ƒã«ã‚ˆã‚‹æ‰“ã¡åˆ‡ã‚Š
  - n_startup_trialsã€n_warmup_stepsã®æ„å‘³

- **ãã®ä»–ã®Pruner**:
  - Percentile Pruner
  - Threshold Pruner
  - Patient Prunerï¼ˆæ”¹è‰¯ç‰ˆï¼‰

- **Pruningã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³**:
  ```python
  def objective(trial):
      model = create_model(trial)
      for epoch in range(100):
          train_one_epoch(model)
          val_score = evaluate(model)

          # ä¸­é–“çµæœã‚’å ±å‘Š
          trial.report(val_score, epoch)

          # Pruningåˆ¤å®š
          if trial.should_prune():
              raise optuna.TrialPruned()

      return val_score
  ```

- **Pruningã®æ³¨æ„ç‚¹**:
  - å­¦ç¿’æ›²ç·šã®æ€§è³ªï¼ˆæ—©æœŸã®æ€§èƒ½ãŒæœ€çµ‚æ€§èƒ½ã‚’äºˆæ¸¬ã§ãã‚‹ã‹ï¼‰
  - warmupã®é‡è¦æ€§
  - Pruneã—ã™ãã®ãƒªã‚¹ã‚¯

- **å¯è¦–åŒ–**:
  - Pruneã•ã‚ŒãŸè©¦è¡Œã®å­¦ç¿’æ›²ç·š
  - ãƒªã‚½ãƒ¼ã‚¹é…åˆ†ã®åŠ¹ç‡æ€§ã®æ¯”è¼ƒ
  - Successive Halvingã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: Notebook 20ï¼ˆOptunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼‰

---

### `optimization_comparison.ipynb`
**Goal**: å„æœ€é©åŒ–æ‰‹æ³•ã‚’æ¯”è¼ƒã—ã€å•é¡Œã«å¿œã˜ãŸæ‰‹æ³•é¸æŠãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹

**Contents**:
- **ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“**:
  - è¤‡æ•°ã®ãƒ†ã‚¹ãƒˆé–¢æ•°ã§ã®æ€§èƒ½æ¯”è¼ƒ
  - åæŸé€Ÿåº¦ã®æ¯”è¼ƒ
  - è§£ã®å“è³ªã®æ¯”è¼ƒ

- **å•é¡Œç‰¹æ€§ã¨æ‰‹æ³•é¸æŠ**:
  | å•é¡Œç‰¹æ€§ | æ¨å¥¨æ‰‹æ³• |
  |----------|----------|
  | ä½æ¬¡å…ƒãƒ»è©•ä¾¡ã‚³ã‚¹ãƒˆé«˜ | ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆGPï¼‰ |
  | ä¸­æ¬¡å…ƒãƒ»è©•ä¾¡ã‚³ã‚¹ãƒˆé«˜ | TPE |
  | é«˜æ¬¡å…ƒãƒ»è©•ä¾¡ã‚³ã‚¹ãƒˆä½ | ç²’å­ç¾¤æœ€é©åŒ–ã€éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
  | é›¢æ•£æœ€é©åŒ– | éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ç„¼ããªã¾ã—æ³• |
  | ãƒã‚¤ã‚ºã®å¤šã„ç›®çš„é–¢æ•° | ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆãƒã‚¤ã‚ºè€ƒæ…®ï¼‰ |
  | ä¸¦åˆ—è©•ä¾¡å¯èƒ½ | ç²’å­ç¾¤æœ€é©åŒ–ã€éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  |
  | ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ãŒå¤šã„ | TPE |

- **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•**:
  - ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ + å±€æ‰€æ¢ç´¢
  - è¤‡æ•°æ‰‹æ³•ã®çµ„ã¿åˆã‚ã›

- **å®Ÿè·µçš„ãªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**:
  - è¨ˆç®—äºˆç®—ã«å¿œã˜ãŸæ‰‹æ³•é¸æŠ
  - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã¸ã®å¿œç”¨
  - AutoML ã¨ã®é–¢ä¿‚

**ã¾ã¨ã‚**:
- å„æ‰‹æ³•ã®é•·æ‰€ãƒ»çŸ­æ‰€ã®æ•´ç†
- å®Ÿå‹™ã§ã®é¸æŠæŒ‡é‡
- ä»Šå¾Œã®å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

---

### Recommended Learning Path (Optimization Methods)

```
H_optimization/
â”œâ”€â”€ blackbox_intro.ipynb            â† 1. æœ€é©åŒ–ã®å…¨ä½“åƒ
â”œâ”€â”€ simulated_annealing.ipynb       â† 2. ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹â‘ 
â”œâ”€â”€ genetic_algorithm.ipynb         â† 3. ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹â‘¡
â”œâ”€â”€ particle_swarm.ipynb            â† 4. ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹â‘¢
â”œâ”€â”€ bayesian_optimization_gp.ipynb  â† 5. ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆGPï¼‰
â”œâ”€â”€ tpe_algorithm.ipynb             â† 6. TPE + Define-by-Run â­
â”œâ”€â”€ pruning_early_stopping.ipynb    â† 7. Pruningï¼ˆæ—©æœŸçµ‚äº†ï¼‰â­
â””â”€â”€ optimization_comparison.ipynb   â† 8. æ‰‹æ³•æ¯”è¼ƒã¨é¸æŠæŒ‡é‡
```

**å­¦ç¿’ã®æµã‚Œ**:
1. **1-4**: å¤å…¸çš„ãªãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹æ‰‹æ³•ã§æœ€é©åŒ–ã®åŸºç¤ã‚’å›ºã‚ã‚‹
2. **5-6**: ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ï¼ˆGP â†’ TPEï¼‰ã§ç¾ä»£çš„æ‰‹æ³•ã‚’å­¦ã¶
3. **7**: è¨ˆç®—åŠ¹ç‡ã‚’ä¸Šã’ã‚‹Pruningã®ä»•çµ„ã¿ã‚’ç†è§£
4. **8**: å…¨æ‰‹æ³•ã‚’æ¯”è¼ƒã—ã€å•é¡Œã«å¿œã˜ãŸé¸æŠãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹

**æ‰€è¦æ™‚é–“**: ç´„3é€±é–“

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: Notebook 20ï¼ˆOptunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼‰ã§å®Ÿè·µçš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¸

---

*You now have a complete roadmap from ML basics to Kaggle Grandmaster-level skills. Let's build amazing models! ğŸš€*
