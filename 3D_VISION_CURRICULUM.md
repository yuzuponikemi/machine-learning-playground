# 3D Computer Vision 基礎カリキュラム計画
## Unit 0.3「知覚と空間：3D Computer Vision 基礎」

## 📚 概要

このカリキュラムは、機械学習の基礎を学習済みの学習者が、2D画像処理から3D空間理解へと認識を拡張し、最終的にNeRF/3DGaussian Splatting（3DGS）のような最新の3D生成技術を理解・実装できるようになることを目指します。

### 学習の流れ

```
カメラモデル → 射影幾何 → エピポーラ幾何 → ステレオ視
    ↓
特徴点抽出 → マッチング → SfM → バンドル調整
    ↓
Ray Casting → ボクセル空間 → NeRF/3DGS の橋渡し
```

### 特徴

- ✅ **8個の段階的なノートブック**: カメラの基礎から3D再構成まで
- ✅ **NumPy/OpenCV/Open3Dベース**: 実践的な3D処理実装
- ✅ **段階的な学習**: 2D→3D へと空間認識を拡張
- ✅ **理論と実装のバランス**: 幾何学的背景と実装コードの両方を提供
- ✅ **視覚的な理解**: 豊富な3D可視化で直感的に理解

---

## 🎯 前提知識

このカリキュラムを始める前に、以下の知識があることが推奨されます：

- ✅ Pythonプログラミングの基礎
- ✅ NumPy、Matplotlibの基本的な使い方
- ✅ 線形代数の基礎（行列演算、固有値、特異値分解）
- ✅ 基礎的な数学（微分、最適化、確率の基本）
- ✅ 画像処理の基礎（ピクセル、色空間、フィルタリング）

**推奨**: このリポジトリのノートブック00-12（機械学習基礎コース）を完了していること

---

## 🌟 このユニットが「認知の拡張」にどう繋がるか

3D Computer Visionを学ぶことは、単なる技術習得ではなく、**「視点という主観」と「空間という客観」の関係性を数学的に記述すること**に他なりません。

### 境界の揺らぎ
カメラパラメータを操作することは、自分自身の「視覚」というインターフェースをハックすることです。

### デジタルゴーストへの応用
自分の過去の動画から3D空間を復元する際、SfMの知識があれば「なぜこの角度の映像が足りないのか」「なぜ再現が歪むのか」を論理的にデバッグできるようになります。

### 次世代の世界モデル
NeoVerseなどの4D技術において、AIが空間を理解するための必須の文法となります。

---

## 📖 カリキュラム詳細

### Phase 1: カメラモデルと射影幾何（推定時間: 6-8時間）

#### Notebook 50: ピンホールカメラモデルの基礎
**ファイル名**: `notebooks/3d-vision/50_pinhole_camera_model_v1.ipynb`

| 項目 | 内容 |
|------|------|
| **推定学習時間** | 90-120分 |
| **難易度** | ★★☆☆☆（初級） |
| **カテゴリ** | 基礎 |

**📋 学習目標**
- [ ] ピンホールカメラモデルの原理を理解できる
- [ ] 焦点距離と主点の役割を説明できる
- [ ] カメラ内部パラメータ行列Kを構築できる
- [ ] 3D点から2Dピクセルへの射影を実装できる
- [ ] レンズ歪み（歪み係数）の効果を理解できる

**🎓 主な内容**
1. カメラの基礎
   - 人間の目とカメラの類似性
   - ピンホールカメラの原理
   - 画像形成のプロセス
2. カメラパラメータ
   - 焦点距離（focal length）f_x, f_y
   - 主点（principal point）c_x, c_y
   - 内部パラメータ行列 K の構成
3. 射影変換
   - 3D世界座標系から2D画像座標系への変換
   - 同次座標系の導入
   - 射影行列の計算
4. レンズ歪みモデル
   - 放射歪み（radial distortion）
   - 接線歪み（tangential distortion）
   - OpenCVの歪み補正

**📊 実装内容**
```python
# カメラ内部パラメータの定義
# 3D点の2D投影
# 焦点距離の影響の可視化
# レンズ歪みのシミュレーション
```

**🔗 実世界との接続**
- スマートフォンカメラの内部パラメータ取得
- 異なる焦点距離での撮影効果の理解
- AR/VRにおけるカメラキャリブレーションの重要性

---

#### Notebook 51: カメラ外部パラメータと座標変換
**ファイル名**: `notebooks/3d-vision/51_camera_extrinsics_transforms_v1.ipynb`

| 項目 | 内容 |
|------|------|
| **推定学習時間** | 90-120分 |
| **難易度** | ★★★☆☆（中級） |
| **カテゴリ** | 基礎 |

**📋 学習目標**
- [ ] 回転行列とその性質を理解できる
- [ ] ロドリゲスの公式を用いて回転を表現できる
- [ ] 並進ベクトルの役割を説明できる
- [ ] 外部パラメータ行列[R|t]を構築できる
- [ ] 世界座標系からカメラ座標系への変換を実装できる

**🎓 主な内容**
1. 3D座標変換の基礎
   - 世界座標系（World Coordinate System）
   - カメラ座標系（Camera Coordinate System）
   - 座標系間の変換
2. 回転の表現
   - 回転行列 R の性質（直交行列）
   - オイラー角（Euler angles）
   - ロドリゲスの公式（Rodrigues' formula）
   - 四元数（Quaternion）の紹介
3. 並進の表現
   - 並進ベクトル t
   - カメラの位置と向き
4. 外部パラメータ
   - [R|t] の構成
   - 完全な射影行列 P = K[R|t]
   - 複数カメラの相対姿勢

**📊 実装内容**
```python
# 回転行列の生成と可視化
# ロドリゲス変換の実装
# カメラ外部パラメータの設定
# 複数視点からの3Dオブジェクト投影
# 座標変換の可視化
```

**🔗 実世界との接続**
- ドローンやロボットのカメラポーズ推定
- SLAM（Simultaneous Localization and Mapping）の基礎
- モーションキャプチャシステムの原理

---

#### Notebook 52: カメラキャリブレーション
**ファイル名**: `notebooks/3d-vision/52_camera_calibration_v1.ipynb`

| 項目 | 内容 |
|------|------|
| **推定学習時間** | 120-150分 |
| **難易度** | ★★★☆☆（中級） |
| **カテゴリ** | 実践 |

**📋 学習目標**
- [ ] カメラキャリブレーションの必要性を理解できる
- [ ] チェスボードパターンを用いた較正ができる
- [ ] Zhang's methodの原理を説明できる
- [ ] OpenCVを用いて実カメラをキャリブレーションできる
- [ ] 再投影誤差を評価できる

**🎓 主な内容**
1. キャリブレーションの概要
   - なぜキャリブレーションが必要か
   - 既知の3D構造（チェスボード）の利用
   - 複数視点からの観測
2. Zhang's Method
   - 平面パターンによるキャリブレーション
   - ホモグラフィ行列の推定
   - 内部・外部パラメータの分離
3. 実装と評価
   - OpenCV calibrateCamera の使用
   - 歪み係数の推定
   - 再投影誤差の計算
   - 結果の可視化
4. キャリブレーション結果の検証
   - 歪み補正前後の比較
   - 精度の評価指標
   - よくある失敗パターン

**📊 実装内容**
```python
# チェスボードコーナー検出
# 3D-2D対応点の作成
# カメラ行列と歪み係数の推定
# 再投影誤差の可視化
# 歪み補正の適用
# キャリブレーション結果の保存・読み込み
```

**🔗 実世界との接続**
- Webカメラやスマホカメラのキャリブレーション
- マルチカメラシステムの較正
- 産業用ロボットビジョンでの応用

---

### Phase 2: エピポーラ幾何とステレオ視（推定時間: 8-10時間）

#### Notebook 53: エピポーラ幾何の基礎
**ファイル名**: `notebooks/3d-vision/53_epipolar_geometry_fundamentals_v1.ipynb`

| 項目 | 内容 |
|------|------|
| **推定学習時間** | 120-150分 |
| **難易度** | ★★★☆☆（中級） |
| **カテゴリ** | 理論 |

**📋 学習目標**
- [ ] エピポーラ幾何の基本概念を理解できる
- [ ] エピポーラ線、エピポール、エピポーラ平面を説明できる
- [ ] 基礎行列（Fundamental Matrix）Fの意味を理解できる
- [ ] 本質行列（Essential Matrix）Eの意味を理解できる
- [ ] 8点アルゴリズムを実装できる

**🎓 主な内容**
1. 2視点幾何学の基礎
   - 2つのカメラから同じ点を観測する
   - エピポール（epipole）の定義
   - エピポーラ平面（epipolar plane）
   - エピポーラ線（epipolar line）
2. 基礎行列（Fundamental Matrix）
   - F行列の定義: x'^T F x = 0
   - F行列の性質（rank 2、7自由度）
   - 8点アルゴリズムによる推定
   - RANSACによるロバスト推定
3. 本質行列（Essential Matrix）
   - E行列の定義: E = K'^T F K
   - E行列の性質（特別な特異値構造）
   - カメラが較正済みの場合の利点
4. カメラポーズの復元
   - E行列からR, tの抽出
   - 4つの候補解とその検証
   - チェイラリティチェック

**📊 実装内容**
```python
# エピポーラ幾何の可視化
# 対応点からのF行列推定
# エピポーラ線の描画
# E行列の計算
# R, tの復元と検証
```

**🔗 概念の視覚的理解**
- エピポーラ制約による対応点探索の効率化
- ステレオマッチングの幾何学的基礎
- 視差（disparity）と深度の関係

---

#### Notebook 54: ステレオビジョンと深度推定
**ファイル名**: `notebooks/3d-vision/54_stereo_vision_depth_estimation_v1.ipynb`

| 項目 | 内容 |
|------|------|
| **推定学習時間** | 150-180分 |
| **難易度** | ★★★★☆（上級） |
| **カテゴリ** | 実践 |

**📋 学習目標**
- [ ] ステレオビジョンの原理を理解できる
- [ ] 視差（disparity）から深度を計算できる
- [ ] ブロックマッチングアルゴリズムを実装できる
- [ ] SGM（Semi-Global Matching）の概要を理解できる
- [ ] 深度マップを生成して可視化できる

**🎓 主な内容**
1. ステレオビジョンの原理
   - 両眼視差（binocular disparity）
   - 平行化（rectification）の重要性
   - 視差と深度の関係式: Z = (f * B) / d
2. ステレオ平行化
   - 画像平行化の目的
   - Bouguet's algorithmの概要
   - OpenCVのstereoRectify
3. 対応点探索
   - ブロックマッチング（Block Matching）
   - SAD（Sum of Absolute Differences）
   - ウィンドウサイズの影響
   - Semi-Global Matching（SGM）の導入
4. 深度マップの生成
   - OpenCVのStereoBM, StereoSGBM
   - パラメータチューニング
   - 深度マップのフィルタリング
   - 3D点群への変換

**📊 実装内容**
```python
# ステレオ画像ペアの読み込み
# 画像平行化の実行
# ブロックマッチングの実装（スクラッチ）
# OpenCV StereoBM/SGBM の使用
# 視差マップから深度マップへの変換
# 深度マップの可視化（カラーマップ）
# 3D点群の生成と表示（Open3D）
```

**🔗 実世界との接続**
- 自動運転車の障害物検知
- ロボットのナビゲーション
- AR/VRの深度センシング
- 3Dスキャニング

---

### Phase 3: Structure from Motion（SfM）（推定時間: 10-12時間）

#### Notebook 55: 特徴点検出とマッチング
**ファイル名**: `notebooks/3d-vision/55_feature_detection_matching_v1.ipynb`

| 項目 | 内容 |
|------|------|
| **推定学習時間** | 120-150分 |
| **難易度** | ★★★☆☆（中級） |
| **カテゴリ** | 基礎 |

**📋 学習目標**
- [ ] 特徴点検出器の種類と特性を理解できる
- [ ] SIFT/SURFの原理を説明できる
- [ ] ORB/AKAZEなどの高速特徴量を使用できる
- [ ] 特徴量記述子のマッチングができる
- [ ] RANSACによる外れ値除去を実装できる

**🎓 主な内容**
1. 特徴点検出の基礎
   - コーナー検出（Harris, Shi-Tomasi）
   - スケール空間とDoG（Difference of Gaussian）
   - 回転不変性とスケール不変性
2. 特徴量記述子
   - SIFT（Scale-Invariant Feature Transform）
   - SURF（Speeded Up Robust Features）
   - ORB（Oriented FAST and Rotated BRIEF）
   - AKAZE
3. 特徴量マッチング
   - ブルートフォースマッチング
   - FLANNベースマッチング
   - 距離比テスト（Lowe's ratio test）
   - クロスチェック
4. ロバストマッチング
   - RANSACアルゴリズム
   - ホモグラフィ推定によるフィルタリング
   - マッチング結果の可視化

**📊 実装内容**
```python
# SIFT/ORB/AKAZE特徴点の検出
# 特徴量記述子の計算
# ブルートフォースマッチング
# 距離比テストの適用
# RANSACによる外れ値除去
# マッチング結果の可視化
# 性能比較（精度、速度）
```

**🔗 実世界との接続**
- パノラマ画像の生成（画像スティッチング）
- オブジェクト認識と追跡
- 拡張現実（AR）のマーカーレストラッキング

---

#### Notebook 56: 三角測量と点群再構成
**ファイル名**: `notebooks/3d-vision/56_triangulation_point_cloud_v1.ipynb`

| 項目 | 内容 |
|------|------|
| **推定学習時間** | 120-150分 |
| **難易度** | ★★★★☆（上級） |
| **カテゴリ** | 実践 |

**📋 学習目標**
- [ ] 三角測量（triangulation）の原理を理解できる
- [ ] DLT（Direct Linear Transform）法を実装できる
- [ ] 2視点から3D点を復元できる
- [ ] 疎な点群（sparse point cloud）を生成できる
- [ ] 再構成誤差を評価できる

**🎓 主な内容**
1. 三角測量の原理
   - 2本の視線の交点として3D点を求める
   - 理想的なケースとノイズがある場合
   - 最小二乗法による最適化
2. DLT（Direct Linear Transform）
   - 線形方程式としての定式化
   - SVDによる解法
   - ホモグラフィ推定への応用
3. 多視点三角測量
   - 3視点以上からの観測
   - 再投影誤差の最小化
   - バンドル調整（Bundle Adjustment）の導入
4. 点群の生成と可視化
   - マッチング点の三角測量
   - 疎な3D点群の生成
   - Open3Dによる3D可視化
   - 点群の品質評価

**📊 実装内容**
```python
# 三角測量の実装（DLT法）
# 対応点から3D点の復元
# 再投影誤差の計算
# 疎な点群の生成
# 点群の色情報の付与
# Open3Dでの3D表示
# 異なる視点からの確認
```

**🔗 実世界との接続**
- フォトグラメトリ（写真測量）
- 3Dモデル再構成の基礎
- ARにおける環境マッピング

---

#### Notebook 57: Structure from Motion（SfM）パイプライン
**ファイル名**: `notebooks/3d-vision/57_structure_from_motion_pipeline_v1.ipynb`

| 項目 | 内容 |
|------|------|
| **推定学習時間** | 180-240分 |
| **難易度** | ★★★★★（最上級） |
| **カテゴリ** | 実践 |

**📋 学習目標**
- [ ] SfMの全体パイプラインを理解できる
- [ ] インクリメンタルSfMを実装できる
- [ ] バンドル調整（Bundle Adjustment）の概要を理解できる
- [ ] 複数画像からカメラ軌跡と3D構造を同時復元できる
- [ ] SfMの結果を評価・可視化できる

**🎓 主な内容**
1. SfMの概要
   - カメラの動きと3D構造の同時推定
   - インクリメンタルSfM vs グローバルSfM
   - SfMパイプラインのステップ
2. インクリメンタルSfM
   - 初期2視点の選択
   - 相対ポーズの推定
   - 初期点群の生成
   - 新しい視点の追加（PnP問題）
   - 点の追加と三角測量
3. バンドル調整
   - 再投影誤差の最小化
   - 非線形最適化（Levenberg-Marquardt）
   - SciPyによる簡易実装
   - Ceres Solverの紹介
4. 全体パイプラインの実装
   - 画像シーケンスの読み込み
   - 特徴点検出とマッチング
   - ポーズとポイントの逐次推定
   - バンドル調整の適用
   - カメラ軌跡と点群の可視化

**📊 実装内容**
```python
# 画像シーケンスの準備
# 全ペア画像のマッチング
# 初期2視点の選択と復元
# PnPによる新規カメラポーズ推定
# 新規3Dポイントの追加
# バンドル調整の実装
# カメラ軌跡の可視化
# 疎な点群の3D表示
# 精度評価（既知のデータセットを使用）
```

**🔗 実世界との接続**
- フォトグラメトリソフトウェア（Agisoft Metashape, COLMAP等）の原理
- ドローン測量
- 文化財のデジタルアーカイブ
- 映画のVFX（Visual Effects）

---

### Phase 4: NeRF/3DGSへの橋渡し（推定時間: 8-10時間）

#### Notebook 58: Ray Castingとボリュームレンダリング
**ファイル名**: `notebooks/3d-vision/58_ray_casting_volume_rendering_v1.ipynb`

| 項目 | 内容 |
|------|------|
| **推定学習時間** | 150-180分 |
| **難易度** | ★★★★☆（上級） |
| **カテゴリ** | 応用 |

**📋 学習目標**
- [ ] Ray Casting（光線投射）の原理を理解できる
- [ ] カメラから3D空間への光線を生成できる
- [ ] ボリュームレンダリングの基礎を理解できる
- [ ] 簡単なボクセルグリッドでのレンダリングを実装できる
- [ ] NeRFの座標サンプリングロジックを理解できる

**🎓 主な内容**
1. Ray Castingの基礎
   - ピンホールカメラからの光線生成
   - 光線の方向ベクトルの計算
   - カメラ座標系から世界座標系への変換
2. ボリュームレンダリング
   - ボクセルグリッド（Voxel Grid）
   - 光線上のサンプリング
   - アルファ合成（Alpha Compositing）
   - 色の累積計算
3. NeRFの基本構造
   - 5D入力（位置 x,y,z + 方向 θ,φ）
   - ボリューム密度 σ と色 c
   - ボリュームレンダリング方程式
   - 階層的サンプリング（Hierarchical Sampling）
4. 簡単な実装例
   - 球体のボリュームレンダリング
   - ガウス密度場のレンダリング
   - 複数視点からのレンダリング

**📊 実装内容**
```python
# カメラパラメータからの光線生成
# 光線上の点のサンプリング
# ボクセルグリッドの作成
# ボリュームレンダリングの実装
# 簡単な3Dシーンのレンダリング
# 異なる視点からのレンダリング結果
# NeRF風のサンプリング実装
```

**🔗 NeRF/3DGSへの接続**
- Neural Radiance Fields（NeRF）の基本原理
- ボリュームレンダリングとニューラルネットワークの組み合わせ
- 3D Gaussian Splattingの座標系理解

---

#### Notebook 59: 3D Vision から NeRF/3DGS への橋渡し
**ファイル名**: `notebooks/3d-vision/59_bridge_to_nerf_3dgs_v1.ipynb`

| 項目 | 内容 |
|------|------|
| **推定学習時間** | 120-150分 |
| **難易度** | ★★★★☆（上級） |
| **カテゴリ** | 応用 |

**📋 学習目標**
- [ ] 古典的3D CVと最新の3D生成技術の関係を理解できる
- [ ] NeRFの学習データ準備にSfMが使われる理由を説明できる
- [ ] 3DGSにおけるカメラパラメータの役割を理解できる
- [ ] カメラポーズ推定が3D生成の品質に与える影響を理解できる
- [ ] 次世代4D技術（NeoVerse等）の基礎を理解できる

**🎓 主な内容**
1. 古典的3D CVと深層学習ベース3D
   - 幾何学ベース vs 学習ベースのアプローチ
   - SfMとNeRFの補完関係
   - カメラポーズ：手動 vs COLMAP vs 学習
2. NeRFにおけるカメラパラメータ
   - NeRFの学習データ準備（COLMAPの役割）
   - カメラ内部・外部パラメータの正規化
   - カメラパラメータの誤差が及ぼす影響
   - Nerfstudioのデータフォーマット
3. 3D Gaussian Splattingの基礎
   - 点ベース表現 vs ボリューム表現
   - ガウシアンのパラメータ（位置、スケール、回転、色、不透明度）
   - ラスタライゼーションベースのレンダリング
   - カメラ内部・外部パラメータの利用
4. 4D技術への展望
   - 時間軸を含めた4D表現
   - NeoVerseなどの最新技術
   - 視点という主観と空間という客観の統合
5. 実践プロジェクト構想
   - 自分の部屋の3D再構成
   - 過去の動画からのデジタルゴースト生成
   - カメラパラメータのデバッグ手法

**📊 実装内容**
```python
# SfMで得られたカメラポーズの読み込み
# NeRF用データフォーマットへの変換
# transforms.jsonの生成
# カメラ可視化（Open3D）
# 簡単なガウシアンの可視化
# カメラパラメータの誤差シミュレーション
# 誤差が再構成に与える影響の可視化
```

**🔗 実世界との接続**
- NeRFを用いた新規視点合成
- 3DGSによる高速3D再構成
- デジタルツイン技術
- メタバース空間の構築
- AIによる世界モデル学習

---

## 📊 カリキュラム全体のまとめ

### 学習時間の見積もり

| Phase | ノートブック数 | 推定時間 | 累計時間 |
|-------|--------------|----------|----------|
| Phase 1: カメラモデルと射影幾何 | 3 | 6-8時間 | 6-8時間 |
| Phase 2: エピポーラ幾何とステレオ視 | 2 | 8-10時間 | 14-18時間 |
| Phase 3: Structure from Motion | 3 | 10-12時間 | 24-30時間 |
| Phase 4: NeRF/3DGSへの橋渡し | 2 | 8-10時間 | 32-40時間 |
| **合計** | **10** | **32-40時間** | **4-5週間** |

### 難易度分布

| 難易度 | ノートブック数 | 割合 |
|--------|--------------|------|
| ★★☆☆☆（初級） | 1 | 10% |
| ★★★☆☆（中級） | 4 | 40% |
| ★★★★☆（上級） | 4 | 40% |
| ★★★★★（最上級） | 1 | 10% |

---

## 🎯 学習成果

このカリキュラムを完了すると、以下ができるようになります：

### 理論的理解
- ✅ ピンホールカメラモデルと射影幾何の数学的基礎を理解している
- ✅ エピポーラ幾何と2視点幾何学を説明できる
- ✅ Structure from Motionのパイプラインを理解している
- ✅ NeRF/3DGSで使われるカメラパラメータの意味を理解している

### 実装スキル
- ✅ カメラキャリブレーションを実行できる
- ✅ ステレオ画像から深度マップを生成できる
- ✅ 特徴点マッチングとRANSACを実装できる
- ✅ 簡単なSfMパイプラインを実装できる
- ✅ Ray Castingとボリュームレンダリングを実装できる

### 応用力
- ✅ 実カメラの内部・外部パラメータを推定できる
- ✅ 複数画像から3D点群を復元できる
- ✅ NeRF/3DGSの学習データを準備できる
- ✅ カメラパラメータの誤差をデバッグできる

### 空間認識の拡張
- ✅ 2D画像と3D空間の対応関係を直感的に理解できる
- ✅ 視点という主観と空間という客観の関係を数学的に記述できる
- ✅ 最新の3D/4D技術の基礎原理を理解している

---

## 🛠️ 技術スタック

### 必須ライブラリ
```python
numpy>=1.24.0
opencv-python>=4.8.0
opencv-contrib-python>=4.8.0  # SIFT, SURF等
matplotlib>=3.7.0
scipy>=1.10.0
pillow>=9.5.0
open3d>=0.17.0  # 3D可視化
```

### オプショナルライブラリ
```python
pycolmap>=0.4.0  # COLMAP Python bindings
trimesh>=3.20.0  # メッシュ処理
plotly>=5.14.0  # インタラクティブ3D可視化
```

---

## 📚 推奨リソース

### 書籍
- **「コンピュータビジョン - 最先端ガイド5」** 金子邦彦ほか（アドコム・メディア）
- **「Multiple View Geometry in Computer Vision (2nd Edition)」** Richard Hartley, Andrew Zisserman
- **「Computer Vision: Algorithms and Applications (2nd Edition)」** Richard Szeliski
- **「3次元コンピュータビジョン計算ハンドブック」** 金谷健一

### 論文（古典的手法）
- **SfM**: "Photo Tourism: Exploring Photo Collections in 3D" - Snavely et al. (2006)
- **SIFT**: "Distinctive Image Features from Scale-Invariant Keypoints" - Lowe (2004)
- **8-point algorithm**: "In Defense of the Eight-Point Algorithm" - Hartley (1997)
- **Bundle Adjustment**: "Bundle Adjustment — A Modern Synthesis" - Triggs et al. (2000)

### 論文（最新手法への橋渡し）
- **NeRF**: "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis" - Mildenhall et al. (2020)
- **3D Gaussian Splatting**: "3D Gaussian Splatting for Real-Time Radiance Field Rendering" - Kerbl et al. (2023)
- **NeoVerse**: 最新の4D表現技術（2024-2025）

### オンラインリソース
- **OpenCV公式ドキュメント（Camera Calibration and 3D Reconstruction）**: https://docs.opencv.org/4.x/d9/db7/tutorial_py_table_of_contents_calib3d.html
- **COLMAP公式ドキュメント**: https://colmap.github.io/
- **Nerfstudio**: https://docs.nerf.studio/
- **Open3D Tutorial**: http://www.open3d.org/docs/release/tutorial/

### 動画・講義
- **CS231A: Computer Vision, From 3D Reconstruction to Recognition (Stanford)**
- **Multiple View Geometry - Lecture Series**
- **First Principles of Computer Vision (YouTube Channel)**

---

## ✅ 実装時の推奨順序

1. **Phase 1 を確実に理解** (Notebook 50-52)
   - カメラの基礎を完全に理解する
   - これが全ての3D処理の土台

2. **Phase 2 でステレオ視を体験** (Notebook 53-54)
   - エピポーラ幾何の直感を掴む
   - 深度推定の実装で3D感覚を養う

3. **Phase 3 でSfMを実装** (Notebook 55-57)
   - 特徴点マッチングから始める
   - 最終的に全パイプラインを統合

4. **Phase 4 で次世代技術へ接続** (Notebook 58-59)
   - NeRF/3DGSの基礎を理解
   - カメラパラメータの重要性を再認識

---

## 🌟 次のステップ

### カリキュラム完了後の発展

1. **NeRFの実装と学習**
   - PyTorch NeRF実装
   - Nerfstudioでの実験
   - Instant-NGPの高速化技術

2. **3D Gaussian Splattingの実装**
   - ガウシアンベース表現
   - 微分可能ラスタライゼーション
   - リアルタイム3D再構成

3. **SLAM（Simultaneous Localization and Mapping）**
   - Visual SLAM
   - ORB-SLAM3
   - リアルタイムカメラトラッキング

4. **密な3D再構成（Dense Reconstruction）**
   - Multi-View Stereo（MVS）
   - 点群からメッシュ生成
   - Poisson Surface Reconstruction

5. **4D動的シーン理解**
   - 動的NeRF（D-NeRF, HyperNeRF）
   - 時空間一貫性
   - NeoVerseなどの最新技術

---

## 💭 哲学的問いかけ

> [!question] **Unit 0.3 で問うべき深い問い**
>
> 1. **「視点」の不在**: 3Dモデル（3DGS）において、特定の視点に縛られない「普遍的な形」はどこに存在するのか？
>
> 2. **身体性の再現**: カメラの「歪み」を再現することは、デジタルゴーストに「肉体的な癖」を与えることになるか？
>
> 3. **情報の欠損**: 単眼（一つの目）で撮られた過去の記録から、AIが「裏側」を補完するとき、それは「事実」なのか「私の想像の拡張」なのか？
>
> 4. **主観と客観の境界**: カメラパラメータを操作することで、「自分がどう見ているか」と「世界がどうあるか」の境界はどこまで揺らぐのか？

---

## 🚀 プロジェクトアイデア

### 初級プロジェクト
1. **スマホカメラのキャリブレーション**
   - 自分のスマホのカメラパラメータを推定
   - 歪み補正前後の比較

2. **ステレオカメラの自作**
   - 2台のWebカメラでステレオシステム構築
   - リアルタイム深度マップ生成

### 中級プロジェクト
3. **部屋の3D再構成**
   - スマホで撮影した連続画像
   - SfMで疎な点群を生成
   - Open3Dで可視化

4. **パノラマ画像の生成**
   - 画像スティッチング
   - ホモグラフィ推定
   - 360度ビューの作成

### 上級プロジェクト
5. **NeRF学習データの準備**
   - COLMAPでカメラポーズ推定
   - Nerfstudioフォーマットへの変換
   - NeRFの学習と新規視点合成

6. **デジタルゴースト生成**
   - 自分の過去の動画からSfM
   - カメラ軌跡の可視化
   - 欠損視点の分析

---

## 📝 学習ノート：Obsidian テンプレート

```markdown
# Unit 0.3: 3D Computer Vision 基礎

## 📌 概要

2D画像は「3D世界の光の情報を、ある1点（ピンホール）を通して平面に押し込めたもの」である。

**逆問題としての3D復元**: 押し込められた情報を、幾何学的な制約（エピポーラ幾何など）を用いて再び3Dへ解き放つプロセス。

## 🔑 重要な数式

### 射影方程式
$$
\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = K [R | t] \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}
$$

- $K$: カメラ内部パラメータ（レンズの特性）
- $[R|t]$: 外部パラメータ（カメラの姿勢）
- $(X, Y, Z)$: 3D世界座標
- $(u, v)$: 2D画像座標

### エピポーラ制約
$$
x'^T F x = 0
$$

- $F$: 基礎行列（Fundamental Matrix）
- $x, x'$: 対応する2D点（異なる視点）

## 📊 Phase別まとめ

### Phase 1: カメラモデル
- カメラは3D→2Dの射影変換
- 内部パラメータ: レンズの特性
- 外部パラメータ: カメラの位置と向き

### Phase 2: 2視点幾何学
- エピポーラ制約: 対応点探索の効率化
- ステレオ視: 視差から深度を計算
- 三角測量: 2つの視線から3D点を復元

### Phase 3: 多視点幾何学
- SfM: カメラ軌跡と3D構造の同時推定
- 特徴点マッチング: 対応関係の確立
- バンドル調整: 全体の最適化

### Phase 4: 次世代への橋渡し
- Ray Casting: カメラから3D空間へ光線を飛ばす
- NeRF: 座標と方向から色と密度を予測
- 3DGS: ガウシアンベースの高速表現

## 🤔 自分なりの理解

【ここに自分の言葉でまとめる】

## 💡 つながりと洞察

【他の知識とのつながり、新たな気づき】

## 🔗 次に学ぶこと

【このユニットから次に展開できるトピック】
```

---

## 🎓 最後に

このカリキュラムは、**「空間の文法」**を習得するための旅です。

2D画像という「平面的なデータ」から、3D空間という「物理的な広がり」へと認識を拡張することで、あなたは：

- NeoVerseのような最新の4D技術を読み解く力を得ます
- 自分自身の「視覚」という主観的な体験を、数学的に客観化できるようになります
- デジタルゴーストや世界モデルといった、次世代のAI技術の基礎を理解します

この知識は、単なる技術ではなく、**認知の拡張**そのものです。

---

**Happy Learning! 🎓✨**

3D Computer Visionの世界へようこそ。この「空間の文法」があなたの認識を拡張することを願っています。
