#!/usr/bin/env python3
"""
残りのノートブック（03-12）に詳細な日本語の説明と具体例を追加するスクリプト
"""

import json
import os
from pathlib import Path

def create_markdown_cell(content):
    """マークダウンセルを作成"""
    return {
        "cell_type": "markdown",
        "metadata": {},
        "source": content.split('\n')
    }

# 各ノートブック用の詳細な説明コンテンツ（03-12）
ADDITIONAL_ENHANCEMENTS = {
    "03": {
        "intro": """## 📖 モデル評価指標：正しい測定なくして改善なし

「測定できないものは改善できない」という経営学の格言があります。
機械学習でも、モデルの性能を正しく測定することが成功の鍵となります。

### なぜ複数の評価指標が必要なのか？

「正解率が95%なら良いモデル」と思うかもしれませんが、必ずしもそうではありません。

**例：がん検診**
- データ: 1000人中10人ががん患者（1%）
- モデル: 全員に「健康」と予測
- 正解率: 99%！

しかし、このモデルはがん患者を一人も見つけられません。
正解率だけでは不十分なことが分かります。

### 医療診断のたとえ

医療診断では、以下の2つのエラーがあります：
- **偽陽性（False Positive）**: 健康な人をがんと診断（不必要な心配）
- **偽陰性（False Negative）**: がん患者を健康と診断（見逃し、命に関わる）

どちらのエラーがより深刻かは、状況によって異なります。
機械学習でも同じく、問題に応じて重視する指標を選ぶ必要があります。

### このノートブックで学ぶこと

- **混同行列（Confusion Matrix）**: 予測結果の詳細な内訳
- **適合率（Precision）**: 陽性と予測したもののうち、実際に陽性の割合
- **再現率（Recall）**: 実際の陽性のうち、正しく陽性と予測できた割合
- **F1スコア**: 適合率と再現率のバランス
- **ROC曲線とAUC**: モデルの総合的な性能評価

それでは、実際のデータを使って各指標を理解していきましょう！
""",
        "confusion_matrix": """### 🎯 混同行列（Confusion Matrix）を理解する

混同行列は、モデルの予測結果を詳細に分析するための基本的なツールです。

#### 混同行列の構造

2クラス分類の場合、混同行列は以下のような2×2の表になります：

```
                予測: 陰性    予測: 陽性
実際: 陰性      TN           FP
実際: 陽性      FN           TP
```

**各要素の意味**：

**TN (True Negative)**: 真陰性
- 実際に陰性で、陰性と予測した（正解）
- 例：健康な人を健康と診断

**FP (False Positive)**: 偽陽性（第1種の過誤）
- 実際は陰性なのに、陽性と予測した（誤り）
- 例：健康な人をがんと診断
- 別名：「狼少年のエラー」

**FN (False Negative)**: 偽陰性（第2種の過誤）
- 実際は陽性なのに、陰性と予測した（誤り）
- 例：がん患者を健康と診断
- 別名：「見逃しのエラー」

**TP (True Positive)**: 真陽性
- 実際に陽性で、陽性と予測した（正解）
- 例：がん患者をがんと診断

#### 具体例で理解する

**スパムメールフィルター**

1000通のメールを分類した結果：

```
                 予測: 正常    予測: スパム
実際: 正常メール    850           50      (TN=850, FP=50)
実際: スパムメール   10           90      (FN=10, TP=90)
```

**この結果から分かること**：
- 正常メール900通のうち、850通を正しく分類（94.4%）
- しかし、50通の正常メールがスパムと誤判定（5.6%）← 重要なメールを見逃す
- スパムメール100通のうち、90通を正しく検出（90%）
- しかし、10通のスパムを見逃した（10%）

#### 正解率だけでは見えないもの

```
正解率 = (TN + TP) / 全体
       = (850 + 90) / 1000
       = 94%
```

94%という数字は良く見えますが、混同行列を見ると：
- 正常メールの5.6%がスパムと誤判定されている
- スパムの10%を見逃している

このように、混同行列は正解率だけでは分からない
モデルの詳細な性能を明らかにします。

#### ビジネスへの影響

**FPが多い場合（偽陽性）**：
- 重要なメールが spam フォルダに行く
- 顧客満足度の低下
- ビジネスチャンスの損失

**FNが多い場合（偽陰性）**：
- スパムメールが inbox に届く
- ユーザーの時間の浪費
- フィッシング詐欺のリスク

どちらのエラーを重視すべきかは、ビジネス要件によって異なります。
""",
        "precision_recall": """### ⚖️ 適合率（Precision）と再現率（Recall）

適合率と再現率は、混同行列から計算される重要な指標です。

#### 適合率（Precision）：「予測の正確さ」

```
Precision = TP / (TP + FP)
```

「陽性と予測したもののうち、実際に陽性だった割合」

**質問**: 「このモデルが『陽性』と言ったら、どれくらい信頼できる？」

**例：スパムフィルター**
```
Precision = 90 / (90 + 50) = 90/140 = 64.3%
```

スパムと判定されたメールのうち、64.3%が本当にスパム。
つまり、35.7%は誤検出（正常メールがスパム扱い）。

#### 再現率（Recall）：「検出の網羅性」

```
Recall = TP / (TP + FN)
```

「実際の陽性のうち、正しく陽性と予測できた割合」

**質問**: 「実際の陽性を、どれだけ見つけられた？」

**例：スパムフィルター**
```
Recall = 90 / (90 + 10) = 90/100 = 90%
```

実際のスパムメールの90%を検出できた。
しかし、10%は見逃してしまった。

#### トレードオフの関係

適合率と再現率は、多くの場合トレードオフの関係にあります。

**例：スパムフィルターの閾値調整**

**厳しい判定（閾値を高く）**：
```
- より確実なものだけをスパムと判定
- 適合率: 高い（誤検出が少ない）
- 再現率: 低い（見逃しが多い）
```

**緩い判定（閾値を低く）**：
```
- 少しでも疑わしいものをスパムと判定
- 適合率: 低い（誤検出が多い）
- 再現率: 高い（見逃しが少ない）
```

#### どちらを重視すべきか？

**適合率を重視すべきケース**：

**1. 犯罪予測システム**
- FP（無実の人を容疑者扱い）は人権問題
- 慎重に、確実な証拠がある場合のみ陽性判定

**2. 高額商品の推薦システム**
- FP（興味のない商品を推薦）は顧客の不快感
- 確実に興味がありそうな商品のみ推薦

**3. 有料サービスの自動アップグレード**
- FP（不要なアップグレード）は顧客の不信感
- 本当に必要な人にのみ提案

**再現率を重視すべきケース**：

**1. がん検診**
- FN（がん患者の見逃し）は命に関わる
- 偽陽性（FP）は再検査で確認できる

**2. 不正取引検出**
- FN（不正の見逃し）は金銭的損失
- 誤検出（FP）は人間が最終確認

**3. 災害警報システム**
- FN（災害の見逃し）は人命に関わる
- 誤報（FP）は避難訓練と考える

#### 具体的な数値例

**医療診断（がん検診）**

ケース1: 適合率重視モデル
```
Precision = 95%  （陽性判定の95%が本当にがん）
Recall = 60%     （がん患者の60%しか検出できない）

問題: 40%のがん患者を見逃す → 危険！
```

ケース2: 再現率重視モデル
```
Precision = 20%  （陽性判定の80%は誤検出）
Recall = 98%     （がん患者の98%を検出）

問題: 多くの健康な人が精密検査を受ける → コスト増
       しかし、がんの見逃しは最小限 → 安全！
```

がん検診では、ケース2の方が適切です。
偽陽性は精密検査で確認できますが、
偽陰性（見逃し）は取り返しがつきません。

#### ビジネス上の意思決定

**コールセンターの顧客離反予測**

離反しそうな顧客を事前に検出し、リテンション施策を実施：

```
施策コスト: 1人あたり5,000円
顧客生涯価値: 1人あたり50,000円
```

**適合率重視（Precision = 80%, Recall = 40%）**：
```
予測: 100人が離反 → 80人が本当に離反、20人は誤検出
コスト: 100人 × 5,000円 = 50万円
効果: 80人 × 50,000円 = 400万円
利益: 350万円

ただし、実際の離反予定者200人のうち120人は見逃し
見逃した損失: 120人 × 50,000円 = 600万円
```

**再現率重視（Precision = 40%, Recall = 80%）**：
```
予測: 400人が離反 → 160人が本当に離反、240人は誤検出
コスト: 400人 × 5,000円 = 200万円
効果: 160人 × 50,000円 = 800万円
利益: 600万円

見逃した離反者: 40人
見逃した損失: 40人 × 50,000円 = 200万円
```

この場合、再現率重視の方が総合的な利益が大きくなります。
"""
    },
    "04": {
        "intro": """## 📖 線形モデル：シンプルだが強力な武器

「シンプルなものほど美しい」という言葉があります。
機械学習の世界でも、最もシンプルな線形モデルは、
多くの実務問題で驚くほど良い結果を出します。

### 線形モデルとは？

線形モデルは、入力（特徴量）と出力（予測値）の関係を
直線（または平面・超平面）で表すモデルです。

**1次元の例（y = ax + b）**：
```
住宅価格 = 面積 × 単価 + 基本価格
```

**多次元の例**：
```
住宅価格 = 面積×w1 + 築年数×w2 + 駅距離×w3 + ... + b
```

### なぜ線形モデルを学ぶのか？

**1. 解釈しやすい**：
- 各特徴量の重要度が数値（重み）で分かる
- ビジネスの意思決定に使いやすい
- 規制産業（金融、医療など）でも使える

**2. 計算が高速**：
- 学習が速い（数秒〜数分）
- 予測も高速（リアルタイム処理可能）
- 大規模データでも扱える

**3. 過学習しにくい**：
- パラメータ数が少ない
- 正則化で制御しやすい
- 小規模データでも使える

**4. ベースラインとして重要**：
- まず線形モデルを試すのが定石
- 複雑なモデルと比較する基準になる
- 「シンプルなモデルで十分」なことも多い

### 実世界での応用例

線形モデルは、以下のような場面で実際に使われています：

**1. 金融**：
- 信用スコアリング（ローン審査）
- 株価予測
- 詐欺検出

**2. マーケティング**：
- 売上予測
- 価格最適化
- 顧客生涯価値の推定

**3. 医療**：
- 疾患リスク予測
- 治療効果の予測
- 医療費予測

**4. 製造業**：
- 需要予測
- 品質予測
- 故障予測

### このノートブックで学ぶこと

- **線形回帰**: 連続値の予測（価格、売上など）
- **ロジスティック回帰**: 分類問題（Yes/No、カテゴリー）
- **正則化**: 過学習を防ぐ技術（Ridge、Lasso）
- **特徴量の重要度**: モデルの解釈方法

それでは、データを生成しながら線形モデルを学んでいきましょう！
""",
        "linear_regression": """### 📈 線形回帰：連続値を予測する

線形回帰は、機械学習の中で最も基本的で重要なアルゴリズムです。

#### 線形回帰の仕組み

**目的**: データに最もよくフィットする直線（または平面）を見つける

**1次元の場合**：
```
y = wx + b

y: 予測値（目的変数）
x: 入力値（説明変数）
w: 重み（傾き）
b: バイアス（切片）
```

**例：広告費と売上の関係**
```
売上 = 広告費 × 5.2 + 100

広告費が1万円増えると、売上が5.2万円増える
広告費ゼロでも、基本売上が100万円ある
```

#### 最小二乗法（Ordinary Least Squares）

線形回帰は、実際の値と予測値の差（残差）の
二乗和を最小にする直線を見つけます。

```
最小化: Σ(実際の値 - 予測値)²
```

**イメージ**：
```
データ点: ●  ●    ●  ●
          ●    ●  ●    ●
直線:     ─────────────────
          ↕    ↕  ↕    ↕
        残差（誤差）

すべての残差の二乗の和が最小になる直線を探す
```

#### 多次元の線形回帰

実世界の問題では、複数の特徴量を使います：

**住宅価格予測の例**：
```
価格 = 面積×w1 + 築年数×w2 + 駅距離×w3 + 階数×w4 + b

例:
価格 = 面積×50 + 築年数×(-30) + 駅距離×(-10) + 階数×20 + 1000

70㎡、築5年、駅5分、3階の物件:
価格 = 70×50 + 5×(-30) + 5×(-10) + 3×20 + 1000
     = 3500 - 150 - 50 + 60 + 1000
     = 4360万円
```

**重みの解釈**：
- w1 = 50: 面積が1㎡増えると、価格が50万円上がる
- w2 = -30: 築年数が1年増えると、価格が30万円下がる
- w3 = -10: 駅から1分遠いと、価格が10万円下がる
- w4 = 20: 階数が1階上がると、価格が20万円上がる

#### 線形回帰の仮定

線形回帰は、以下を仮定しています：

**1. 線形性**：
入力と出力が線形関係にある

**2. 独立性**：
各サンプルが独立している

**3. 正規性**：
誤差が正規分布に従う

**4. 等分散性**：
誤差の分散が一定

**5. 多重共線性がない**：
特徴量同士が強く相関していない

これらの仮定が満たされない場合、モデルの性能が低下します。

#### 線形回帰の利点と欠点

**利点**：
- 計算が速い（大規模データでも使える）
- 解釈しやすい（重みを見れば分かる）
- 実装が簡単
- 外挿（範囲外の予測）もある程度可能

**欠点**：
- 非線形関係を捉えられない
- 外れ値の影響を受けやすい
- 特徴量の前処理（スケーリングなど）が重要
- 特徴量間の相互作用を自動で考慮しない

#### 実務での使い方

**1. まずは試す**：
どんな問題でも、まず線形回帰を試すのが定石

**2. ベースラインとして使う**：
複雑なモデルと比較する基準になる

**3. 特徴量エンジニアリング**：
多項式特徴量や交互作用項を追加して非線形関係を捉える

```python
# 2次の項を追加
X_poly = [x, x²]

# 交互作用項を追加
X_interaction = [x1, x2, x1×x2]
```

**4. 正則化を使う**：
過学習を防ぐため、Ridge や Lasso を使う
""",
        "logistic_regression": """### 🎲 ロジスティック回帰：分類問題への応用

名前に「回帰」とありますが、ロジスティック回帰は**分類**アルゴリズムです。

#### なぜ線形回帰では分類できないのか？

**例：スパムメール判定**

線形回帰で試すと：
```
スコア = 単語頻度1×w1 + 単語頻度2×w2 + ...

結果: -5.2, 0.3, 1.8, 25.7, ...

問題: これをどうやって「スパム or 正常」に変換する？
```

出力が連続値（-∞ to +∞）なので、
0 or 1 の分類に直接使えません。

#### シグモイド関数による変換

ロジスティック回帰は、線形回帰の出力を
**シグモイド関数**で0〜1の範囲に変換します：

```
P(y=1) = 1 / (1 + e^(-z))

z = wx + b (線形回帰と同じ)
```

**シグモイド関数の特徴**：
```
入力(z)    出力P(y=1)
-∞    →    0
-2    →    0.12
0     →    0.50
+2    →    0.88
+∞    →    1.00

どんな入力でも、出力は0〜1の範囲に収まる
これを確率として解釈できる！
```

#### 確率から分類へ

```
P(スパム) = 0.85 → スパムと判定
P(スパム) = 0.23 → 正常メールと判定

閾値（デフォルト: 0.5）を超えたら陽性と判定
```

#### ロジスティック回帰の具体例

**顧客の購入予測**：

```
購入確率 = sigmoid(年齢×w1 + 収入×w2 + 訪問回数×w3 + b)

例:
年齢30歳、収入500万円、訪問回数10回の顧客

z = 30×0.02 + 500×0.001 + 10×0.3 + (-2)
  = 0.6 + 0.5 + 3 - 2
  = 2.1

P(購入) = 1 / (1 + e^(-2.1))
        = 1 / (1 + 0.122)
        = 0.89 (89%)

→ 購入する可能性が高い！
```

#### 多クラス分類

ロジスティック回帰は、2クラスだけでなく多クラスにも対応できます。

**ソフトマックス回帰（Softmax Regression）**：

```
3クラス分類（A, B, C）の例:

P(A) = e^(z_A) / (e^(z_A) + e^(z_B) + e^(z_C))
P(B) = e^(z_B) / (e^(z_A) + e^(z_B) + e^(z_C))
P(C) = e^(z_C) / (e^(z_A) + e^(z_B) + e^(z_C))

P(A) + P(B) + P(C) = 1.0 （確率の合計は1）

例:
P(A) = 0.7
P(B) = 0.2
P(C) = 0.1

→ クラスAと予測
```

#### 決定境界の可視化

ロジスティック回帰の決定境界は直線（または平面）になります：

```
2次元空間での境界:

  特徴量2
    ↑
    │  ●●●     ○○○
    │  ●●●  ／ ○○○
    │  ●●● ／  ○○○
    │      ／
    │  ●●／●
    │  ●／●●
    └─────────→ 特徴量1

境界線の左側: クラス●
境界線の右側: クラス○
```

#### ロジスティック回帰 vs 他の分類器

**線形SVM**：
- 境界線からの「マージン」を最大化
- 外れ値に強い
- 確率を直接出力しない

**決定木**：
- 非線形な境界を作れる
- 特徴量間の相互作用を捉える
- 解釈しやすい（ルールが見える）

**ニューラルネットワーク**：
- 複雑な非線形関係を学習
- 大量のデータが必要
- 計算コストが高い

**ロジスティック回帰の強み**：
- 計算が速い
- 確率を直接出力（意思決定に使いやすい）
- 正則化で過学習を防ぎやすい
- 大規模データでも使える

#### 実務での応用例

**1. クレジットカード不正検出**：
```
P(不正) = sigmoid(金額×w1 + 時刻×w2 + 場所×w3 + ...)

P(不正) > 0.9 → 取引を停止
P(不正) = 0.5-0.9 → 追加認証を要求
P(不正) < 0.5 → 取引を承認
```

**2. メールの優先度判定**：
```
P(重要) = sigmoid(送信者×w1 + 件名×w2 + ...)

P(重要) > 0.8 → 通知を送る
P(重要) < 0.8 → 通知しない
```

**3. 病気の診断補助**：
```
P(疾患) = sigmoid(症状1×w1 + 症状2×w2 + ...)

P(疾患) > 0.7 → 精密検査を推奨
P(疾患) < 0.7 → 経過観察
```

確率が出力されるので、閾値を調整して
リスクとコストのバランスを取れます。
""",
    },
}

def enhance_notebook_with_explanations(nb_path, enhancements):
    """ノートブックに詳細な説明を追加（より積極的に）"""
    print(f"\nProcessing: {nb_path}")

    with open(nb_path, 'r', encoding='utf-8') as f:
        nb = json.load(f)

    new_cells = []
    intro_added = False

    for i, cell in enumerate(nb['cells']):
        # イントロダクションを最初のセルの後に追加
        if i == 1 and not intro_added and 'intro' in enhancements:
            new_cells.append(create_markdown_cell(enhancements['intro']))
            intro_added = True

        # 元のセルを追加
        new_cells.append(cell)

        # コードセルの後に説明を追加
        if cell['cell_type'] == 'code' and cell.get('source'):
            source_text = ''.join(cell['source']).lower()

            # キーワードマッチングで適切な説明を追加
            for key, content in enhancements.items():
                if key == 'intro':
                    continue

                # キーワード検出
                keywords = {
                    'confusion_matrix': ['confusion_matrix', 'confusionmatrix'],
                    'precision_recall': ['precision', 'recall', 'f1'],
                    'linear_regression': ['linearregression', 'linear_model', 'fit'],
                    'logistic_regression': ['logisticregression', 'logistic', 'classifier'],
                }

                if key in keywords:
                    if any(kw in source_text for kw in keywords[key]):
                        # 説明セルを追加（重複を避けるため、チェック）
                        if i + 1 < len(new_cells):
                            next_cell = new_cells[-1]
                            if next_cell['cell_type'] != 'markdown' or key not in ''.join(next_cell.get('source', [])):
                                new_cells.append(create_markdown_cell(content))

    nb['cells'] = new_cells

    with open(nb_path, 'w', encoding='utf-8') as f:
        json.dump(nb, f, ensure_ascii=False, indent=1)

    print(f"✅ Enhanced: {nb_path} ({len(new_cells)} cells)")

def main():
    """メイン処理"""
    print("=" * 70)
    print("📚 ノートブック03-04に詳細な説明を追加")
    print("=" * 70)

    notebooks_dir = Path("/home/user/machine-learning-playground/notebooks")

    # 処理対象のノートブック
    target_notebooks = {
        "03": notebooks_dir / "03_model_evaluation_metrics_improved_v2.ipynb",
        "04": notebooks_dir / "04_linear_models_simulation_improved_v2.ipynb",
    }

    for nb_num, nb_path in target_notebooks.items():
        if nb_path.exists() and nb_num in ADDITIONAL_ENHANCEMENTS:
            enhance_notebook_with_explanations(nb_path, ADDITIONAL_ENHANCEMENTS[nb_num])

    print("\n" + "=" * 70)
    print("✅ 完了！")
    print("=" * 70)

if __name__ == "__main__":
    main()
