#!/usr/bin/env python3
"""Create improved version of 07_mlp_fundamentals.ipynb with all framework elements."""

import json
import copy
from pathlib import Path

def create_markdown_cell(content):
    """Create a markdown cell."""
    return {
        "cell_type": "markdown",
        "metadata": {},
        "source": content if isinstance(content, list) else [content]
    }

def create_code_cell(code):
    """Create a code cell."""
    return {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": code if isinstance(code, list) else [code]
    }

# Load original notebook
nb_path = Path("notebooks/07_mlp_fundamentals.ipynb")
with open(nb_path, 'r', encoding='utf-8') as f:
    nb = json.load(f)

# Create improved notebook
improved_nb = copy.deepcopy(nb)
improved_nb['cells'] = []

# ==== 1. IMPROVED TITLE AND LEARNING OBJECTIVES ====
improved_nb['cells'].append(create_markdown_cell([
    "# ç¬¬7ç« : MLPï¼ˆå¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼‰ã®åŸºç¤\n",
    "\n",
    "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŸºæœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç†è§£ã—ã€ç›´æ„Ÿã‚’é¤Šã„ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“‹ ã“ã®ç« ã§å­¦ã¶ã“ã¨\n",
    "\n",
    "ã“ã®ç« ã‚’çµ‚ãˆã‚‹ã¨ã€ä»¥ä¸‹ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š\n",
    "\n",
    "- [ ] MLPã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆå…¥åŠ›å±¤ã€éš ã‚Œå±¤ã€å‡ºåŠ›å±¤ï¼‰ã‚’ç†è§£ã—èª¬æ˜ã§ãã‚‹\n",
    "- [ ] æ´»æ€§åŒ–é–¢æ•°ï¼ˆReLUã€Sigmoidã€Tanhï¼‰ã®å½¹å‰²ã¨ç‰¹æ€§ã‚’ç†è§£ã§ãã‚‹\n",
    "- [ ] scikit-learnã‚’ä½¿ã£ã¦MLPã®åˆ†é¡å™¨ã¨å›å¸°å™¨ã‚’è¨“ç·´ã§ãã‚‹\n",
    "- [ ] æå¤±æ›²ç·šã‚’èª­ã¿å–ã‚Šã€åæŸã‚’åˆ¤æ–­ã§ãã‚‹\n",
    "- [ ] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå±¤æ•°ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã€å­¦ç¿’ç‡ï¼‰ã®å½±éŸ¿ã‚’ç†è§£ã§ãã‚‹\n",
    "- [ ] Early Stoppingã‚’ä½¿ã£ã¦éå­¦ç¿’ã‚’é˜²ã’ã‚‹\n",
    "\n",
    "## ğŸ¯ å‰æçŸ¥è­˜\n",
    "\n",
    "ã“ã®ç« ã‚’å­¦ã¶ã«ã¯ä»¥ä¸‹ã®çŸ¥è­˜ãŒå¿…è¦ã§ã™ï¼š\n",
    "\n",
    "- âœ… PythonåŸºç¤ï¼ˆé–¢æ•°ã€ãƒ«ãƒ¼ãƒ—ã€ãƒªã‚¹ãƒˆï¼‰\n",
    "- âœ… NumPyåŸºç¤ï¼ˆé…åˆ—æ“ä½œã€åŸºæœ¬æ¼”ç®—ï¼‰â† Notebook 01ã§å­¦ç¿’\n",
    "- âœ… ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆStandardScalerï¼‰â† Notebook 02ã§å­¦ç¿’\n",
    "- âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™ï¼ˆæ­£è§£ç‡ã€RMSEã€RÂ²ï¼‰â† Notebook 03ã§å­¦ç¿’\n",
    "- âœ… æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µï¼ˆè¨“ç·´ã‚»ãƒƒãƒˆã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã€éå­¦ç¿’ï¼‰â† Notebook 04-06ã§å­¦ç¿’\n",
    "\n",
    "**å¿…é ˆã§ã¯ãªã„ãŒå½¹ç«‹ã¤çŸ¥è­˜ï¼š**\n",
    "- ç·šå½¢ä»£æ•°ã®åŸºç¤ï¼ˆè¡Œåˆ—ã®æ›ã‘ç®—ï¼‰\n",
    "- å¾®åˆ†ã®åŸºæœ¬ï¼ˆå°é–¢æ•°ã®æ¦‚å¿µï¼‰\n",
    "\n",
    "â±ï¸ **æ¨å®šå­¦ç¿’æ™‚é–“**: 90-120åˆ†  \n",
    "ğŸ“Š **é›£æ˜“åº¦**: â˜…â˜…â˜…â˜†â˜†ï¼ˆä¸­ç´šï¼‰  \n",
    "ğŸ“ **ã‚«ãƒ†ã‚´ãƒª**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "\n",
    "---\n"
]))

# ==== 2. MOTIVATION SECTION ====
improved_nb['cells'].append(create_markdown_cell([
    "## ğŸ’¡ ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ï¼šãªãœMLPã‚’å­¦ã¶ã®ã‹ï¼Ÿ\n",
    "\n",
    "### ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "\n",
    "**Q: ãªãœãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒé‡è¦ãªã®ã‹ï¼Ÿ**\n",
    "\n",
    "1. **ä¸‡èƒ½è¿‘ä¼¼å®šç†**: MLPã¯ç†è«–çš„ã«ä»»æ„ã®é€£ç¶šé–¢æ•°ã‚’è¿‘ä¼¼ã§ãã‚‹\n",
    "2. **å®Ÿä¸–ç•Œã§ã®æˆåŠŸ**: ç”»åƒèªè­˜ã€éŸ³å£°èªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ã§æœ€å…ˆç«¯ã®æ€§èƒ½\n",
    "3. **Deep Learningã®åŸºç¤**: ã‚ˆã‚Šé«˜åº¦ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNã€RNNï¼‰ã®åŸºç›¤\n",
    "\n",
    "### ğŸŒ å®Ÿä¸–ç•Œã®å¿œç”¨ä¾‹\n",
    "\n",
    "- **åŒ»ç™‚è¨ºæ–­**: MRIã‚¹ã‚­ãƒ£ãƒ³ã‹ã‚‰ç—…æ°—ã‚’æ¤œå‡º\n",
    "- **é‡‘è**: æ ªä¾¡äºˆæ¸¬ã€ä¸æ­£æ¤œå‡º\n",
    "- **è£½é€ æ¥­**: è£½å“ã®ä¸è‰¯å“æ¤œå‡º\n",
    "- **ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°**: é¡§å®¢ã®è¡Œå‹•äºˆæ¸¬\n",
    "\n",
    "### ã“ã®ç« ã§ä½œã‚‹ã‚‚ã®\n",
    "\n",
    "- æ§˜ã€…ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®MLPã‚’è¨“ç·´\n",
    "- æ±ºå®šå¢ƒç•Œã‚’å¯è¦–åŒ–ã—ã¦ç›´æ„Ÿã‚’é¤Šã†\n",
    "- æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã‚‹æ–¹æ³•ã‚’å­¦ã¶\n",
    "\n",
    "---\n"
]))

# ==== 3. ORIGINAL IMPORTS ====
improved_nb['cells'].append(nb['cells'][1])  # Import cell

# ==== 4. PART 1: ACTIVATION FUNCTIONS ====
improved_nb['cells'].append(nb['cells'][2])  # Part 1 title
improved_nb['cells'].append(nb['cells'][3])  # Activation functions code
improved_nb['cells'].append(nb['cells'][4])  # Visualization
improved_nb['cells'].append(nb['cells'][5])  # Comparison plot

# Add explanation after activation functions
improved_nb['cells'].append(create_markdown_cell([
    "### ğŸ’¡ æ´»æ€§åŒ–é–¢æ•°ã®é¸ã³æ–¹\n",
    "\n",
    "| æ´»æ€§åŒ–é–¢æ•° | ã„ã¤ä½¿ã†ï¼Ÿ | ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |\n",
    "|-----------|----------|---------|----------|\n",
    "| **ReLU** | ğŸ‘ **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆã»ã¨ã‚“ã©ã®å ´åˆï¼‰** | è¨ˆç®—ãŒé€Ÿã„ã€å‹¾é…æ¶ˆå¤±å•é¡Œãªã— | Dead neuronsï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒæ­»ã¬ï¼‰|\n",
    "| **Tanh** | ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆ | å‡ºåŠ›ãŒ-1ï½1ã§ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ | å‹¾é…æ¶ˆå¤±å•é¡Œã‚ã‚Š |\n",
    "| **Sigmoid** | å‡ºåŠ›å±¤ï¼ˆäºŒå€¤åˆ†é¡ã®ç¢ºç‡ï¼‰ | 0ï½1ã®ç¢ºç‡ã¨ã—ã¦è§£é‡ˆå¯èƒ½ | å‹¾é…æ¶ˆå¤±å•é¡Œã‚ã‚Š |\n",
    "| **Leaky ReLU** | ReLUã§Dead neuronsãŒå•é¡Œã®æ™‚ | Dead neuronsã‚’é˜²ã | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒå¿…è¦ |\n",
    "\n",
    "**ğŸ‘ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: è¿·ã£ãŸã‚‰ReLUã‚’ä½¿ã†ï¼\n",
    "\n",
    "---\n"
]))

# ==== 5. PART 2: MLP ARCHITECTURE ====
improved_nb['cells'].append(nb['cells'][6])  # Part 2 title
improved_nb['cells'].append(nb['cells'][7])  # Data generation
improved_nb['cells'].append(nb['cells'][8])  # Train simple MLP

# Add common errors section
improved_nb['cells'].append(create_markdown_cell([
    "### âš ï¸ ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ #1: ConvergenceWarning\n",
    "\n",
    "ä¸Šã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªè­¦å‘ŠãŒå‡ºã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼š\n",
    "\n",
    "```\n",
    "ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached \n",
    "and the optimization hasn't converged yet.\n",
    "```\n",
    "\n",
    "**åŸå› :**\n",
    "- ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ï¼ˆ`max_iter`ï¼‰ãŒä¸è¶³\n",
    "- å­¦ç¿’ç‡ãŒä¸é©åˆ‡ï¼ˆå¤§ãã™ãã‚‹ or å°ã•ã™ãã‚‹ï¼‰\n",
    "- ãƒ‡ãƒ¼ã‚¿ãŒã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„\n",
    "\n",
    "**âœ… è§£æ±ºæ³•:**\n",
    "\n",
    "```python\n",
    "# âŒ å•é¡Œã‚ã‚Š\n",
    "mlp = MLPClassifier(max_iter=100)  # å°‘ãªã™ãã‚‹\n",
    "\n",
    "# âœ… æ”¹å–„ç­–1: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã‚’å¢—ã‚„ã™\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# âœ… æ”¹å–„ç­–2: Early Stoppingã‚’ä½¿ã†ï¼ˆæ¨å¥¨ï¼‰\n",
    "mlp = MLPClassifier(max_iter=1000, early_stopping=True)\n",
    "\n",
    "# âœ… æ”¹å–„ç­–3: ãƒ‡ãƒ¼ã‚¿ã‚’å¿…ãšã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n"
]))

improved_nb['cells'].append(nb['cells'][9])  # Weight examination

# ==== 6. PARTS 3-9: KEEP ORIGINAL CONTENT ====
for i in range(10, 26):
    improved_nb['cells'].append(nb['cells'][i])

# Add another common error section after Part 6
improved_nb['cells'].append(create_markdown_cell([
    "### âš ï¸ ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ #2: ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—å¿˜ã‚Œã‚‹\n",
    "\n",
    "**å•é¡Œã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰:**\n",
    "\n",
    "```python\n",
    "# âŒ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã—\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)  # X_trainãŒç”Ÿãƒ‡ãƒ¼ã‚¿\n",
    "```\n",
    "\n",
    "**çµæœ:**\n",
    "- å­¦ç¿’ãŒé…ã„ã€ã¾ãŸã¯å…¨ãåæŸã—ãªã„\n",
    "- ç²¾åº¦ãŒä½ã„\n",
    "- æå¤±æ›²ç·šãŒä¸å®‰å®š\n",
    "\n",
    "**âœ… æ­£ã—ã„ã‚³ãƒ¼ãƒ‰:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # æ³¨æ„: fit_transformã§ã¯ãªãtransform\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "**é‡è¦**: ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«ã¯`transform()`ã®ã¿ä½¿ç”¨ï¼ˆ`fit_transform()`ã§ã¯ãªã„ï¼‰ï¼\n",
    "\n",
    "---\n"
]))

# ==== 7. PART 10: PRACTICAL TIPS ====
improved_nb['cells'].append(nb['cells'][26])  # Part 10 title
improved_nb['cells'].append(nb['cells'][27])  # Practical tips

# Add enhanced best practices
improved_nb['cells'].append(create_markdown_cell([
    "### ğŸ’¡ å®Ÿè·µçš„ãªãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "\n",
    "#### ğŸ¯ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã®æŒ‡é‡\n",
    "\n",
    "**é–‹å§‹ç‚¹:**\n",
    "1. **å°ã•ãå§‹ã‚ã‚‹**: `hidden_layer_sizes=(50,)` ã‹ã‚‰é–‹å§‹\n",
    "2. **å¿…è¦ã«å¿œã˜ã¦æ‹¡å¤§**: ç²¾åº¦ãŒä¸ååˆ†ãªã‚‰å±¤ã‚„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’è¿½åŠ \n",
    "3. **ãƒ”ãƒ©ãƒŸãƒƒãƒ‰æ§‹é€ **: `(100, 50, 25)` ã®ã‚ˆã†ã«æ¸›å°‘ã•ã›ã‚‹\n",
    "\n",
    "**å±¤æ•°ã®é¸ã³æ–¹:**\n",
    "- **1å±¤**: å˜ç´”ãªéç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "- **2-3å±¤**: ã»ã¨ã‚“ã©ã®å®Ÿç”¨çš„ãªå•é¡Œ\n",
    "- **4å±¤ä»¥ä¸Š**: è¤‡é›‘ãªå•é¡Œï¼ˆãŸã ã—ã€scikit-learnã‚ˆã‚Šæ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æ¤œè¨ï¼‰\n",
    "\n",
    "#### âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å¥¨å€¤\n",
    "\n",
    "```python\n",
    "# æ¨å¥¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),      # 2å±¤ã€ãƒ”ãƒ©ãƒŸãƒƒãƒ‰æ§‹é€ \n",
    "    activation='relu',                 # ReLUãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ\n",
    "    solver='adam',                     # AdamãŒæœ€ã‚‚æ±ç”¨çš„\n",
    "    alpha=0.0001,                      # L2æ­£å‰‡åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰\n",
    "    learning_rate_init=0.001,          # Adamã®æ¨å¥¨å­¦ç¿’ç‡\n",
    "    max_iter=1000,                     # ååˆ†ãªã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "    early_stopping=True,               # éå­¦ç¿’é˜²æ­¢\n",
    "    validation_fraction=0.1,           # æ¤œè¨¼ç”¨ã«10%\n",
    "    n_iter_no_change=10,               # 10å›æ”¹å–„ãªã—ã§åœæ­¢\n",
    "    random_state=42                    # å†ç¾æ€§ã®ãŸã‚\n",
    ")\n",
    "```\n",
    "\n",
    "#### ğŸ” ãƒ‡ãƒãƒƒã‚°ã®ã‚³ãƒ„\n",
    "\n",
    "**å­¦ç¿’ãŒã†ã¾ãã„ã‹ãªã„æ™‚ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ:**\n",
    "\n",
    "1. âœ… ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãŸã‹ï¼Ÿ\n",
    "2. âœ… è¨“ç·´ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§åŒã˜scalerã‚’ä½¿ã£ã¦ã„ã‚‹ã‹ï¼Ÿ\n",
    "3. âœ… `max_iter`ã¯ååˆ†ã‹ï¼Ÿ\n",
    "4. âœ… æå¤±æ›²ç·šã‚’ç¢ºèªã—ãŸã‹ï¼Ÿï¼ˆæŒ¯å‹•ï¼ŸåæŸï¼Ÿï¼‰\n",
    "5. âœ… ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡ã¯ãªã„ã‹ï¼Ÿ\n",
    "\n",
    "**æå¤±æ›²ç·šã®èª­ã¿æ–¹:**\n",
    "- **å³è‚©ä¸‹ãŒã‚Š**: âœ… æ­£å¸¸ã«å­¦ç¿’ä¸­\n",
    "- **å¹³å¦**: åæŸã—ãŸ or å­¦ç¿’ç‡ãŒå°ã•ã™ãã‚‹\n",
    "- **æŒ¯å‹•**: å­¦ç¿’ç‡ãŒå¤§ãã™ãã‚‹\n",
    "- **ä¸Šæ˜‡**: å•é¡Œã‚ã‚Šï¼å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹\n",
    "\n",
    "---\n"
]))

# ==== 8. SUMMARY (keep original) ====
improved_nb['cells'].append(nb['cells'][28])  # Summary

# ==== 9. SELF-ASSESSMENT QUIZ ====
improved_nb['cells'].append(create_markdown_cell([
    "---\n",
    "\n",
    "## ğŸ“ è‡ªå·±è©•ä¾¡ã‚¯ã‚¤ã‚º\n",
    "\n",
    "å­¦ç¿’å†…å®¹ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ï¼\n",
    "\n",
    "### Q1: MLPã§æœ€ã‚‚é‡è¦ãªå‰å‡¦ç†ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ ç­”ãˆã‚’è¦‹ã‚‹</summary>\n",
    "\n",
    "**ç­”ãˆ**: ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆStandardScalerãªã©ï¼‰\n",
    "\n",
    "**ç†ç”±**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«éå¸¸ã«æ•æ„Ÿã§ã™ã€‚ã‚¹ã‚±ãƒ¼ãƒ«ãŒç•°ãªã‚‹ç‰¹å¾´é‡ãŒã‚ã‚‹ã¨ï¼š\n",
    "- å­¦ç¿’ãŒé…ããªã‚‹\n",
    "- åæŸã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹\n",
    "- ä¸€éƒ¨ã®ç‰¹å¾´é‡ãŒéåº¦ã«é‡è¦–ã•ã‚Œã‚‹\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: ReLUã®ä¸»ãªåˆ©ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ ç­”ãˆã‚’è¦‹ã‚‹</summary>\n",
    "\n",
    "**ç­”ãˆ**: \n",
    "1. è¨ˆç®—ãŒé«˜é€Ÿï¼ˆmax(0, x)ã ã‘ï¼‰\n",
    "2. å‹¾é…æ¶ˆå¤±å•é¡ŒãŒãªã„ï¼ˆx > 0ã®å ´åˆã€å‹¾é…=1ï¼‰\n",
    "3. å®Ÿè·µçš„ã«å„ªã‚ŒãŸæ€§èƒ½\n",
    "\n",
    "**æ¬ ç‚¹**: Dead neuronsï¼ˆä¸€éƒ¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå¸¸ã«0ã‚’å‡ºåŠ›ã—ã€å­¦ç¿’ã—ãªããªã‚‹ï¼‰\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: Early Stoppingã¨ã¯ä½•ã§ã™ã‹ï¼Ÿãªãœä½¿ã†ã®ã§ã™ã‹ï¼Ÿ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ ç­”ãˆã‚’è¦‹ã‚‹</summary>\n",
    "\n",
    "**ç­”ãˆ**: æ¤œè¨¼ã‚»ãƒƒãƒˆã®æ€§èƒ½ãŒæ”¹å–„ã—ãªããªã£ãŸã‚‰è¨“ç·´ã‚’æ—©æœŸã«åœæ­¢ã™ã‚‹æ‰‹æ³•ã€‚\n",
    "\n",
    "**ä½¿ã†ç†ç”±**:\n",
    "- éå­¦ç¿’ã‚’é˜²ã\n",
    "- è¨“ç·´æ™‚é–“ã‚’ç¯€ç´„\n",
    "- æœ€é©ãªã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã‚’è‡ªå‹•ã§è¦‹ã¤ã‘ã‚‹\n",
    "\n",
    "**ä½¿ã„æ–¹**:\n",
    "```python\n",
    "mlp = MLPClassifier(early_stopping=True, validation_fraction=0.1, n_iter_no_change=10)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q4: `hidden_layer_sizes=(100, 50)` ã¯ä½•ã‚’æ„å‘³ã—ã¾ã™ã‹ï¼Ÿ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ ç­”ãˆã‚’è¦‹ã‚‹</summary>\n",
    "\n",
    "**ç­”ãˆ**: \n",
    "- ç¬¬1éš ã‚Œå±¤: 100å€‹ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³\n",
    "- ç¬¬2éš ã‚Œå±¤: 50å€‹ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³\n",
    "- åˆè¨ˆ2ã¤ã®éš ã‚Œå±¤\n",
    "\n",
    "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“:\n",
    "```\n",
    "å…¥åŠ›å±¤(n_features) â†’ éš ã‚Œå±¤1(100) â†’ éš ã‚Œå±¤2(50) â†’ å‡ºåŠ›å±¤(n_classes)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q5: æå¤±æ›²ç·šãŒæŒ¯å‹•ã—ã¦ã„ã‚‹å ´åˆã€ã©ã†ã™ã¹ãã§ã™ã‹ï¼Ÿ\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ ç­”ãˆã‚’è¦‹ã‚‹</summary>\n",
    "\n",
    "**ç­”ãˆ**: å­¦ç¿’ç‡ï¼ˆ`learning_rate_init`ï¼‰ã‚’ä¸‹ã’ã‚‹\n",
    "\n",
    "**å¯¾å‡¦æ³•**:\n",
    "```python\n",
    "# æŒ¯å‹•ã—ã¦ã„ã‚‹å ´åˆ\n",
    "mlp = MLPClassifier(learning_rate_init=0.0001)  # 0.001ã‹ã‚‰0.0001ã«ä¸‹ã’ã‚‹\n",
    "```\n",
    "\n",
    "**é€†ã«**ã€åæŸãŒé…ã™ãã‚‹å ´åˆã¯å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹ã€‚\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n"
]))

# ==== 10. EXERCISES ====
improved_nb['cells'].append(create_markdown_cell([
    "## ğŸ‹ï¸ æ¼”ç¿’å•é¡Œ\n",
    "\n",
    "å®Ÿéš›ã«æ‰‹ã‚’å‹•ã‹ã—ã¦ç†è§£ã‚’æ·±ã‚ã¾ã—ã‚‡ã†ï¼\n",
    "\n",
    "---\n"
]))

improved_nb['cells'].append(create_markdown_cell([
    "### ğŸ“ åŸºç¤å•é¡Œ1: Circles ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§MLPã‚’è¨“ç·´\n",
    "\n",
    "**èª²é¡Œ**: `make_circles` ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§MLPã‚’è¨“ç·´ã—ã€ãƒ†ã‚¹ãƒˆç²¾åº¦0.90ä»¥ä¸Šã‚’é”æˆã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ ãƒ’ãƒ³ãƒˆ1</summary>\n",
    "\n",
    "- `make_circles`ã¯2ã¤ã®åŒå¿ƒå††ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "- ç·šå½¢åˆ†é›¢ä¸å¯èƒ½ãªã®ã§ã€éš ã‚Œå±¤ãŒå¿…è¦\n",
    "- ã¾ãš`hidden_layer_sizes=(10,)`ã‹ã‚‰è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ ãƒ’ãƒ³ãƒˆ2</summary>\n",
    "\n",
    "- ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å¿˜ã‚Œãšã«ï¼\n",
    "- `noise=0.1`ã§ç”Ÿæˆã™ã‚‹ã¨é©åº¦ãªé›£æ˜“åº¦ã«ãªã‚Šã¾ã™\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>âœ… è§£ç­”ä¾‹</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=42)\n",
    "\n",
    "# åˆ†å‰²\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# MLPè¨“ç·´\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(20, 10),\n",
    "    activation='relu',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# è©•ä¾¡\n",
    "print(f\"Test Accuracy: {mlp.score(X_test_scaled, y_test):.4f}\")\n",
    "# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›: Test Accuracy: 0.9700ä»¥ä¸Š\n",
    "```\n",
    "\n",
    "</details>\n"
]))

improved_nb['cells'].append(create_code_cell([
    "# ã“ã“ã«ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„\n",
    "\n"
]))

improved_nb['cells'].append(create_markdown_cell([
    "---\n",
    "\n",
    "### ğŸ“ åŸºç¤å•é¡Œ2: æå¤±æ›²ç·šã®åˆ†æ\n",
    "\n",
    "**èª²é¡Œ**: ä¸Šã§è¨“ç·´ã—ãŸMLPã®æå¤±æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã€åæŸã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ ãƒ’ãƒ³ãƒˆ</summary>\n",
    "\n",
    "- `mlp.loss_curve_`ã«æå¤±ã®å±¥æ­´ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™\n",
    "- `plt.plot()`ã§ç°¡å˜ã«ãƒ—ãƒ­ãƒƒãƒˆã§ãã¾ã™\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>âœ… è§£ç­”ä¾‹</summary>\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {mlp.loss_curve_[0]:.4f}\")\n",
    "print(f\"Final loss: {mlp.loss_curve_[-1]:.4f}\")\n",
    "print(f\"Total iterations: {mlp.n_iter_}\")\n",
    "```\n",
    "\n",
    "</details>\n"
]))

improved_nb['cells'].append(create_code_cell([
    "# ã“ã“ã«ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„\n",
    "\n"
]))

improved_nb['cells'].append(create_markdown_cell([
    "---\n",
    "\n",
    "### ğŸ“ å¿œç”¨å•é¡Œ: ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "\n",
    "**èª²é¡Œ**: `GridSearchCV`ã‚’ä½¿ã£ã¦ã€`make_moons`ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹æœ€é©ãªMLPãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "**æ¢ç´¢ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**\n",
    "- `hidden_layer_sizes`: `[(10,), (50,), (20, 10)]`\n",
    "- `alpha`: `[0.0001, 0.001, 0.01]`\n",
    "- `learning_rate_init`: `[0.001, 0.01]`\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ ãƒ’ãƒ³ãƒˆ</summary>\n",
    "\n",
    "- `GridSearchCV`ã¯`sklearn.model_selection`ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "- `cv=5`ã§5åˆ†å‰²äº¤å·®æ¤œè¨¼\n",
    "- `n_jobs=-1`ã§ä¸¦åˆ—å‡¦ç†ï¼ˆé«˜é€ŸåŒ–ï¼‰\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>âœ… è§£ç­”ä¾‹</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (20, 10)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# çµæœ\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test score: {grid_search.score(X_test_scaled, y_test):.4f}\")\n",
    "```\n",
    "\n",
    "</details>\n"
]))

improved_nb['cells'].append(create_code_cell([
    "# ã“ã“ã«ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„\n",
    "\n"
]))

improved_nb['cells'].append(create_markdown_cell([
    "---\n",
    "\n",
    "### ğŸ”¥ ãƒãƒ£ãƒ¬ãƒ³ã‚¸å•é¡Œ: å¤šã‚¯ãƒ©ã‚¹åˆ†é¡\n",
    "\n",
    "**èª²é¡Œ**: `make_classification`ã§4ã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã€MLPã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚æ··åŒè¡Œåˆ—ã¨ã‚¯ãƒ©ã‚¹ã”ã¨ã®ç²¾åº¦ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "**è¦ä»¶:**\n",
    "- 4ã‚¯ãƒ©ã‚¹ï¼ˆ`n_classes=4`ï¼‰\n",
    "- ç‰¹å¾´é‡20å€‹ï¼ˆ`n_features=20`ï¼‰\n",
    "- ã‚µãƒ³ãƒ—ãƒ«æ•°2000å€‹\n",
    "- å…¨ä½“ç²¾åº¦0.80ä»¥ä¸Š\n",
    "- æ··åŒè¡Œåˆ—ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ ãƒ’ãƒ³ãƒˆ</summary>\n",
    "\n",
    "- `make_classification(n_classes=4, n_clusters_per_class=1)`\n",
    "- `confusion_matrix`ã¯`sklearn.metrics`ã‹ã‚‰\n",
    "- `sns.heatmap()`ã§ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º\n",
    "- `classification_report`ã§ã‚¯ãƒ©ã‚¹ã”ã¨ã®è©³ç´°ã‚’è¡¨ç¤º\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>âœ… è§£ç­”ä¾‹</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_classes=4,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# åˆ†å‰²ãƒ»ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# MLPè¨“ç·´\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# äºˆæ¸¬\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# æ··åŒè¡Œåˆ—\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹ã”ã¨ã®è©³ç´°\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"\\nOverall Accuracy: {mlp.score(X_test_scaled, y_test):.4f}\")\n",
    "```\n",
    "\n",
    "</details>\n"
]))

improved_nb['cells'].append(create_code_cell([
    "# ã“ã“ã«ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„\n",
    "\n"
]))

# ==== 11. COLUMN ====
improved_nb['cells'].append(create_markdown_cell([
    "---\n",
    "\n",
    "## ğŸ“– ã‚³ãƒ©ãƒ : ä¸‡èƒ½è¿‘ä¼¼å®šç†ã¨ã¯ï¼Ÿ\n",
    "\n",
    "### ç†è«–çš„èƒŒæ™¯\n",
    "\n",
    "1989å¹´ã€George Cybenkoã¯é©šãã¹ãå®šç†ã‚’è¨¼æ˜ã—ã¾ã—ãŸï¼š\n",
    "\n",
    "> **ä¸‡èƒ½è¿‘ä¼¼å®šç†**: ååˆ†ãªæ•°ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’æŒã¤1å±¤ã®éš ã‚Œå±¤ã‚’æŒã¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€  \n",
    "> ä»»æ„ã®é€£ç¶šé–¢æ•°ã‚’ä»»æ„ã®ç²¾åº¦ã§è¿‘ä¼¼ã§ãã‚‹ã€‚\n",
    "\n",
    "### ã“ã‚Œã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã®ã‹ï¼Ÿ\n",
    "\n",
    "ç†è«–çš„ã«ã¯ã€`hidden_layer_sizes=(10000,)` ã®ã‚ˆã†ãªå˜å±¤ã®å·¨å¤§ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§  \n",
    "ã‚ã‚‰ã‚†ã‚‹é–¢æ•°ã‚’è¿‘ä¼¼ã§ãã¾ã™ã€‚\n",
    "\n",
    "### ã—ã‹ã—ã€å®Ÿéš›ã«ã¯...\n",
    "\n",
    "**å•é¡Œ:**\n",
    "- ã€Œè¿‘ä¼¼ã§ãã‚‹ã€â‰ ã€ŒåŠ¹ç‡çš„ã«å­¦ç¿’ã§ãã‚‹ã€\n",
    "- å˜å±¤ã§ã¯å¿…è¦ãªãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ãŒè†¨å¤§ã«ãªã‚‹\n",
    "- è¨“ç·´ã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã‚‹\n",
    "\n",
    "**è§£æ±ºç­–: æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**\n",
    "- å¤šå±¤ï¼ˆæ·±ã„ï¼‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€ã‚ˆã‚Šå°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¤‡é›‘ãªé–¢æ•°ã‚’è¡¨ç¾å¯èƒ½\n",
    "- ä¾‹: `(100, 50, 25)` ã¯ `(1000,)` ã‚ˆã‚ŠåŠ¹ç‡çš„ãªã“ã¨ãŒå¤šã„\n",
    "\n",
    "ã“ã‚ŒãŒ**Deep Learning**ï¼ˆæ·±å±¤å­¦ç¿’ï¼‰ã®ç†è«–çš„åŸºç¤ã§ã™ï¼\n",
    "\n",
    "### å®Ÿé¨“ã§ç¢ºèª\n",
    "\n",
    "åŒã˜æ€§èƒ½ã‚’é”æˆã™ã‚‹ã®ã«ï¼š\n",
    "- å˜å±¤: `(500,)` â†’ ç´„10,000ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "- å¤šå±¤: `(50, 25, 10)` â†’ ç´„2,000ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "\n",
    "**çµè«–**: æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ–¹ãŒåŠ¹ç‡çš„ï¼\n",
    "\n",
    "### å‚è€ƒæ–‡çŒ®\n",
    "- Cybenko, G. (1989) \"Approximation by superpositions of a sigmoidal function\"\n",
    "- Hornik, K. (1991) \"Approximation capabilities of multilayer feedforward networks\"\n",
    "\n",
    "---\n"
]))

# ==== 12. PROGRESS TRACKING ====
improved_nb['cells'].append(create_markdown_cell([
    "## ğŸ¯ å­¦ç¿’é€²æ—ã®è¨˜éŒ²\n",
    "\n",
    "ã“ã®ç« ã‚’å®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦é€²æ—ã‚’è¨˜éŒ²ã—ã¾ã—ã‚‡ã†ï¼\n"
]))

improved_nb['cells'].append(create_code_cell([
    "# ğŸ¯ é€²æ—ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ \n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def mark_complete(notebook_number, notebook_name):\n",
    "    \"\"\"ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Œäº†æ¸ˆã¿ã«ãƒãƒ¼ã‚¯\"\"\"\n",
    "    progress_file = Path(\"../learning_progress.json\")\n",
    "    \n",
    "    # Load existing progress\n",
    "    if progress_file.exists():\n",
    "        with open(progress_file, 'r') as f:\n",
    "            progress = json.load(f)\n",
    "    else:\n",
    "        progress = {\"completed\": [], \"timestamps\": {}}\n",
    "    \n",
    "    # Mark as complete\n",
    "    if notebook_number not in progress[\"completed\"]:\n",
    "        progress[\"completed\"].append(notebook_number)\n",
    "        progress[\"completed\"].sort()\n",
    "        progress[\"timestamps\"][str(notebook_number)] = datetime.now().isoformat()\n",
    "    \n",
    "    # Save\n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "    \n",
    "    # Display progress\n",
    "    total_notebooks = 13\n",
    "    completion_rate = len(progress[\"completed\"]) / total_notebooks * 100\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ‰ Notebook {notebook_number:02d}: {notebook_name} å®Œäº†ï¼\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nğŸ“Š å…¨ä½“é€²æ—: {completion_rate:.1f}% ({len(progress['completed'])}/{total_notebooks})\")\n",
    "    \n",
    "    # Progress bar\n",
    "    bar_length = 40\n",
    "    filled = int(bar_length * completion_rate / 100)\n",
    "    bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)\n",
    "    print(f\"\\n[{bar}] {completion_rate:.0f}%\\n\")\n",
    "    \n",
    "    # Badges\n",
    "    if completion_rate >= 25 and completion_rate < 50:\n",
    "        print(\"ğŸ† ãƒãƒƒã‚¸ç²å¾—: ãƒ“ã‚®ãƒŠãƒ¼ï¼ˆ25%é”æˆï¼‰\")\n",
    "    elif completion_rate >= 50 and completion_rate < 75:\n",
    "        print(\"ğŸ† ãƒãƒƒã‚¸ç²å¾—: ä¸­ç´šè€…ï¼ˆ50%é”æˆï¼‰\")\n",
    "    elif completion_rate >= 75 and completion_rate < 100:\n",
    "        print(\"ğŸ† ãƒãƒƒã‚¸ç²å¾—: ä¸Šç´šè€…ï¼ˆ75%é”æˆï¼‰\")\n",
    "    elif completion_rate >= 100:\n",
    "        print(\"ğŸ“ ãƒãƒƒã‚¸ç²å¾—: ãƒã‚¹ã‚¿ãƒ¼ï¼ˆ100%é”æˆï¼‰\")\n",
    "        print(\"\\nâœ¨ ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼å…¨ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Œäº†ã—ã¾ã—ãŸï¼ âœ¨\")\n",
    "    \n",
    "    # Next notebook\n",
    "    if notebook_number < total_notebooks:\n",
    "        print(f\"\\nâ¡ï¸  æ¬¡ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯: Notebook {notebook_number+1:02d}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "# ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Œäº†ã—ãŸã‚‰ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦å®Ÿè¡Œ\n",
    "# mark_complete(7, \"MLP Fundamentals\")"
]))

# ==== 13. FINAL NEXT STEPS ====
improved_nb['cells'].append(create_markdown_cell([
    "---\n",
    "\n",
    "## â¡ï¸ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "### å­¦ç¿’ã‚’ç¶šã‘ã‚‹\n",
    "\n",
    "MLPã®åŸºç¤ã‚’ç†è§£ã—ãŸã‚‰ã€æ¬¡ã¯ï¼š\n",
    "\n",
    "**ğŸ“— Notebook 08: MLP Parameter Space Exploration**\n",
    "- ã‚ˆã‚Šåºƒç¯„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢\n",
    "- ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’å¯è¦–åŒ–\n",
    "- æœ€é©åŒ–æˆ¦ç•¥ã®ç†è§£\n",
    "\n",
    "### å¾©ç¿’ãŒå¿…è¦ãªå ´åˆ\n",
    "\n",
    "- **Notebook 02**: å‰å‡¦ç†ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å¾©ç¿’\n",
    "- **Notebook 03**: è©•ä¾¡æŒ‡æ¨™ã‚’å¾©ç¿’\n",
    "- **Notebook 04-06**: ä»–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨æ¯”è¼ƒ\n",
    "\n",
    "### ã•ã‚‰ã«å­¦ã¶ãŸã‚ã«\n",
    "\n",
    "**æ›¸ç±:**\n",
    "- \"Neural Networks and Deep Learning\" by Michael Nielsenï¼ˆç„¡æ–™ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ï¼‰\n",
    "- \"Deep Learning\" by Goodfellow, Bengio, and Courville\n",
    "\n",
    "**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ã‚¹:**\n",
    "- Coursera: \"Neural Networks and Deep Learning\" by Andrew Ng\n",
    "- Fast.ai: \"Practical Deep Learning for Coders\"\n",
    "\n",
    "**å®Ÿè·µ:**\n",
    "- Kaggle competitions ã§å®Ÿãƒ‡ãƒ¼ã‚¿ã«æŒ‘æˆ¦\n",
    "- UCI ML Repository ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç·´ç¿’\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‰ ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼\n",
    "\n",
    "ã“ã®ç« ã§å­¦ã‚“ã MLPã®çŸ¥è­˜ã¯ã€Deep Learningã¸ã®ç¬¬ä¸€æ­©ã§ã™ã€‚  \n",
    "æ¬¡ã®ç« ã§ã•ã‚‰ã«æ·±ãæ¢æ±‚ã—ã¾ã—ã‚‡ã†ï¼\n"
]))

# Save improved notebook
output_path = Path("notebooks/07_mlp_fundamentals_improved.ipynb")
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(improved_nb, f, indent=1, ensure_ascii=False)

print(f"âœ… Improved notebook created: {output_path}")
print(f"ğŸ“Š Total cells: {len(improved_nb['cells'])}")
print(f"ğŸ“ˆ Original cells: {len(nb['cells'])}")
print(f"â• Added cells: {len(improved_nb['cells']) - len(nb['cells'])}")
